[{"content":"This is an excellent blog post summarizing architectural differences among several popular LLMs.\n1. Original Transformer vs Modern Variants The table below summarizes the major architectural and training differences between the original Transformer (Vaswani et al., 2017) and common Transformer variants used in modern LLMs:\nAspect Original Transformer (2017) Common Variants in Modern LLMs Normalization order Post-LN Pre-LN Activation ReLU SwiGLU (GELU/SiLU/Swish variants, etc.) Dropout Widely used Often reduced or removed for large models Normalization type LayerNorm RMSNorm (also LayerNorm/ScaleNorm variants) Linear layers With bias Bias-free Attention heads Multi-head attention (fixed) GQA / MQA, etc. Positional encoding Absolute (sinusoidal) RoPE, etc. Other - FlashAttention, MoE, hierarchical parallelism, etc. 1.1. Pre-norm vs Post-norm Almost all modern language models use pre-norm (except BERT), which tends to make training more stable.\nLeft: pre-norm. Right: post-norm.\nNew! Left: pre-norm. Right: “double norm” (used by e.g. Grok, Gemma 2).\nNew! OlMo 2 applies post-norm only to the non-residual branch.\n1.2. LayerNorm vs RMSNorm Original Transformer: LayerNorm (GPT-1/2/3, OPT, GPT-J, BLOOM)\n$$ y = \\frac{x - \\textbf{E}[x]}{\\sqrt{\\textbf{Var}[x] + \\epsilon}} * \\gamma + \\beta $$\nModern LMs: RMSNorm (LLaMA family, PaLM, T5)\n$$ y = \\frac{x}{\\sqrt{\\lVert x \\rVert^2_2 + \\epsilon}} * \\gamma $$\nAdvantages of RMSNorm: faster in practice without hurting accuracy\nfewer operations (no mean computation) fewer parameters (no bias term) 1.3. FFN: With Bias vs Bias-free Original Transformer: with bias\n$$ \\textbf{FFN}(x) = \\max(0,xW_1+b_1)W_2+b_2 $$\nModern LMs: bias-free\n$$ \\textbf{FFN}(x) = \\sigma(xW_1)W_2 $$\nAdvantages of bias-free FFNs: smaller memory footprint and more stable optimization.\n1.4. Activation Functions Activation Model ReLU Original transformer, T5, Gopher, Chinchilla, OPT GeLU GPT1/2/3, GPTJ, GPT-Neox, BLOOM GeGLU T5 v1.1, mT5, LaMDA, Phi3, Gemma 2, Gemma 3 SwiGLU LLaMa 1/2/3, PaLM, Mistral, OlMo, most models post 2023 For an introduction to activations, see this post.\n1.5. Positional Encoding 1.5.1. Sinusoidal Positional Encoding Key idea: use sine/cosine waves of different frequencies to encode position information across dimensions.\nFor position $pos$ in the sequence and dimension index $i$, the encoding is:\n$$ \\begin{aligned} PE_{(pos,2i)} = sin(\\frac{pos}{10000^{2i/d_{model}}}) \\ PE_{(pos,2i+1)} = cos(\\frac{pos}{10000^{2i/d_{model}}}) \\end{aligned} $$\nWhere:\n$pos$ is the position index (starting from 0) $i$ is the dimension index $d_{model}$ is the model hidden size 10000 is a heuristic constant that controls the frequency range Capturing relative positions Suppose the model attends to two tokens at positions $pos_1$ and $pos_2$. Their encodings are:\n$$ \\begin{aligned} E_1 = [sin(\\frac{pos_1}{10000^{0}}), cos(\\frac{pos_1}{10000^{0}}), sin(\\frac{pos_1}{10000^{2/d_{model}}}), cos(\\frac{pos_1}{10000^{2/d_{model}}}), \\dots ] \\\\ E_2 = [sin(\\frac{pos_2}{10000^{0}}), cos(\\frac{pos_2}{10000^{0}}), sin(\\frac{pos_2}{10000^{2/d_{model}}}), cos(\\frac{pos_2}{10000^{2/d_{model}}}), \\dots ] \\end{aligned} $$\nInside the model we may take an inner product:\n$$ E_1 \\cdot E_2 = sin(\\frac{pos_1}{10000^{0}})sin(\\frac{pos_2}{10000^{0}}) + cos(\\frac{pos_1}{10000^{0}})cos(\\frac{pos_2}{10000^{0}}) + \\dots $$\nUsing the trigonometric identity:\n$$ \\sin a \\sin b + \\cos a \\cos b = \\cos(a - b) $$\nWe obtain:\n$$ E_1 \\cdot E_2 = \\cos(\\frac{pos_1 - pos_2}{10000^{0}}) + \\cos(\\frac{pos_1 - pos_2}{10000^{2/d_{model}}}) + \\dots $$\nThis shows the dot product depends on the relative distance $pos_1 - pos_2$.\n1.5.2. Absolute Positional Encoding / Learnable Positional Embedding My take: “absolute positional encoding” is more of a concept — it encodes a token’s absolute index directly, rather than encoding relative offsets between tokens.\nA learnable positional embedding learns an embedding matrix:\n$$ P = [u_0, u_1, u_2, \\dots, u_{L-1}] \\in \\mathbb{R}^{L \\times d_{model}} $$\nWhere:\n$L$ is the maximum sequence length each row $u_i$ is a trainable embedding vector for position $i$ The final input to the model becomes:\n$$ Embed(x, i) = v_x + u_i $$\nwhere $v_x$ is the token embedding of token $x$.\n1.5.3. Relative Positional Encoding Limitations of absolute positional encoding:\npoor length generalization: if trained with a fixed length (e.g., 512), it may not extrapolate beyond that weak relative awareness: the model knows “the 5th word” and “the 10th word” but not explicitly that they are “5 positions apart” However, word order relations in natural language are often relative:\nIn the dependency between “the cat” and “the big cat”, “cat” is only a few steps away from “the”.\nSo the goal of relative positional encoding is to let the model learn how the distance $(i-j)$ between token $i$ and token $j$ affects attention.\nOne approach is to inject relative position information directly into the attention logits:\n$$ e_{ij} = \\frac{(x_i W_Q)(x_j W_K + a^K_{ij})^T}{\\sqrt{d_k}} $$\nwhere $a_{ij}^K$ is a vector representing the relative position between token $i$ and token $j$.\n1.5.4. RoPE (Rotary Position Embedding) How can we transform the position-encoded embeddings $x$ and $y$ such that their dot product depends only on the relative position? Concretely, we want:\n$$ \\langle f(x, i), f(y, j) \\rangle = g(x, y, i-j) \\tag{1} $$ where $\\langle \\cdot, \\cdot \\rangle$ denotes an inner product.\nSinusoidal encoding does not satisfy Eq. (1): With sinusoidal encoding, the embedding can be written as $Embed(x, i) = v_x + E_i$, where $v_x$ is the token embedding and $E_i$ is the positional encoding. Then $\\langle Embed(x, i), Embed(y, j) \\rangle = \\langle v_x + E_i, v_y + E_j \\rangle = \\langle v_x, v_y \\rangle + \\langle v_x, E_j \\rangle + \\langle E_i, v_y \\rangle + \\langle E_i, E_j \\rangle$, which contains terms depending on absolute positions $i$ and $j$, not only their difference $(i-j)$. Absolute (learnable) positional embeddings also do not satisfy Eq. (1). Relative positional encoding does not preserve the pure inner-product form in Eq. (1): the geometric interpretation is weakened: the dot product is no longer simply cosine similarity between vectors once the bias term is injected symmetry is broken: $e_{ij}$ and $e_{ji}$ can differ, enabling directionality the probabilistic interpretation weakens: logits before softmax are no longer determined only by vector similarity; extra bias can affect stability The core idea of RoPE is to embed position into each pair of dimensions via complex rotation (equivalently, a 2D plane rotation).\n$$ \\begin{aligned} x_p \u0026amp;= [x_{p,0}, x_{p,1}, \\dots, x_{p,d-1}]\\\\ f_{{q,k}}(x_p,p) \u0026amp;= \\mathbf{R}^d_{\\Theta,p},W_{{q,k}},x_p\\\\ \\mathbf{R}^d_{\\Theta,p} \u0026amp;= \\begin{bmatrix} \\cos(p\\theta_0) \u0026amp; -\\sin(p\\theta_0) \u0026amp; \u0026amp; \u0026amp; \\\\ \\sin(p\\theta_0) \u0026amp; \\cos(p\\theta_0) \u0026amp; \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026amp; \\cos(p\\theta_1) \u0026amp; -\\sin(p\\theta_1) \u0026amp; \\\\ \u0026amp; \u0026amp; \\sin(p\\theta_1) \u0026amp; \\cos(p\\theta_1) \u0026amp; \\\\ \u0026amp; \u0026amp; \u0026amp; \u0026amp; \\ddots \\\\ \u0026amp; \u0026amp; \u0026amp; \u0026amp; \u0026amp; \\cos(p\\theta_{d/2-1}) \u0026amp; -\\sin(p\\theta_{d/2-1}) \\\\ \u0026amp; \u0026amp; \u0026amp; \u0026amp; \u0026amp; \\sin(p\\theta_{d/2-1}) \u0026amp; \\cos(p\\theta_{d/2-1}) \\end{bmatrix} \\end{aligned} $$\nwhere $\\theta_k = 10000^{-2k/d}$.\nNext, we show why RoPE satisfies Eq. (1).\nFor two adjacent embedding dimensions $2k$ and $2k+1$ of token $x$, we have:\n$$ \\tilde{x}_{p}^{(k)} = \\begin{bmatrix} cos(p\\theta_k) \u0026amp; -sin(p\\theta_k) \\\\ sin(p\\theta_k) \u0026amp; cos(p\\theta_k) \\end{bmatrix} \\begin{bmatrix} x_{p,2k} \\\\ x_{p,2k+1} \\end{bmatrix} =(x_{p,2k}+ix_{p,2k+1})e^{ip\\theta_k} $$\nTherefore:\n$$ \\langle \\tilde{x}_{p}^{(k)}, \\tilde{y}_{q}^{(k)} \\rangle = \\tilde{x}_{p}^{(k)} \\cdot \\overline{\\tilde{y}}_{q}^{(k)} = (x_{p,2k}+ix_{p,2k+1})(y_{q,2k}-iy_{q,2k+1})e^{i(p-q)\\theta_k} $$\nThis depends on $(p-q)$, meeting the requirement in Eq. (1).\n2. Hyperparameters 2.1. FFN hidden size Let $d_{ff}$ be the FFN hidden size and $d_{model}$ be the model hidden size.\n$$ d_{ff} = 4d_{model} $$\nThen the parameter count of a standard FFN is:\nfirst layer: $d_{model} \\times d_{ff} = 4d_{model}^2$ second layer: $d_{ff} \\times d_{model} = 4d_{model}^2$ total: $8d_{model}^2$ For an FFN with a GLU-type activation, the parameter count is:\nGLU content projection: $d_{model} \\times d\u0026rsquo;_{ff}$ GLU gate projection: $d_{model} \\times d\u0026rsquo;_{ff}$ second layer: $d\u0026rsquo;_{ff} \\times d_{model}$ total: $3d_{model} \\times d\u0026rsquo;_{ff}$ To match the parameter count of a standard FFN, we need:\n$$ 3d_{model} \\times d\u0026rsquo;_{ff} = 8d_{model}^2 $$\n即\n$$ d\u0026rsquo;_{ff} = \\frac{8}{3}d_{model} $$\nThe table below lists $d_{ff}/d_{model}$ ratios used by some popular models:\nModel $( d_{ff} / d_{model} )$ PaLM 4.00 Mistral 7B 3.50 LLaMA-2 70B 3.50 LLaMA 70B 2.68 Qwen 14B 2.67 DeepSeek 67B 2.68 Yi 34B 2.85 T5 v1.1 2.50 2.2. Number of attention heads and head dimension In practice, many models set $d_{head} = d_{model} / num_{heads}$ (and we generally want $d_{head} \\gtrsim d_{model}/num_{heads}$).\nModel Num heads Head dim Model dim Ratio GPT3 96 128 12288 1 T5 128 128 1024 16 T5 v1.1 64 64 4096 1 LaMDA 128 128 8192 2 PaLM 48 258 18432 1.48 LLaMA2 64 128 8192 1 2.3. Model aspect ratio Here the “aspect ratio” refers to:\n$$ d_{model} / num_{layers} $$\nModel ( d_{model} / n_{layer} ) BLOOM 205 T5 v1.1 171 PaLM (540B) 156 GPT3 / OPT / Mistral / Qwen 128 LLaMA / LLaMA2 / Chinchila 102 T5 (11B) 43 GPT2 33 Very deep models are harder to parallelize and tend to have higher latency.\n2.4. Vocabulary size Monolingual: 30k–50k tokens Multilingual: 100k–250k tokens 2.5. Dropout and weight decay Older models tend to use more dropout. Newer models more often rely on weight decay; empirically it interacts with the loss dynamics (e.g., faster loss decrease later in training) rather than merely preventing overfitting. Model Dropout* Weight decay Original transformer 0.1 0 GPT2 0.1 0.1 T5 0.1 0 GPT3 0.1 0.1 T5 v1.1 0 0 PaLM 0 (variable) OPT 0.1 0.1 LLaMA 0 0.1 Qwen 14B 0.1 0.1 3. Training stability tips During training, we want to avoid “spikes” (the blue curve below):\nz-loss Consider the softmax at the final layer of an LLM:\n$$ P(y=i|x) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} = \\frac{e^{z_i}}{Z} $$\nSo for cross-entropy loss we have:\n$$ Loss_{CE} = -\\log P(y=i|x) = -\\log \\frac{e^{z_i}}{\\sum_j e^{z_j}} = -z_i + \\log Z $$\nIf $Z$ becomes too small, $Loss_{CE}$ can become too large, leading to instability.\nSo we try to keep $Z$ close to 1 (equivalently, keep $\\log Z$ close to 0) by adding a z-loss term:\n$$ Loss_{z} = ((\\log Z)^2 - 0)^2 = (\\log Z)^2 $$\nFinally:\n$$ Loss = Loss_{CE} + \\lambda Loss_{z} $$\nwhere $\\lambda$ is a small coefficient, typically $1e-3$ or $1e-4$.\n4. Architecture optimizations 4.1. KV Cache Image source: link\nStandard attention computation:\n$$ Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V $$\nAssume $X \\in \\mathbb{R}^{b \\times T \\times D}$ and $W_{\\{Q, K, V\\}} \\in \\mathbb{R}^{D \\times (hd)}$, where $T$ is the sequence length, $h$ is the number of attention heads, and $d$ is the per-head dimension. Let $D = hd$. The compute cost is:\ncompute K/Q/V: $3 \\times 2bTD^2 = 6bT(hd)^2$ compute $Q \\times K$: $2bhT^2d$ compute softmax: $n \\times bhT^2$ (softmax involves $n$ operations) compute $Output_{softmax} \\times V$: $2bhT^2d$ output projection: $2bTD^2$ total $\\approx 8bTD^2 + 4bhT^2d$ (ignoring softmax) The memory cost is:\nweight parameters: $W_{\\{Q, K, V\\}}$: $3 \\times D(hd) = 3(hd)^2$ output projection: $(hd)D = (hd)^2$ intermediate activations: input: $bTD$ K/Q/V: $3 \\times bhTd$ attention weights (after softmax): $bhT^2$ output: $bTD$ (input to the next layer; often not counted for this layer) total $\\approx 4(hd)^2 + bTD + 3bhTd + bhT^2$ Attention with KV cache:\nDuring training, KV caching does not reduce compute, so it is often not used.\nIn inference, if the current sequence length is $t$, predicting the next token costs $\\approx 8btD^2 + 4bht^2d$. Without KV cache, predicting the next token after that would cost $\\approx 8b(t+1)D^2 + 4bh(t+1)^2d$, which grows significantly with sequence length.\nWith KV cache, predicting the next token costs:\ncompute K/Q/V: since we reuse $K_{1:(t-1)}, V_{1:(t-1)}$, we only compute $Q_t, K_t, V_t$, costing $3 \\times 2bD^2$ compute $Q_t \\times K_{1:t}$: $2bhtd$ softmax: $n \\times bht$ compute $Output_{softmax} \\times V_{1:t}$: $2bhtd$ output projection: $2bD^2$ total $\\approx 8bD^2 + 4bhtd$ (ignoring softmax) The compute now grows only mildly with sequence length.\nKV cache slightly increases memory in inference. Without KV cache, memory is dominated by weights $4(hd)^2$. With KV cache, we add cache storage $2bhtd$, so total memory is $\\approx 4(hd)^2 + 2bhtd$.\n4.2. MQA and GQA To reduce KV-cache memory, a simple idea is to let attention heads share $K$ and $V$:\nif all $h$ heads share a single $K$ and $V$, it is MQA (Multi-query Attention) if we split $h$ heads into $g$ groups and each group shares $K$ and $V$, it is GQA (Grouped-query Attention) Illustration:\nModel Training-time Inference-time Notes GPT-3 / GPT-4 MHA MHA / GQA (partially optimized) GPT-4 reportedly uses GQA PaLM 2 GQA GQA native training structure Claude 3 GQA GQA publicly described by Anthropic LLaMA 2 MHA GQA (converted) converted later by Meta Mistral GQA GQA end-to-end GQA Falcon MQA MQA optimized for long-context inference Gemini 1.5 GQA GQA used by Google for multimodal LLMs Compared to MHA, MQA/GQA keep compute complexity roughly unchanged but reduce memory. Suppose $h$ query heads share $k$ key/value heads:\nweight parameters: $W_{\\{Q\\}}$: $D(hd) = (hd)^2$ $W_{\\{K, V\\}}$: $2 \\times D(kd) = 2(hd)(kd)$ output projection: $(hd)D = (hd)^2$ intermediate activations: input: $bTD$ $Q$: $bhTd$ $KV$: $2bkdT$ attention weights (after softmax): $bhT^2$ output: $bTD$ (input to the next layer; often not counted for this layer) total memory $\\approx 2(hd)^2 + 2hkd^2 + 2bTD + 2bkdT + bhT^2$ 4.3. Sparse Attention Sparse attention: see this blog post.\nReferences stanford-cs336 lecture 3 ","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/transformer_in_llm/","summary":"\u003cp\u003eThis is an excellent \u003ca href=\"https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison\"\u003eblog post\u003c/a\u003e summarizing architectural differences among several popular LLMs.\u003c/p\u003e\n\u003ch2 id=\"1-original-transformer-vs-modern-variants\"\u003e1. Original Transformer vs Modern Variants\u003c/h2\u003e\n\u003cp\u003eThe table below summarizes the major architectural and training differences between the original Transformer (Vaswani et al., 2017) and common Transformer variants used in modern LLMs:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAspect\u003c/th\u003e\n          \u003cth\u003eOriginal Transformer (2017)\u003c/th\u003e\n          \u003cth\u003eCommon Variants in Modern LLMs\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eNormalization order\u003c/td\u003e\n          \u003ctd\u003ePost-LN\u003c/td\u003e\n          \u003ctd\u003ePre-LN\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eActivation\u003c/td\u003e\n          \u003ctd\u003eReLU\u003c/td\u003e\n          \u003ctd\u003eSwiGLU (GELU/SiLU/Swish variants, etc.)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDropout\u003c/td\u003e\n          \u003ctd\u003eWidely used\u003c/td\u003e\n          \u003ctd\u003eOften reduced or removed for large models\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eNormalization type\u003c/td\u003e\n          \u003ctd\u003eLayerNorm\u003c/td\u003e\n          \u003ctd\u003eRMSNorm (also LayerNorm/ScaleNorm variants)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eLinear layers\u003c/td\u003e\n          \u003ctd\u003eWith bias\u003c/td\u003e\n          \u003ctd\u003eBias-free\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eAttention heads\u003c/td\u003e\n          \u003ctd\u003eMulti-head attention (fixed)\u003c/td\u003e\n          \u003ctd\u003eGQA / MQA, etc.\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePositional encoding\u003c/td\u003e\n          \u003ctd\u003eAbsolute (sinusoidal)\u003c/td\u003e\n          \u003ctd\u003eRoPE, etc.\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eOther\u003c/td\u003e\n          \u003ctd\u003e-\u003c/td\u003e\n          \u003ctd\u003eFlashAttention, MoE, hierarchical parallelism, etc.\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"11-pre-norm-vs-post-norm\"\u003e1.1. Pre-norm vs Post-norm\u003c/h3\u003e\n\u003cp\u003eAlmost all modern language models use pre-norm (except BERT), which tends to make training more stable.\u003c/p\u003e","title":"LM Architecture and Training"},{"content":"1. Generating Objects 1.1 Represent the object as a vector Images: Spatial dimensions: height $H$ and width $W$ Color channels: RGB $$ z \\in \\mathbb{R}^{H \\times W \\times 3} $$\nVideos: Temporal dimension: $T$ frames Each frame is an image $$ z \\in \\mathbb{R}^{T \\times H \\times W \\times 3} $$\nMolecular structures: $N$ atoms Each atom has 3 coordinates $$ z \\in \\mathbb{R}^{N \\times 3} $$\nIn general, we can represent the object we want to generate as a vector:\n$$ z \\in \\mathbb{R}^{d} $$\n1.2 Generation: sampling from the data distribution Data distribution: the distribution of objects we want to generate, denoted by $p_{data}$.\nProbability density: $p_{data}: \\mathbb{R}^d \\to \\mathbb{R}_{\\ge 0}$.\n⚠ NoteWe do not know the true probability density in practice. \u0026times; Generation means sampling $z \\sim p_{data}$. For example:\n1.3 Dataset: finite samples from the data distribution Dataset: a finite collection of samples from the data distribution, $z_1, \\cdots, z_N \\sim p_{data}$.\n1.4 Conditional generation Conditional generation means sampling from a conditional distribution $z \\sim p_{data}(\\cdot \\mid y)$.\n2. Flow and Diffusion Models 2.1 Flow Models 2.1.1 Preliminaries Definition 1 (Trajectory): $X: [0, 1] \\to \\mathbb{R}^d, t \\mapsto X_t$.\nIntuition: a trajectory describes how a particle’s position changes over time $t \\in [0, 1]$. The input is time $t$, and the output is the position $X_t$ at that time.\nDefinition 2 (Vector field): $u: \\mathbb{R}^d \\times [0,1] \\to \\mathbb{R}^d, (x, t) \\mapsto u_t(x)$, where $x$ is the location and $u_t(x)$ is the vector direction.\nDefinition 3 (Ordinary Differential Equation, ODE): given the initial condition $X_0 = x_0$, the dynamics are\n$$ \\frac{dX_t}{dt} = u_t(X_t) $$\n$dX_t/dt$ is the tangent vector of the trajectory. You can interpret it as velocity, so an ODE describes how a particle moves in a vector field.\n2.1.2 Definition of a flow Definition 4 (Flow): $\\phi: \\mathbb{R}^d \\times [0, 1] \\to \\mathbb{R}^d, (t, x) \\mapsto \\phi_t(x)$, where $\\phi_t(x)$ is the new position at time $t$ of a particle that starts at position $x$ and follows the ODE trajectory. A flow satisfies:\n$$ \\phi_0(x_0) = x_0 $$\n$$ \\frac{d}{dt}\\phi_t(x_0) = u_t(\\phi_t(x_0)) $$\nIntuitively, a flow is a collection of ODE solutions for many different initial conditions.\nFlow visualization:\nLinear ODE:\nA simple vector field: $$ u_t(x) = - \\theta x $$\nThe flow is defined by the ODE: $$ \\frac{d}{dt} \\phi_t(x) = - \\theta \\phi_t(x) $$ $$ \\frac{d \\phi_t(x)}{\\phi_t(x)} = - \\theta dt $$ $$ \\int \\frac{d \\phi_t(x)}{\\phi_t(x)} = \\int - \\theta dt $$ $$ \\log \\phi_t(x) = - \\theta t + C(x) $$ $$ \\phi_t(x) = e^{-\\theta t + C(x)} = e^{C(x)} e^{-\\theta t} $$\nUse the initial condition to determine the constant $C(x)$: $$ \\phi_0(x) = e^{C(x)} e^{0} = e^{C(x)} = x \\Rightarrow e^{C(x)} = x \\Rightarrow C(x) = \\log x $$\nThe closed-form flow is: $$ \\phi_t(x) = x e^{-\\theta t} $$\nVerify it satisfies the definition: $$ \\phi_0(x) = e^{C(x)} e^{0} = e^{C(x)} = x \\Rightarrow e^{C(x)} = x $$ $$ \\frac{d}{dt} \\phi_t(x) = \\frac{d}{dt} (x e^{-\\theta t}) = -\\theta x e^{-\\theta t} = -\\theta \\phi_t(x) $$\nAlgorithm 1 Solving an ODE with Euler’s method Input: vector field $u_t$, initial position $x_0$, number of steps $n$ 1: Set $t = 0$ 2: Set step size $h = 1/n$ 3: Set $X_0 = x_0$ 4: for $i = 0$ to $n-1$ do 5: $~~~~$Update $X_{t+h} = X_t + h \\cdot u_{t}(X_t)$ 6: $~~~~$Update $t = t + h$ 7: end for Output: $X_0, X_h, X_{2h}, \\cdots, X_1$ 2.1.3 Definition of a flow model $p_{init} \\xrightarrow{ODE} p_{data}$\nDefinition 5 (Neural vector field): $u_t^{\\theta}: \\mathbb{R}^d \\times [0,1] \\to \\mathbb{R}^d$, where $\\theta$ denotes parameters.\nSample an initial point: $X_0 \\sim p_{init}$ Simulate the ODE driven by $u_t^{\\theta}$ Goal: $X_1 \\sim p_{data}$ Algorithm 2 Sampling from a flow model with Euler’s method Input: neural vector field $u_t^{\\theta}$, number of steps $n$ 1: Set $t = 0$ 2: Set step size $h = 1/n$ 3: Sample $X_0 \\sim p_{init}$ 4: for $i = 0$ to $n-1$ do 5: $~~~~$Update $X_{t+h} = X_t + h \\cdot u_{t}^{\\theta}(X_t)$ 6: $~~~~$Update $t = t + h$ 7: end for Output: $X_1$ 2.2 Diffusion models Definition 6 (Stochastic process): random variables $X_t, 0 \\le t \\le 1$, whose trajectory is $X: [0, 1] \\to \\mathbb{R}^d, t \\mapsto X_t$.\nDefinition 2 (Vector field): $u: \\mathbb{R}^d \\times [0,1] \\to \\mathbb{R}^d, (x, t) \\mapsto u_t(x)$.\nDefinition 7 (Diffusion coefficient): $\\sigma: [0,1] \\to \\mathbb{R}, t \\mapsto \\sigma_t$.\nDefinition 8 (Stochastic Differential Equation, SDE): given $X_0 = x_0$, the dynamics are\n$$ dX_t = u_t(X_t)dt + \\sigma_t dW_t. $$\nHere $u_t(X_t)dt$ is the deterministic ODE part, $\\sigma_t dW_t$ is the stochastic (noise) part, and $W_t$ is standard Brownian motion (Wiener process).\n2.2.1 Brownian motion As a stochastic process $W_t$:\n$W_0 = 0$ Gaussian increments: for any $0 \\leq s \u0026lt; t \\leq 1$, $W_t - W_s \\sim \\mathcal{N}(0, (t-s)I_d)$ Independent increments: for any $0 \\leq t_0 \u0026lt; t_1 \u0026lt; \\cdots \u0026lt; t_n \\leq 1$, the increments $W_{t_1} - W_{t_0}, W_{t_2} - W_{t_1}, \\cdots, W_{t_n} - W_{t_{n-1}}$ are independent 2.2.2 Expansions for $X_t$ For an ODE, $X_t$ can be expanded as:\n$$ \\frac{d}{dt} X_t = u_t(X_t) \\longleftrightarrow X_{t+h} = X_t + h \\cdot u_t(X_t) + h \\cdot R_t(h) $$\nwhere $\\lim_{h \\to 0} R_t(h) = 0$ and $h \\cdot R_t(h)$ is a higher-order infinitesimal (often written as $o(h)$). The right-hand side can be viewed as a Taylor expansion.\nDerivation: $$ \\frac{d}{dt} X_t = u_t(X_t) $$ $$ \\lim_{h \\to 0} \\frac{X_{t+h} - X_t}{h} = u_t(X_t) $$ $$ \\frac{X_{t+h} - X_t}{h} = u_t(X_t) + R_t(h) $$ $$ X_{t+h} = X_t + h \\cdot u_t(X_t) + h \\cdot R_t(h) $$\nFor an SDE, $X_t$ can be expanded as:\n$$ dX_t = u_t(X_t)dt + \\sigma_t dW_t \\longleftrightarrow X_{t+h} = X_t + h \\cdot u_t(X_t) + \\sigma_t (W_{t+h} - W_t) + h \\cdot R_t(h), $$\nwhere $\\lim_{h \\to 0} \\sqrt{\\mathbb{E}[|R_t(h)|^2]} = 0$. Because Brownian motion is not differentiable, the remainder term is typically defined via mean-square ($L^2$) convergence. Since $W_{t+h} - W_t \\sim \\mathcal{N}(0, hI_d)$, we can write it as $\\sqrt{h},\\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, I_d)$.\nAlgorithm 3 Sampling from a diffusion model (Euler–Maruyama for SDEs) Input: vector field $u_t$, diffusion coefficient $\\sigma_t$, number of steps $n$ 1: Set $t = 0$ 2: Set step size $h = 1/n$ 3: Set $X_0 = x_0$ 4: for $i = 0$ to $n-1$ do 5: $~~~~$Sample $\\epsilon \\sim \\mathcal{N}(0, I_d)$ 6: $~~~~$Update $X_{t+h} = X_t + h \\cdot u_{t}(X_t) + \\sigma_t \\sqrt{h} \\cdot \\epsilon$ 7: $~~~~$Update $t = t + h$ 8: end for Output: $X_0, X_h, X_{2h}, \\cdots, X_1$ 2.2.3 Definition of a diffusion model $p_{init} \\xrightarrow{SDE} p_{data}$\nDefinition 5 (Neural network): $u_t^{\\theta}: \\mathbb{R}^d \\times [0,1] \\to \\mathbb{R}^d$, where $\\theta$ denotes parameters.\nDefinition 7 (Diffusion coefficient): $\\sigma: [0,1] \\to \\mathbb{R}, t \\mapsto \\sigma_t$\nSample an initial point: $X_0 \\sim p_{init}$ Simulate the SDE: $dX_t = u_t^{\\theta}(X_t)dt + \\sigma_t dW_t$ Goal: $X_1 \\sim p_{data}$ References [1] GPT中英字幕课程资源, \u0026ldquo;《流匹配与扩散模型|6.S184 Flow Matching and Diffusion Models》中英字幕（Claude-3.7-s）》,\u0026rdquo; Bilibili, Jul. 29, 2025. [Online video]. Available: https://www.bilibili.com/video/BV1gc8Ez8EFL. Accessed: Jan. 30, 2026.\n","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/generation_with_sdes/","summary":"\u003ch2 id=\"1-generating-objects\"\u003e1. Generating Objects\u003c/h2\u003e\n\u003ch3 id=\"11-represent-the-object-as-a-vector\"\u003e1.1 Represent the object as a vector\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eImages:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eSpatial dimensions: height $H$ and width $W$\u003c/li\u003e\n\u003cli\u003eColor channels: RGB\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\nz \\in \\mathbb{R}^{H \\times W \\times 3}\n$$\u003c/p\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eVideos:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eTemporal dimension: $T$ frames\u003c/li\u003e\n\u003cli\u003eEach frame is an image\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\nz \\in \\mathbb{R}^{T \\times H \\times W \\times 3}\n$$\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eMolecular structures:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e$N$ atoms\u003c/li\u003e\n\u003cli\u003eEach atom has 3 coordinates\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\nz \\in \\mathbb{R}^{N \\times 3}\n$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIn general, we can represent the object we want to generate as a vector:\u003c/strong\u003e\u003c/p\u003e","title":"Generation Models with SDEs"},{"content":"1. Post-Training: Three Stages From InstructGPT[1]\nCollect data and train a supervised policy. Sample a prompt from the prompt dataset. Annotators label the desired output. Use the labeled data to perform supervised fine-tuning of the LLM. Collect comparison data and train a reward model. Sample a prompt and multiple model outputs. Annotators rank these outputs from “best” to “worst”. Use the ranking data to train the reward model. With the trained reward model, optimize the policy using reinforcement learning. Sample a new prompt from the dataset. Generate an output with the current policy. The reward model scores the output (Reward). Update the policy using PPO (or other RL methods) based on the reward. 2. Building an SFT Dataset 2.1. Issues in the dataset An example from the FLAN dataset:\nWhat is this text about? OPTIONS: - World - Sport - Business - Science/Tech\nNatural conversations usually do not contain such explicit options. An example from OpenAssistant:\nQuestion: Can you write a short introduction about the relevance of the term\u0026quot;monopsony\u0026quot; in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\nAnswer: \u0026ldquo;Monopsony\u0026rdquo; refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. [\u0026hellip;]. Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue. References: Bivens, J., \u0026amp; Mishel, L.(2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.\nHuman annotators often cannot write answers that are this long and detailed. How should you choose notation and even writing style? Should the answer include references? Getting references right is hard. We often want academic-style outputs, so we add references into answers in the dataset. But the model may only learn the shallow pattern “answers should include references”, which can increase hallucinations. Should the answers contain complex knowledge? Dataset size. Dataset safety. How to decide the output length: as the figure below shows, instruction datasets can have very different input/output lengths. 2.2. Practical tips for constructing SFT data SFT works best when the base model already has certain capabilities, and the data helps “extract” them. If you try to use SFT to “add” behaviors the model fundamentally lacks, results are often poor. Not all factually correct data improves performance. Even high-quality factual data can disrupt the model’s existing distribution/alignment and degrade performance. Some data types (e.g., safety, instruction-following, style) can yield large gains even in small amounts. However, improving long-tail behaviors (broad coverage, sparse scenarios) typically requires much more data. 2.3. Instruction tuning during pretraining Pretrain on web data or a pretraining corpus. Mix instruction-tuning data into pretraining. Do an additional short instruction-tuning stage. 2.4. Midtraining / Two-phase training This recipe[2] seems to be adopted by many LLM companies (though detailed docs are rare):\nIn the Stable stage, train on a pure pretraining dataset (left in the figure). In the Decay stage, train on a mixture of pretraining + instruction-tuning data (right in the figure). 3. RLHF (Reinforcement Learning with Human Feedback) 3.1. From imitation to optimization Imitation (SFT): adjust the model’s output distribution to match a reference distribution $p^*(y|x)$, so that $\\hat{p}(y|x) \\approx p^*(y|x)$.\nFrom a pure generative modeling perspective, SFT teaches the model to imitate the reference distribution. Training requires data from the reference policy (e.g., human-labeled datasets); otherwise, there is nothing to imitate. Optimization (RLHF): continuously adjust $\\hat{p}(y|x)$ to maximize $\\max\\limits_{p} E_p[R(y,x)]$, where $R(y,x)$ is the reward.\nWe are not optimizing an abstract “true distribution”, but a reward function we can define and measure (in RLHF, this reward comes from human feedback: rankings, pairwise preferences, or a trained reward model). At this stage, we no longer view the LM as an approximation to $p^*(y|x)$ (as in SFT). Instead, we treat it as a policy to maximize the reward signal. 3.2. Why RLHF is needed Cost: SFT is expensive, especially the annotation cost. G–V Gap (Generation–Value Gap): what people write (generation distribution $G$) is not always aligned with what people actually prefer (from the viewpoint of a value model $V$). A prior study[3] found that when some annotators compared their own summaries with model-written summaries, they sometimes preferred the model’s summaries—suggesting human-written references are not always optimal.\n3.3. How to collect RLHF data Option 1: (On the web) ask the model to produce $N$ outputs and let annotators (users) rank them.\nPotential issues: The labels may be low-quality or incorrect, or even generated by other LLMs—depending on the annotators. The annotator population distribution can significantly shape model behavior. For example, if many annotators are from Asia, the model may drift toward “Asian-style” outputs.\nDifferent annotators care about different things. Some focus on formatting; others focus on content.\nOption 2: use a large language model (e.g., GPT-4) to rank $N$ outputs (possibly from multiple models). This is often called AI Feedback.\n3.4. Methods to implement RLHF 3.4.1. PPO with human feedback Vanilla PPO = policy gradient + off-policy correction. Its objective is:\n$$ \\max L^{CLIP}(\\theta) - \\beta KL[\\pi_\\theta||\\pi_{old}] $$\n其中$L^{CLIP}(\\theta) = \\mathbb{E}_t \\Big[ \\min \\big( r_t(\\theta) \\hat{A}_t,; \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t \\big) \\Big]$，\n其中$r_t(\\theta) = \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}$，\n人类反馈下的PPO的优化目标为：\n$$ L^{RLHF}(\\theta) = \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) ] - \\beta D_{KL} \\left[ \\pi(y|x) || \\pi_{\\text{ref}}(y|x) \\right] $$\nWhere:\n$r(x,y)$: the reward model’s score for output $y$ given input $x$. $\\pi(y\\mid x)$: the current policy (the RLHF-trained model), i.e., the distribution over outputs $y$ given input $x$. $\\pi_{\\text{ref}}(y\\mid x)$: the reference policy, usually a frozen SFT model, used to prevent the policy from drifting too far. $D_{KL} \\left[ \\pi(y|x) || \\pi_{\\text{ref}}(y|x) \\right]$: the KL divergence between the current and reference policies. Q1: Why does vanilla PPO optimize $\\mathbb{E}_t[ r_t(\\theta)\\hat{A}t ]$, while RLHF optimizes $\\mathbb{E}{x,y}[r(x,y)]$?\nIn standard RL tasks, advantages $\\hat{A}_t$ are often estimated from an offline behavior policy, so the objective takes the form $\\max \\mathbb{E}_t[ r_t(\\theta)\\hat{A}_t ]$. In RLHF for LLMs, we use a reward model to directly score the on-policy outputs from the current LLM, so we do not need importance sampling correction via $r_t(\\theta)$. Q2: What is the difference between $\\pi_{old}$ in vanilla PPO and $\\pi_{\\text{ref}}$ in RLHF PPO?\n$\\pi_{old}$: the policy from the previous iteration; used for data collection and importance sampling; updated continuously. $\\pi_{\\text{ref}}$: a fixed reference policy, typically the frozen SFT model before RLHF; it is not updated. 3.4.2. DPO DPO uses the same high-level objective as PPO:\n$$ \\max L^{RLHF}(\\theta) = \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) ] - \\beta D_{KL} \\left[ \\pi(y|x) || \\pi_{\\text{ref}}(y|x) \\right] $$\nFrom this objective, the optimal policy is:\n$$ \\pi(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y)) $$\n其中，$Z(x) = \\sum\\limits_y \\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))$\nClick to expand the derivation $$ \\begin{align} \u0026amp;\\max\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) ] - \\beta D_{KL} \\left[ \\pi(y|x) || \\pi_{\\text{ref}}(y|x) \\right] \\\\ = \u0026amp;\\max\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) ] - \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\beta \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} ] \\\\ = \u0026amp;\\max\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) - \\beta \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} ] \\\\ = \u0026amp;\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\beta \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - r(x,y) ] \\\\ = \u0026amp;\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\log \\exp(\\frac{1}{\\beta}r(x,y)) ] \\\\ = \u0026amp;\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))}] \\\\ = \u0026amp;\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))\\frac{1}{Z(x)} Z(x)}] \\\\ = \u0026amp;\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))} - \\log Z(x)] \\\\ \\end{align} $$\nwhere $Z(x) = \\sum\\limits_y \\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))$.\nLet $\\pi^\\star(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y)) = \\frac{\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))}{\\sum\\limits_y \\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))}$. Then:\n$$ \\begin{align} \u0026amp;\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))} - \\log Z(x)] \\\\ = \u0026amp;\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi^\\star(y|x)} - \\log Z(x)] \\\\ = \u0026amp;\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi^\\star(y|x)}] \\\\ = \u0026amp;\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [D_{KL} (\\pi(y|x) || \\pi^\\star(y|x))] \\\\ \\end{align} $$\nThe optimum is $\\pi(y|x) = \\pi^\\star(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))$.\nPlugging the optimal policy into the maximum likelihood form of the Bradley–Terry model yields the DPO loss:\n$$ Loss_{DPO} = - \\ln \\sigma \\left( \\beta \\ln \\frac{\\pi(y^+ \\mid x)}{\\pi_{ref}(y^+ \\mid x)} - \\beta \\ln \\frac{\\pi(y^- \\mid x)}{\\pi_{ref}(y^- \\mid x)} \\right) $$\nHere, $y^+$ is the preferred (higher-quality) sample and $y^-$ is the dispreferred (lower-quality) sample.\nClick to expand: Bradley–Terry model Bradley–Terry model:\n$$ P(i\u0026gt;j) = \\frac{\\alpha_i}{\\alpha_i + \\alpha_j} $$\n$P(i\u0026gt;j)$ is the probability that item $i$ beats item $j$. A common loss is\n$$ Loss = -\\mathbb{E}_{(\\alpha_x, \\alpha_y) \\sim D} [ \\ln \\frac{\\alpha_i}{\\alpha_i + \\alpha_j}] $$\nBradley–Terry model for LLM preferences:\n$$ P(y_1\u0026gt;y_2) = \\frac{r(x,y_1)}{r(x,y_1) + r(x,y_2)} $$\nwhere $x$ is the input prompt, $y$ is an output, and $r(x,y)$ is the reward score. To avoid negative values, we exponentiate:\n$$ P(y_1\u0026gt;y_2) = \\frac{\\exp(r(x,y_1))}{\\exp(r(x,y_1)) + \\exp(r(x,y_2))} $$\nThe loss is\n$$ \\begin{align} \\text{Loss} \u0026amp;= - \\mathbb{E}_{(x, y^+, y^-) \\sim D} [ \\ln ( \\frac{\\exp(r(x, y^+))}{\\exp(r(x, y^+)) + \\exp(r(x, y^-))} ) ] \\\\ \u0026amp;= - \\mathbb{E}_{(x, y^+, y^-) \\sim D} [ \\ln ( \\frac{1}{1 + \\exp(r(x, y^-)) - r(x, y^+)} ) ] \\\\ \u0026amp;= - \\mathbb{E}_{(x, y^+, y^-) \\sim D} [ \\ln \\sigma ( r(x, y^+) - r(x, y^-) ) ] \\end{align} $$\nwhere $\\sigma(x) = \\frac{1}{1+\\exp(-x)}$ is the sigmoid.\nClick to expand the derivation $$ \\begin{align} \u0026amp;\\pi(y \\mid x) = \\frac{1}{Z(x)} \\pi_{ref}(y \\mid x) \\exp( \\frac{1}{\\beta} r(x, y)) \\\\ \\Rightarrow \u0026amp;\\exp( \\frac{1}{\\beta} r(x, y) ) = \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} Z(x) \\\\ \\Rightarrow \u0026amp;r(x, y) = \\beta \\ln ( \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} Z(x) ) \\\\ \\Rightarrow \u0026amp;r(x, y) = \\beta \\ln ( \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} ) + \\beta \\ln Z(x) \\\\ \\end{align} $$\nSubstitute into the Bradley–Terry formulation:\n$$ \\begin{align} Loss \u0026amp; = - \\ln \\sigma ( r(x, y^+) - r(x, y^-) ) \\\\ \u0026amp; = - \\ln \\sigma (\\beta \\ln ( \\frac{\\pi(y^+ \\mid x)}{\\pi_{ref}(y^+ \\mid x)} ) + \\beta \\ln Z(x) - \\beta \\ln ( \\frac{\\pi(y^- \\mid x)}{\\pi_{ref}(y^- \\mid x)} ) - \\beta \\ln Z(x)) \\\\ \u0026amp; = - \\ln \\sigma (\\beta \\ln ( \\frac{\\pi(y^+ \\mid x)}{\\pi_{ref}(y^+ \\mid x)} ) - \\beta \\ln ( \\frac{\\pi(y^- \\mid x)}{\\pi_{ref}(y^- \\mid x)} )) \\end{align} $$\nReferences Ouyang L, Wu J, Jiang X, et al. Training language models to follow instructions with human feedback[J]. Advances in neural information processing systems, 2022, 35: 27730-27744. Hu S, Tu Y, Han X, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies[J]. arXiv preprint arXiv:2404.06395, 2024. Zhang T, Ladhak F, Durmus E, et al. Benchmarking large language models for news summarization[J]. Transactions of the Association for Computational Linguistics, 2024, 12: 39-57. stanford-cs336 lecture 15 ","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/sft_rlhf/","summary":"\u003ch2 id=\"1-post-training-three-stages\"\u003e1. Post-Training: Three Stages\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eFrom InstructGPT\u003ca href=\"/cspaulia-blog/en/posts/sft_rlhf/#ref1\"\u003e[1]\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg alt=\"stages\" loading=\"lazy\" src=\"/cspaulia-blog/posts/sft_rlhf/stage.png\"\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCollect data and train a \u003cstrong\u003esupervised\u003c/strong\u003e policy.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eSample a prompt from the prompt dataset.\u003c/li\u003e\n\u003cli\u003eAnnotators label the desired output.\u003c/li\u003e\n\u003cli\u003eUse the labeled data to perform supervised fine-tuning of the LLM.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eCollect \u003cstrong\u003ecomparison data\u003c/strong\u003e and train a \u003cstrong\u003ereward model\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eSample a prompt and multiple model outputs.\u003c/li\u003e\n\u003cli\u003eAnnotators rank these outputs from “best” to “worst”.\u003c/li\u003e\n\u003cli\u003eUse the ranking data to train the reward model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eWith the \u003cstrong\u003etrained reward model\u003c/strong\u003e, optimize the policy using \u003cstrong\u003ereinforcement learning\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eSample a new prompt from the dataset.\u003c/li\u003e\n\u003cli\u003eGenerate an output with the current policy.\u003c/li\u003e\n\u003cli\u003eThe reward model scores the output (Reward).\u003c/li\u003e\n\u003cli\u003eUpdate the policy using PPO (or other RL methods) based on the reward.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"2-building-an-sft-dataset\"\u003e2. Building an SFT Dataset\u003c/h2\u003e\n\u003ch3 id=\"21-issues-in-the-dataset\"\u003e2.1. Issues in the dataset\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAn example from the FLAN dataset:\u003c/p\u003e","title":"SFT and RLHF"},{"content":"Key resources in deep learning Memory (GB): stores parameters, gradients, optimizer states, activations, etc. Compute (FLOPs): number of floating-point operations required for training. 1. Tensor basics and memory management 1.1. Creating and storing tensors A tensor is the basic unit for storing parameters, gradients, optimizer states, data, and activations. PyTorch supports many ways to create tensors (e.g., torch.zeros, torch.ones, torch.randn). x = torch.tensor([[1., 2, 3], [4, 5, 6]]) # @inspect x x = torch.zeros(4, 8) # 4x8 matrix of all zeros @inspect x x = torch.ones(4, 8) # 4x8 matrix of all ones @inspect x x = torch.randn(4, 8) # 4x8 matrix of iid Normal(0, 1) samples @inspect x You can also allocate memory first and then initialize values. x = torch.empty(4, 8) # 4x8 matrix of uninitialized values @inspect x nn.init.trunc_normal_(x, mean=0, std=1, a=-2, b=2) # @inspect x Tensor memory usage is determined by the number of elements and the dtype. 1.2. Common dtypes Parameters, gradients, activations, and optimizer states are almost always stored as floating-point values.\n1.2.1 float32 (single precision) Default dtype, 4 bytes, wide dynamic range.\nMemory is determined by (i) the number of values and (ii) the dtype.\nx = torch.zeros(4, 8) # @inspect x assert x.dtype == torch.float32 # Default type assert x.numel() == 4 * 8 assert x.element_size() == 4 # Float is 4 bytes assert get_memory_usage(x) == 4 * 8 * 4 # 128 bytes 1.2.2 float16 (half precision) 2 bytes. Saves memory but has a smaller dynamic range and is more prone to underflow.\nx = torch.zeros(4, 8, dtype=torch.float16) # @inspect x assert x.element_size() == 2 # Float is 2 bytes Smaller dynamic range (easy to underflow):\nx = torch.tensor([1e-8], dtype=torch.float16) # @inspect x assert x == 0 # Underflow! 1.2.3 bfloat16 2 bytes. Same dynamic range as float32, slightly lower precision.\nLess likely to underflow:\nx = torch.tensor([1e-8], dtype=torch.bfloat16) # @inspect x assert x != 0 # No underflow! Compare dynamic range and memory usage across dtypes:\nfloat32_info = torch.finfo(torch.float32) # @inspect float32_info float16_info = torch.finfo(torch.float16) # @inspect float16_info bfloat16_info = torch.finfo(torch.bfloat16) # @inspect bfloat16_info Output:\nfloat32 info=\u0026#34;finfo(resolution=1e-06,min=-3.40282e+38,max=3.40282e+38, eps=1.19209e-07,smallest normal=1.17549e-38,tiny=1.17549e-38dtype=float32)\u0026#34; float1l6 info=\u0026#34;finfo(resolution=0.001,min=-65504,max=65504, eps=0.000976562,sma1lest norma1=6.10352e-05,tiny=6.10352e-05,dtype=float16)\u0026#34; bfloat16 info=\u0026#34;finfo(resolution=0.01, min=-3.38953e+38,max=3.38953e+38, eps=0.0078125,smallest normal=1.17549e38, tiny=1.17549e-38,dtype=bfloat16)\u0026#34; 1.2.4 fp8 FP8 primer (NVIDIA docs)\n1 byte. Extreme compression, designed for newer hardware (e.g., H100).\nH100 supports two FP8 formats: E4M3 (range [-448, 448]) and E5M2 ([-57344, 57344]).\n1.2.5 Mixed-precision training TODO: expand mixed-precision training notes\nUsing different dtypes comes with trade-offs:\nHigher precision: more accurate and stable, but needs more memory and more compute. Lower precision: less accurate/stable, but reduces memory and compute requirements. A common mixed-precision recipe:\nUse bfloat16/fp8 for forward activations. Keep parameters and gradients in float32. Mixed-precision training paper/PyTorch AMP docs/NVIDIA mixed-precision training docs\n2. Compute resources 2.1. Tensor operations 2.1.1. Storage and views A tensor is a memory pointer + metadata (describing how to index into storage, e.g., strides). PyTorch stride definition\nFor a tensor:\nx = torch.tensor([ [0., 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], ]) To move to the next row (dim 0), you skip 4 elements in storage:\nassert x.stride(0) == 4 To move to the next column (dim 1), you skip 1 element in storage:\nassert x.stride(1) == 1 Indexing into storage using strides:\nr, c = 1, 2 index = r * x.stride(0) + c * x.stride(1) # @inspect index assert index == 6 2.1.2. Tensor slicing Many tensor operations return a different view of the same underlying storage (no copy), so modifying one view can affect another.\nx = torch.tensor([[1., 2, 3], [4, 5, 6]]) # @inspect x Operation 1: get row 0\ndef same_storage(x: torch.Tensor, y: torch.Tensor): return x.untyped_storage().data_ptr() == y.untyped_storage().data_ptr() y = x[0] # @inspect y assert torch.equal(y, torch.tensor([1., 2, 3])) assert same_storage(x, y) Operation 2: get column 1\ny = x[:, 1] # @inspect y assert torch.equal(y, torch.tensor([2, 5])) assert same_storage(x, y) Operation 3: reshape a 2×3 matrix into a 3×2 matrix (view)\ny = x.view(3, 2) # @inspect y assert torch.equal(y, torch.tensor([[1, 2], [3, 4], [5, 6]])) assert same_storage(x, y) Operation 4: transpose the matrix\ny = x.transpose(1, 0) # @inspect y assert torch.equal(y, torch.tensor([[1, 4], [2, 5], [3, 6]])) assert same_storage(x, y) Operation 5: modifying x also modifies y\nx[0][0] = 100 # @inspect x, @inspect y assert y[0][0] == 100 Operation 6: storage contiguity (contiguous)\nSome transforms (e.g., transpose, some view patterns) make a tensor non-contiguous in memory. A non-contiguous tensor often cannot be reshaped with view without copying.\nx = torch.tensor([[1., 2, 3], [4, 5, 6]]) # @inspect x y = x.transpose(1, 0) # @inspect y assert not y.is_contiguous() try: y.view(2, 3) assert False except RuntimeError as e: assert \u0026#34;view size is not compatible with input tensor\u0026#39;s size and stride\u0026#34; in str(e) You can force a tensor to be contiguous, but this allocates new storage.\ny = x.transpose(1, 0).contiguous().view(2, 3) # @inspect y assert not same_storage(x, y) 2.1.3. Tensor elementwise operations (elementwise) These operations apply a function to each element and return a tensor of the same shape.\nx = torch.tensor([1, 4, 9]) assert torch.equal(x.pow(2), torch.tensor([1, 16, 81])) assert torch.equal(x.sqrt(), torch.tensor([1, 2, 3])) assert torch.equal(x.rsqrt(), torch.tensor([1, 1 / 2, 1 / 3])) # i -\u0026gt; 1/sqrt(x_i) assert torch.equal(x + x, torch.tensor([2, 8, 18])) assert torch.equal(x * 2, torch.tensor([2, 8, 18])) assert torch.equal(x / 0.5, torch.tensor([2, 8, 18])) triu constructs the upper-triangular part of a matrix, which is useful for building causal attention masks.\nx = torch.ones(3, 3).triu() # @inspect x assert torch.equal(x, torch.tensor([ [1, 1, 1], [0, 1, 1], [0, 0, 1]], )) 2.1.4. Tensor multiplication x = torch.ones(16, 32) w = torch.ones(32, 2) y = x @ w assert y.size() == torch.Size([16, 2]) In practice, we apply this multiplication per example in the batch and per token position in the sequence.\nx = torch.ones(4, 8, 16, 32) ## [batch, sequence, H, W] w = torch.ones(32, 2) y = x @ w assert y.size() == torch.Size([4, 8, 16, 2]) 2.2. Tensor einops 2.2.1. Why use einops x = torch.ones(2, 2, 3) # batch, sequence, hidden @inspect x y = torch.ones(2, 2, 3) # batch, sequence, hidden @inspect y z = x @ y.transpose(-2, -1) # batch, sequence, sequence @inspect z What do dimensions -2 and -1 mean?\nTensor dimensions are easy to mix up.\neinops is a Python library for naming tensor dimensions and transforming tensors with readable patterns.\neinops docs\n2.2.2. Naming dimensions with jaxtyping How to define tensor dimensions.\nOld approach\nx = torch.ones(2, 2, 1, 3) # batch seq heads hidden @inspect x New approach (jaxtyping)\nx: Float[torch.Tensor, \u0026#34;batch seq heads hidden\u0026#34;] = torch.ones(2, 2, 1, 3) # @inspect x 2.2.3. einops operations Operation 1: einsum einsum is a general-purpose tensor contraction API with a compact, documented notation.\nDefine two tensors\nx: Float[torch.Tensor, \u0026#34;batch seq1 hidden\u0026#34;] = torch.ones(2, 3, 4) # @inspect x y: Float[torch.Tensor, \u0026#34;batch seq2 hidden\u0026#34;] = torch.ones(2, 3, 4) # @inspect y Old approach\nz = x @ y.transpose(-2, -1) # batch, sequence, sequence @inspect z New approach (einops.einsum)\nz = einsum(x, y, \u0026#34;batch seq1 hidden, batch seq2 hidden -\u0026gt; batch seq1 seq2\u0026#34;) # @inspect z z = einsum(x, y, \u0026#34;... seq1 hidden, ... seq2 hidden -\u0026gt; ... seq1 seq2\u0026#34;) # @inspect z Any dimensions that do not appear in the output pattern are reduced (summed over).\nOperation 2: reduce You can reduce a tensor along one or more axes, e.g. sum, mean, max, min.\nDefine a tensor\nx: Float[torch.Tensor, \u0026#34;batch seq hidden\u0026#34;] = torch.ones(2, 3, 4) # @inspect x Old approach\ny = x.mean(dim=-1) # @inspect y New approach (einops.reduce)\ny = reduce(x, \u0026#34;... hidden -\u0026gt; ...\u0026#34;, \u0026#34;sum\u0026#34;) # @inspect y Operation 3: rearrange Sometimes a single axis is a flattened representation of multiple logical axes, and you want to operate on just one of them.\nDefine a tensor\nx: Float[torch.Tensor, \u0026#34;batch seq total_hidden\u0026#34;] = torch.ones(2, 3, 8) # @inspect x Here total_hidden is the flattened representation of heads * hidden1.\nw: Float[torch.Tensor, \u0026#34;hidden1 hidden2\u0026#34;] = torch.ones(4, 4) Split total_hidden into heads and hidden1:\nx = rearrange(x, \u0026#34;... (heads hidden1) -\u0026gt; ... heads hidden1\u0026#34;, heads=2) # @inspect x Merge heads and hidden2 back into a single axis:\nx = rearrange(x, \u0026#34;... heads hidden2 -\u0026gt; ... (heads hidden2)\u0026#34;) # @inspect x 2.3. Tensor operation FLOPs A floating-point operation (FLOP) is a basic arithmetic op such as addition ($x + y$) or multiplication ($x \\cdot y$).\n⚠ WarningTwo extremely confusing abbreviations (they sound the same!):\nFLOPs: the number of floating-point operations (a measure of compute) FLOP/s: floating-point operations per second (also written as FLOPS), a measure of hardware throughput \u0026times; Training GPT-3 (2020) requires 3.14e23 FLOPs\nTraining GPT-4 (2023) requires about 2e25 FLOPs\nA100 peak throughput: 312 teraFLOP/s for torch.bfloat16 or torch.float16; 19.5 teraFLOP/s for torch.float32\nH100 peak throughput: 1979 teraFLOP/s for torch.bfloat16 or torch.float16 (often ~50% lower in practice); 67.5 teraFLOP/s for torch.float32\nWith 8× H100 GPUs, two weeks gives:\ntotal_flops = 8 * (60 * 60 * 24 * 7) * h100_flop_per_sec # @inspect total_flops Output:\ntotal_flops = 4.788e+21 2.3.1. FLOPs calculation Linear layer / matrix multiply: for a $B \\times D$ matrix times a $D \\times K$ matrix, the FLOPs are:\nx = torch.ones(B, D, device=device) w = torch.randn(D, K, device=device) y = x @ w actual_num_flops = 2 * B * D * K # @inspect actual_num_flops For each triple $(i, j, k)$, you do one multiply $(x[i][j] * w[j][k])$ and one add, which is why the factor is 2.\nElementwise ops: an $m \\times n$ tensor costs $O(mn)$ FLOPs.\nAddition: adding two $m \\times n$ tensors costs $mn$ FLOPs.\nIn practice, matrix multiplication dominates FLOPs in deep learning. A good first-order estimate is to count the matmul-like operations.\n2.3.2. Model FLOPs utilization (MFU) Definition: (actual FLOP/s) / (promised FLOP/s), ignoring communication overhead.\nIn practice, MFU $\\ge 0.5$ is already great (and can be higher when matmuls dominate).\n2.3.3. Summary Matrix multiplication dominates: $(2 \\times m \\times n \\times p)$ FLOPs.\nFLOP/s depends on hardware (H100 \u0026raquo; A100) and dtype (bfloat16 \u0026raquo; float32).\nMFU: (actual FLOP/s) / (promised FLOP/s).\n2.4. Gradients and backpropagation 2.4.1. Gradient basics Assume we have a simple linear model:\n$$ y = 0.5 \\cdot (x \\times w - 5)^2 $$\nForward pass: compute the loss\nx = torch.tensor([1., 2, 3]) w = torch.tensor([1., 1, 1], requires_grad=True) # Want gradient pred_y = x @ w loss = 0.5 * (pred_y - 5).pow(2) Backward pass: compute gradients\nloss.backward() assert loss.grad is None assert pred_y.grad is None assert x.grad is None assert torch.equal(w.grad, torch.tensor([1, 2, 3])) 2.4.2. Gradient FLOPs To reason about gradient FLOPs, consider a simple two-layer linear example:\nx = torch.ones(B, D, device=device) w1 = torch.randn(D, D, device=device, requires_grad=True) w2 = torch.randn(D, K, device=device, requires_grad=True) h1 = x @ w1 h2 = h1 @ w2 loss = h2.pow(2).mean() Recall the forward-pass FLOPs:\nMultiply x[i][j] * w1[j][k] Add to h1[i][k] Multiply h1[i][j] * w2[j][k] Add to h2[i][k] num_forward_flops = (2 * B * D * D) + (2 * B * D * K) # @inspect num_forward_flops Backprop path: loss \u0026ndash;\u0026gt; h2 \u0026ndash;\u0026gt; w2 \u0026ndash;\u0026gt; h1 \u0026ndash;\u0026gt; w1 \u0026ndash;\u0026gt; x\nFor parameter $w2$, the chain rule gives:\n$$ \\text{w2.grad} = \\frac{\\partial loss}{\\partial w2} = \\frac{\\partial loss}{\\partial h2} \\cdot \\frac{\\partial h2}{\\partial w2} $$ $$ w2.grad[j,k] = \\frac{\\partial loss}{\\partial w2[j, k]} = \\sum_{i=0}^{N-1} \\frac{\\partial loss}{\\partial h2[i, k]} \\cdot \\frac{\\partial h2[i, k]}{\\partial w2[j, k]} = \\sum_{i=0}^{N-1} h2.grad[i,k] \\cdot h1[i,j] $$\nFor each triple $(i, j, k)$, you do one multiply and one add, so:\nnum_backward_flops += 2 * B * D * K # @inspect num_backward_flops There are four gradient computations in this toy graph, so the total backward FLOPs are:\nnum_backward_flops = (2 + 2) * B * D * K + (2 + 2) * B * D * D # @inspect num_backward_flops i Summary Forward pass: 2 × (# data points) × (# parameters) FLOPs Backward pass: 4 × (# data points) × (# parameters) FLOPs Total: 6 × (# data points) × (# parameters) FLOPs \u0026times; 3. Models 3.1. Model parameters Model parameters are stored as nn.Parameter objects in PyTorch.\ninput_dim = 16384 output_dim = 32 w = nn.Parameter(torch.randn(input_dim, output_dim)) assert isinstance(w, torch.Tensor) # Behaves like a tensor assert type(w.data) == torch.Tensor # Access the underlying tensor 3.1.1. Parameter initialization Assume we randomly initialize the weight matrix w and multiply it with x.\nx = nn.Parameter(torch.randn(input_dim)) output = x @ w # @inspect output assert output.size() == torch.Size([output_dim]) Output:\noutput = [ 18.919979095458984, ... ] Since $output[k] = x \\times w[:, k]$, the magnitude of each output element grows with input_dim.\nIf input_dim is too large, gradients can blow up, making training unstable.\nWe want the initialization scale to be roughly independent of input_dim, so we scale by $1/\\sqrt{\\text{input_dim}}$.\nw = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim)) output = x @ w # @inspect output Output:\noutput = [ -1.5302726030349731, ... ] This is essentially Xavier initialization.\nFor extra safety, we truncate the normal distribution to [-3, 3] to avoid extreme outliers.\nw = nn.Parameter(nn.init.trunc_normal_(torch.empty(input_dim, output_dim), std=1 / np.sqrt(input_dim), a=-3, b=3)) 3.1.2. Building a model Using a simple linear model as an example:\nclass Linear(nn.Module): \u0026#34;\u0026#34;\u0026#34;Simple linear layer.\u0026#34;\u0026#34;\u0026#34; def __init__(self, input_dim: int, output_dim: int): super().__init__() self.weight = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim)) def forward(self, x: torch.Tensor) -\u0026gt; torch.Tensor: return x @ self.weight class Cruncher(nn.Module): def __init__(self, dim: int, num_layers: int): super().__init__() self.layers = nn.ModuleList([ Linear(dim, dim) for i in range(num_layers) ]) self.final = Linear(dim, 1) def forward(self, x: torch.Tensor) -\u0026gt; torch.Tensor: # Apply linear layers B, D = x.size() for layer in self.layers: x = layer(x) # Apply final head x = self.final(x) assert x.size() == torch.Size([B, 1]) # Remove the last dimension x = x.squeeze(-1) assert x.size() == torch.Size([B]) return x B = 8 # Batch size x = torch.randn(B, D, device=device) y = model(x) assert y.size() == torch.Size([B]) Model parameters:\nparam_sizes = [ (name, param.numel()) for name, param in model.state_dict().items() ] assert param_sizes == [ (\u0026#34;layers.0.weight\u0026#34;, D * D), (\u0026#34;layers.1.weight\u0026#34;, D * D), (\u0026#34;final.weight\u0026#34;, D), ] num_parameters = get_num_parameters(model) assert num_parameters == (D * D) + (D * D) + D 3.2. Model training 3.2.1. Randomness Randomness appears in many places: parameter init, dropout, data shuffling, etc. For reproducibility, set seeds explicitly whenever you rely on randomness. Determinism is especially useful for debugging, because it makes issues easier to reproduce. In practice you should set seeds in three places (PyTorch, NumPy, Python\u0026rsquo;s random). # Torch seed = 0 torch.manual_seed(seed) # NumPy import numpy as np np.random.seed(seed) # Python import random random.seed(seed) 3.2.2. Data loading In language models, data can be represented as sequences of integers (tokens).\nYou can serialize sequences with a NumPy array:\norig_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.int32) orig_data.tofile(\u0026#34;data.npy\u0026#34;) You can load the data with NumPy.\nIf you don\u0026rsquo;t want to load the entire dataset into memory (some datasets are huge; e.g., LLaMA can be ~2.8TB), you can use memmap to map only the accessed parts.\ndata = np.memmap(\u0026#34;data.npy\u0026#34;, dtype=np.int32) assert np.array_equal(data, orig_data) A dataloader generates a batch of training data:\ndef get_batch(data: np.array, batch_size: int, sequence_length: int, device: str) -\u0026gt; torch.Tensor: # Sample batch_size random positions into data. start_indices = torch.randint(len(data) - sequence_length, (batch_size,)) assert start_indices.size() == torch.Size([batch_size]) # Index into the data. x = torch.tensor([data[start:start + sequence_length] for start in start_indices]) assert x.size() == torch.Size([batch_size, sequence_length]) # Pinned memory if torch.cuda.is_available(): x = x.pin_memory() x = x.to(device, non_blocking=True) return x B = 2 # Batch size L = 4 # Length of sequence x = get_batch(data, batch_size=B, sequence_length=L, device=get_device()) assert x.size() == torch.Size([B, L]) By default, CPU tensors live in paged memory. You can explicitly pin them via x = x.pin_memory().\nThis allows two tasks to overlap:\nFetch the next batch on the CPU Process the current x on the GPU 3.2.3. Optimizer We\u0026rsquo;ll use the familiar linear example again.\nB = 2 D = 4 num_layers = 2 model = Cruncher(dim=D, num_layers=num_layers).to(get_device()) Define an AdaGrad optimizer\nmomentum = SGD + exponential averaging of grad AdaGrad = SGD + averaging by grad^2 RMSProp = AdaGrad + exponentially averaging of grad^2 Adam = RMSProp + momentum AdaGrad\nclass AdaGrad(torch.optim.Optimizer): def __init__(self, params: Iterable[nn.Parameter], lr: float = 0.01): super(AdaGrad, self).__init__(params, dict(lr=lr)) def step(self): for group in self.param_groups: lr = group[\u0026#34;lr\u0026#34;] for p in group[\u0026#34;params\u0026#34;]: # Optimizer state state = self.state[p] grad = p.grad.data # Get squared gradients g2 = sum_{i\u0026lt;t} g_i^2 g2 = state.get(\u0026#34;g2\u0026#34;, torch.zeros_like(grad)) # Update optimizer state g2 += torch.square(grad) state[\u0026#34;g2\u0026#34;] = g2 # Update parameters p.data -= lr * grad / torch.sqrt(g2 + 1e-5) optimizer = AdaGrad(model.parameters(), lr=0.01) state = model.state_dict() # @inspect state Compute gradients\nx = torch.randn(B, D, device=get_device()) y = torch.tensor([4., 5.], device=get_device()) pred_y = model(x) loss = F.mse_loss(input=pred_y, target=y) loss.backward() Run one optimizer step\noptimizer.step() state = model.state_dict() # @inspect state Free memory (optional)\noptimizer.zero_grad(set_to_none=True) Memory accounting Memory for parameters\ndef get_num_parameters(model: nn.Module) -\u0026gt; int: return sum(param.numel() for param in model.parameters()) num_parameters = (D * D * num_layers) + D # @inspect num_parameters assert num_parameters == get_num_parameters(model) Output\nnum_parameters = 36 Memory for activations\nnum_activations = B * D * num_layers # @inspect num_activations Output\nnum_parameters = 16 Memory for gradients\nnum_gradients = num_parameters # @inspect num_gradients Output\nnum_parameters = 36 Memory for optimizer state\nnum_optimizer_states = num_parameters # @inspect num_optimizer_states Output\nnum_parameters = 36 Total memory (assuming float32 storage, 4 bytes)\ntotal_memory = 4 * (num_parameters + num_activations + num_gradients + num_optimizer_states) # @inspect total_memory Output\nnum_parameters = 496 3.2.4. Training loop def train(name: str, get_batch, D: int, num_layers: int, B: int, num_train_steps: int, lr: float): model = Cruncher(dim=D, num_layers=0).to(get_device()) optimizer = SGD(model.parameters(), lr=0.01) for t in range(num_train_steps): # Get data x, y = get_batch(B=B) # Forward (compute loss) pred_y = model(x) loss = F.mse_loss(pred_y, y) # Backward (compute gradients) loss.backward() # Update parameters optimizer.step() optimizer.zero_grad(set_to_none=True) train(\u0026#34;simple\u0026#34;, get_batch, D=D, num_layers=0, B=4, num_train_steps=10, lr=0.01) 3.2.5. Checkpointing During training, periodically saving the model and optimizer state to disk is very useful.\ncheckpoint = { \u0026#34;model\u0026#34;: model.state_dict(), \u0026#34;optimizer\u0026#34;: optimizer.state_dict(), } torch.save(checkpoint, \u0026#34;model_checkpoint.pt\u0026#34;) Load a checkpoint\nloaded_checkpoint = torch.load(\u0026#34;model_checkpoint.pt\u0026#34;) 3.2.6. Mixed-precision training Choosing the dtype (float32, bfloat16, fp8) involves trade-offs: Higher precision: more accurate/stable, more memory, more compute Lower precision: less accurate/stable, less memory, less compute How do we get the best of both? Typical approach: default to float32, but use {bfloat16, fp8} where possible A common plan: Use {bfloat16, fp8} in the forward pass (activations) Keep the rest in float32 (parameters, gradients) Mixed-precision training PyTorch automatic mixed precision (AMP) NVIDIA\u0026rsquo;s Transformer Engine supports FP8 for linear layers and is widely used in training; see FP8 References stanford-cs336 lecture 2 ","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/primitives/","summary":"\u003ch2 id=\"key-resources-in-deep-learning\"\u003eKey resources in deep learning\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMemory\u003c/strong\u003e (GB): stores parameters, gradients, optimizer states, activations, etc.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCompute\u003c/strong\u003e (FLOPs): number of floating-point operations required for training.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-tensor-basics-and-memory-management\"\u003e1. Tensor basics and memory management\u003c/h2\u003e\n\u003ch3 id=\"11-creating-and-storing-tensors\"\u003e1.1. Creating and storing tensors\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eA tensor is the basic unit for storing parameters, gradients, optimizer states, data, and activations.\u003c/li\u003e\n\u003cli\u003ePyTorch supports many ways to create tensors (e.g., \u003ccode\u003etorch.zeros\u003c/code\u003e, \u003ccode\u003etorch.ones\u003c/code\u003e, \u003ccode\u003etorch.randn\u003c/code\u003e).\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etensor\u003c/span\u003e\u003cspan class=\"p\"\u003e([[\u003c/span\u003e\u003cspan class=\"mf\"\u003e1.\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e6\u003c/span\u003e\u003cspan class=\"p\"\u003e]])\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# @inspect x\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ezeros\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# 4x8 matrix of all zeros @inspect x\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eones\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# 4x8 matrix of all ones @inspect x\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erandn\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# 4x8 matrix of iid Normal(0, 1) samples @inspect x\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003eYou can also allocate memory first and then initialize values.\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eempty\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# 4x8 matrix of uninitialized values @inspect x\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einit\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etrunc_normal_\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003emean\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003estd\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ea\u003c/span\u003e\u003cspan class=\"o\"\u003e=-\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# @inspect x\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003eTensor memory usage is determined by \u003cstrong\u003ethe number of elements\u003c/strong\u003e and \u003cstrong\u003ethe dtype\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"12-common-dtypes\"\u003e1.2. Common dtypes\u003c/h3\u003e\n\u003cp\u003eParameters, gradients, activations, and optimizer states are almost always stored as floating-point values.\u003c/p\u003e","title":"Training Primitives and Resource Accounting"},{"content":"👉 Try it online: Tokenization Visualization Tool\nWe represent raw text as a Unicode string.\nstring = \u0026#34;Hello, 🌍! 你好!\u0026#34; Language models operate on a sequence of tokens (usually represented by integer IDs) and model a probability distribution over them.\nindices = [15496, 11, 995, 0] We need:\n✅ A method to encode a string into tokens ✅ A method to decode tokens back into a string class Tokenizer: def encode(self, string: str) -\u0026gt; list[int]: ... def decode(self, indices: list[int]) -\u0026gt; str: ... vocab_size: the vocabulary size, i.e., the total number of possible token IDs. 1. Character-based tokenization 1.1. Unicode overview A universal standard for character encoding (around 150,000 characters) ord(char): get the decimal code point of a character ord(\u0026#34;h\u0026#34;) # 104 ord(\u0026#34;😊\u0026#34;) # 128522 1.2. Encoding and decoding class CharacterTokenizer(Tokenizer): \u0026#34;\u0026#34;\u0026#34;Represent a string as a sequence of Unicode code points.\u0026#34;\u0026#34;\u0026#34; def encode(self, string: str) -\u0026gt; list[int]: return list(map(ord, string)) def decode(self, indices: list[int]) -\u0026gt; str: return \u0026#34;\u0026#34;.join(map(chr, indices)) Example:\ntokenizer = CharacterTokenizer() string = \u0026#34;Hello, 🌍! 你好!\u0026#34; # @inspect string indices = tokenizer.encode(string) # @inspect indices reconstructed_string = tokenizer.decode(indices) # @inspect reconstructed_string Output:\nstring = \u0026#34;Hello, 🌍! 你好!\u0026#34; indices = [72, 101, 108, 108, 111, 44, 32, 127757, 33, 32, 20320, 22909, 33] reconstructed_string = \u0026#34;Hello, 🌍! 你好!\u0026#34; 1.3. Limitations Issue 1: the vocabulary can become very large.\nIssue 2: many characters occur very rarely (e.g., 🌍), which is inefficient for vocabulary usage.\ndef get_compression_ratio(string: str, indices: list[int]) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;Return the number of UTF-8 bytes per token for a tokenized string.\u0026#34;\u0026#34;\u0026#34; num_bytes = len(bytes(string, encoding=\u0026#34;utf-8\u0026#34;)) # @inspect num_bytes num_tokens = len(indices) # @inspect num_tokens return num_bytes / num_tokens vocabulary_size = max(indices) + 1 # This is a lower bound @inspect vocabulary_size compression_ratio = get_compression_ratio(string, indices) # @inspect compression_ratio Output:\nvocabulary_size = 127758 num_bytes = 20 num_tokens = 13 compression_ratio = 1.5384615384615385 2. Byte-based tokenization A Unicode string can be represented as a sequence of bytes. Each byte (8 bits) is a number from 0 to 255.\nThe most common Unicode encoding is UTF-8.\nInput:\nbytes(\u0026#34;a\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) bytes(\u0026#34;🌍\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) Output:\nb\u0026#34;a\u0026#34; # one byte b\u0026#34;\\xf0\\x9f\\x8c\\x8d\u0026#34; # multiple bytes 2.1. Encoding and decoding class ByteTokenizer(Tokenizer): \u0026#34;\u0026#34;\u0026#34;Represent a string as a sequence of bytes.\u0026#34;\u0026#34;\u0026#34; def encode(self, string: str) -\u0026gt; list[int]: string_bytes = string.encode(\u0026#34;utf-8\u0026#34;) # @inspect string_bytes indices = list(map(int, string_bytes)) # @inspect indices return indices def decode(self, indices: list[int]) -\u0026gt; str: string_bytes = bytes(indices) # @inspect string_bytes string = string_bytes.decode(\u0026#34;utf-8\u0026#34;) # @inspect string return string Example:\ntokenizer = ByteTokenizer() string = \u0026#34;Hello, 🌍! 你好!\u0026#34; # @inspect string indices = tokenizer.encode(string) # @inspect indices reconstructed_string = tokenizer.decode(indices) # @inspect reconstructed_string Output:\nstring = \u0026#34;Hello, 🌍! 你好!\u0026#34; string_bytes = \u0026#34;b\u0026#39;Hello, \\\\xf0\\\\x9f\\\\x8c\\\\x8d!\\\\xe4\\\\xbd\\\\xa0\\\\xe5\\\\xa5\\\\xbd\u0026#39;\u0026#34; indices = [72, 101, 108, 108, 111, 44, 32, 240, 159, 140, 141, 33, 32, 228, 189, 160, 229, 165, 189, 33] reconstructed_string = \u0026#34;Hello, 🌍! 你好!\u0026#34; 2.2. Limitations Issue 1: although the vocabulary is tiny (only 256), sequences become much longer. In Transformers, compute scales quadratically with sequence length.\nvocabulary_size = 256 # This is a lower bound @inspect vocabulary_size compression_ratio = get_compression_ratio(string, indices) # @inspect compression_ratio Output:\nnum_bytes = 20 num_tokens = 20 compression_ratio = 1.0 3. Word-based tokenization Split a string using a traditional NLP-style word segmentation. Input:\nstring = \u0026#34;I\u0026#39;ll say supercalifragilisticexpialidocious!\u0026#34; segments = regex.findall(r\u0026#34;\\w+|.\u0026#34;, string) # @inspect segments Output:\nsegments = [\u0026#34;I\u0026#34;, \u0026#34;ll\u0026#34;, \u0026#34;say\u0026#34;, \u0026#34;supercalifragilisticexpialidocious\u0026#34;, \u0026#34;!\u0026#34;] 3.1. Encoding and decoding To turn this into a Tokenizer, we need to map each segment to an integer ID. Build a mapping from each segment to an integer. 3.2. Limitations The number of words is enormous. Many words are rare, so the model learns very little from them. It does not naturally provide a fixed-size vocabulary. 4. Byte Pair Encoding (BPE) Core idea: train a tokenizer on raw text and automatically build a vocabulary.\nMotivation: represent frequent character/byte sequences with a single token, and represent rare sequences with multiple tokens.\nHigh-level algorithm: treat each byte as a token first, then repeatedly merge the most frequent adjacent token pair into a new token.\n4.1. Merge procedure def merge(indices: list[int], pair: tuple[int, int], new_index: int) -\u0026gt; list[int]: # @inspect indices, @inspect pair, @inspect new_index \u0026#34;\u0026#34;\u0026#34;Return `indices`, but with all instances of `pair` replaced with `new_index`.\u0026#34;\u0026#34;\u0026#34; new_indices = [] # @inspect new_indices i = 0 # @inspect i while i \u0026lt; len(indices): if i + 1 \u0026lt; len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]: new_indices.append(new_index) i += 2 else: new_indices.append(indices[i]) i += 1 return new_indices 4.2. A BPE tokenizer class BPETokenizer(Tokenizer): \u0026#34;\u0026#34;\u0026#34;BPE tokenizer given a set of merges and a vocabulary.\u0026#34;\u0026#34;\u0026#34; def __init__(self, params: BPETokenizerParams): self.params = params def encode(self, string: str) -\u0026gt; list[int]: indices = list(map(int, string.encode(\u0026#34;utf-8\u0026#34;))) # @inspect indices # Note: this is a very slow implementation for pair, new_index in self.params.merges.items(): # @inspect pair, @inspect new_index indices = merge(indices, pair, new_index) return indices def decode(self, indices: list[int]) -\u0026gt; str: bytes_list = list(map(self.params.vocab.get, indices)) # @inspect bytes_list string = b\u0026#34;\u0026#34;.join(bytes_list).decode(\u0026#34;utf-8\u0026#34;) # @inspect string return string 4.3. Training BPE def train_bpe(string: str, num_merges: int) -\u0026gt; BPETokenizerParams: # @inspect string, @inspect num_merges # Start with the list of bytes of string. indices = list(map(int, string.encode(\u0026#34;utf-8\u0026#34;))) # @inspect indices merges: dict[tuple[int, int], int] = {} # index1, index2 =\u0026gt; merged index vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)} # index -\u0026gt; bytes for i in range(num_merges): # Count the number of occurrences of each pair of tokens counts = defaultdict(int) for index1, index2 in zip(indices, indices[1:]): # For each adjacent pair counts[(index1, index2)] += 1 # @inspect counts # Find the most common pair. pair = max(counts, key=counts.get) # @inspect pair index1, index2 = pair # Merge that pair. new_index = 256 + i # @inspect new_index merges[pair] = new_index # @inspect merges vocab[new_index] = vocab[index1] + vocab[index2] # @inspect vocab indices = merge(indices, pair, new_index) # @inspect indices return BPETokenizerParams(vocab=vocab, merges=merges) Example:\ntext = \u0026#34;the cat in the hat\u0026#34; # @inspect string params = train_bpe(text, num_merges=3) string = \u0026#34;the quick brown fox\u0026#34; # @inspect string tokenizer = BPETokenizer(params) indices = tokenizer.encode(string) # @inspect indices reconstructed_string = tokenizer.decode(indices) # @inspect reconstructed_string What the code is doing:\nInitialize the vocabulary with IDs 0–255 for bytes. Repeatedly find the most frequent adjacent byte pair. (116, 104) → 256, i.e., (\u0026rsquo;t\u0026rsquo;, \u0026lsquo;h\u0026rsquo;) → \u0026rsquo;th\u0026rsquo; (256, 101) → 257, i.e., (\u0026rsquo;th\u0026rsquo;, \u0026rsquo;e\u0026rsquo;) → \u0026rsquo;the' (257, 32) → 258, i.e., (\u0026rsquo;the\u0026rsquo;, \u0026rsquo; \u0026lsquo;) → \u0026rsquo;the ' The vocabulary size becomes 259. Encode strings using the learned merges/vocabulary. References stanford-cs336 lecture 1 ","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/tokenization/","summary":"\u003cp\u003e👉 Try it online: \u003ca href=\"https://tiktokenizer.vercel.app\"\u003eTokenization Visualization Tool\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eWe represent raw text as a Unicode string.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003estring\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Hello, 🌍! 你好!\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eLanguage models operate on a sequence of tokens (usually represented by integer IDs) and model a probability distribution over them.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eindices\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e15496\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e11\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e995\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWe need:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e✅ A method to \u003cstrong\u003eencode a string into tokens\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e✅ A method to \u003cstrong\u003edecode tokens back into a string\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eTokenizer\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eencode\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003estring\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003elist\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"o\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003edecode\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eindices\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003elist\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"o\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003ccode\u003evocab_size\u003c/code\u003e: the vocabulary size, i.e., the total number of possible token IDs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-character-based-tokenization\"\u003e1. Character-based tokenization\u003c/h2\u003e\n\u003ch3 id=\"11-unicode-overview\"\u003e1.1. Unicode overview\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eA universal standard for character encoding (around 150,000 characters)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eord(char)\u003c/code\u003e: get the decimal code point of a character\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eord\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;h\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e     \u003cspan class=\"c1\"\u003e# 104\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eord\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;😊\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e    \u003cspan class=\"c1\"\u003e# 128522\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"12-encoding-and-decoding\"\u003e1.2. Encoding and decoding\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eCharacterTokenizer\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTokenizer\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u0026#34;Represent a string as a sequence of Unicode code points.\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eencode\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003estring\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003elist\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e]:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"nb\"\u003elist\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003emap\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003eord\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003estring\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003edecode\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eindices\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003elist\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"nb\"\u003eint\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejoin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003emap\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003echr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eindices\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eExample\u003c/strong\u003e:\u003c/p\u003e","title":"Tokenization"},{"content":"Classification metrics Core concepts: TP / TN / FP / FN\nSuppose the task is “detect whether someone is sick”. The model prediction vs. reality can be summarized as:\nReality \\ Prediction Predicted positive (sick) Predicted negative (not sick) Actually positive (sick) ✅ TP (true positive) ❌ FN (false negative) Actually negative (not sick) ❌ FP (false positive) ✅ TN (true negative) TP (True Positive): predict “sick”, actually “sick” → correct TN (True Negative): predict “not sick”, actually “not sick” → correct FP (False Positive): predict “sick”, actually “not sick” → false alarm FN (False Negative): predict “not sick”, actually “sick” → miss (often more severe) Accuracy Overall fraction of correct predictions $$ Accuracy = \\frac{(TP + TN)}{(TP + TN + FP + FN)} $$\nRecall Among all true positives, the fraction that the model successfully identifies (don’t miss) $$ Recall = \\frac{TP}{(TP + FN)} $$\nPrecision Among all predicted positives, the fraction that are truly positive (don’t accuse) $$ Precision = \\frac{TP}{(TP + FP)} $$\nF1-score Harmonic mean of precision and recall $$ F1 = \\frac{2 \\times (Precision \\times Recall)}{(Precision + Recall)} $$\nMulti-class variants Macro-F1: arithmetic mean of per-class F1. Micro-F1: compute global TP/FP/FN then F1. Weighted-F1: weighted average of per-class F1 by class frequency. Ranking quality metrics Average Precision (AP) One sentence: AP is the average precision for a single class (commonly explained via object detection).\n1) Sort predicted boxes by confidence (high → low) Each prediction has:\nbounding box confidence score predicted label Sort by confidence descending.\n2) Mark each prediction as TP or FP Iterate predictions in that order:\nIf it matches an unmatched ground-truth box with IoU ≥ a threshold (e.g., 0.5), mark as TP. Otherwise mark as FP. 3) Compute cumulative precision \u0026amp; recall As you traverse predictions, keep cumulative counts:\ncumulative TP count: $TP_i$ cumulative FP count: $FP_i$ Then:\nprecision_i = TP_i / (TP_i + FP_i) recall_i = TP_i / GT_total # GT_total = number of ground-truth boxes 4) Plot the PR curve \u0026amp; compute area Approximate the integral:\n# Recall, Precision are sorted by recall ascending AP = 0 for i in range(1, len(recall)): AP += precision[i] * (recall[i] - recall[i-1]) mAP (mean Average Precision) Mean of AP over classes:\n$$ mAP = \\sum_i AP_i $$\nmAP@IoU=0.5 (mAP@0.5) A prediction is counted as TP only when IoU ≥ 0.5. Compute AP per class, then average → mAP@0.5. mAP@0.5:0.95 Sweep IoU thresholds from 0.5 to 0.95 with step 0.05 (10 thresholds). Average the AP across these thresholds. mAP@k Common in image retrieval / recommendation / multi-label ranking:\nFor each query, compute AP using only the top-$k$ retrieved results, then average across queries.\nExample If you search “dog” and the system returns top 10 images:\n6 are dogs, 4 are not, and dogs appear at ranks 1, 2, 3, 6, 7, 9 Compute precision at those dog positions and average → AP@10 Average AP@10 across all queries → mAP@10 Metrics for image captioning SPICE Why SPICE? Traditional metrics:\nBLEU: focuses on n-gram overlap (like machine translation) ROUGE: emphasizes recall (often used for summarization) CIDEr: TF-IDF weighted n-gram similarity They largely measure word/phrase overlap, but may miss whether the semantic structure is correct.\nSPICE is motivated by:\n“When humans judge captions, they care whether you mention the right objects, attributes, and relations.”\nHow SPICE works Key idea: parse sentences into a semantic graph (scene graph): objects + attributes + relations, then compare candidate vs. references.\nScene graph structure A sentence is represented as a set of triples:\nG = {object, attribute, relation} Example:\nSentence: \u0026#34;A red car is parked beside a white house\u0026#34; G = { (object: car), (object: house), (attribute: car, red), (attribute: house, white), (relation: car, beside, house) } Mathematical definition Inputs:\n$G$: semantic triples from the generated sentence (candidate graph) $R$: semantic triples from all reference sentences (reference graph) Goal:\nCompute the $F_1$ score between $G$ and $R$ Set matching:\nPrecision: $$ P = \\frac{|\\mathcal{G} \\cap \\mathcal{R}|}{|\\mathcal{G}|} $$\nRecall: $$ R = \\frac{|\\mathcal{G} \\cap \\mathcal{R}|}{|\\mathcal{R}|} $$\nF1-score: $$ F_1 = \\frac{2PR}{P + R} $$\nSo: $\\text{SPICE} = F_1(G, R)$\nSPICE can be computed for sub-categories, such as:\nobject matching (object F1) attribute matching (attribute F1) relation matching (relation F1) finer types like color, count, size, etc. The final SPICE score is a weighted average:\n$$ ext{SPICE}=\\sum_i w_i \\cdot F_1^i $$\nwhere $F_1^i$ is the F1 for a semantic category and $w_i$ is its weight.\n","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/metric/","summary":"\u003ch2 id=\"classification-metrics\"\u003eClassification metrics\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCore concepts: TP / TN / FP / FN\u003c/p\u003e\n\u003cp\u003eSuppose the task is “detect whether someone is sick”. The model prediction vs. reality can be summarized as:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eReality \\ Prediction\u003c/th\u003e\n          \u003cth\u003ePredicted positive (sick)\u003c/th\u003e\n          \u003cth\u003ePredicted negative (not sick)\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eActually positive (sick)\u003c/td\u003e\n          \u003ctd\u003e✅ TP (true positive)\u003c/td\u003e\n          \u003ctd\u003e❌ FN (false negative)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eActually negative (not sick)\u003c/td\u003e\n          \u003ctd\u003e❌ FP (false positive)\u003c/td\u003e\n          \u003ctd\u003e✅ TN (true negative)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTP (True Positive)\u003c/strong\u003e: predict “sick”, actually “sick” → correct\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTN (True Negative)\u003c/strong\u003e: predict “not sick”, actually “not sick” → correct\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFP (False Positive)\u003c/strong\u003e: predict “sick”, actually “not sick” → false alarm\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFN (False Negative)\u003c/strong\u003e: predict “not sick”, actually “sick” → miss (often more severe)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch3 id=\"accuracy\"\u003eAccuracy\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eOverall fraction of correct predictions\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\nAccuracy = \\frac{(TP + TN)}{(TP + TN + FP + FN)}\n$$\u003c/p\u003e","title":"100 Evaluation Metrics (Work in Progress)"},{"content":"GPT-1 Motivation It was unclear which training objective could best learn text representations that transfer well across tasks. (Intuitively: for different NLP tasks, we don\u0026rsquo;t know which objective works best.)\nMethod A semi-supervised setup Transformer (a more structured memory for handling long-range dependencies) \u001e strong transfer performance 1. Unsupervised pre-training Objective:\nGiven an unsupervised corpus of tokens $U = \\{u_1, \\cdots , u_n\\}$,\n$$ L_1(U) = \\sum_i \\log{P(u_i | u_{i-k}, \\cdots, u_{i-1}; \\Theta)},\\quad i \\in \\{1,\\cdots, n\\} $$\n$k$ is the context window size; a neural network with parameters $\\Theta$ models the conditional probability. $P(u_i | u_{i-k}, \\cdots, u_{i-1}; \\Theta)$ is the probability of token $u_i$ given the previous $k$ tokens. Since GPT is trained with an unsupervised language modeling objective, it learns to predict the next token given the context. Maximizing the summed log-likelihood above yields an objective that can be applied broadly.\nArchitecture:\nA multi-layer Transformer decoder\n$$ \\begin{aligned} h_0 \u0026amp;= UW_e + W_p \\\\ h_i \u0026amp;= \\text{transformer block}(h_{i-1}) \\\\ P(u) \u0026amp;= \\text{softmax}(h_n W_e^T) \\end{aligned} $$\n$W_e$ is the token embedding matrix $W_p$ is the position embedding matrix 2. Supervised fine-tuning Assume a labeled dataset that contains:\nan input token sequence $x^1, \\cdots, x^m$ a label $y$ Let $h_l^m$ be the activation from the last Transformer block. Then:\n$$ P(y|x^1, \\dots, x^m) = \\text{softmax}(h_l^m W_y) $$\n$$ L_2(\\hat{C}) = \\sum_{(x, y)} \\log P(y|x^1, \\dots, x^m) $$\nSimilar to $L_1$, but now maximizing the label likelihood. The final objective is:\n$$ L_3(\\hat{C}) = L_2(\\hat{C}) + \\lambda * L_1(\\hat{C}) $$\nSo the fine-tuning objective combines $L_1$ and $L_2$.\n3. Task-specific input transformations For similarity tasks, because GPT is a unidirectional model (generating tokens left-to-right), the order of Text 1 and Text 2 matters. You can feed different orderings and average the resulting similarity scores.\nGPT-2 Motivation A common way to build ML systems is to collect a dataset for training. However, training on a single domain-specific dataset often harms generalization.\nMethod Multitask learning: language models can perform downstream tasks in a zero-shot setting without changing parameters or architecture.\nPre-training + supervised fine-tuning\nModel the objective as $p(\\text{output}|\\text{input}, \\text{task})$, written as a sequence: {task (as a prompt), input, output}\nExample 1 (translation): (translate to French, English text, French text) Example 2 (reading comprehension): (answer the question, document, question, answer) Training data Collected Reddit posts with at least 3 karma: 45M links \u001e 8M documents, ~40GB of text Model Roughly similar to GPT.\nGPT-3 Motivation Even when tasks are unknown in advance, most language models still require task-specific datasets and task-specific fine-tuning.\nNeed large labeled datasets tailored to each task Strong performance on a fine-tuning set does not necessarily mean strong generalization Method Meta-learning: train a model that generalizes well\nIn-context learning: given a few training examples at inference time, do not update the model weights (intuitively: include demonstrations in the prompt)\nzero-shot one-shot few-shot Model \u0026amp; architecture Same model and architecture as GPT-2 Sparse Transformer 8 different sizes References GPT, GPT-2, GPT-3 paper walkthrough (Bilibili) Improving language understanding by generative pre-training Language models are unsupervised multitask learners Language Models are Few-Shot Learners ","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/gpt/","summary":"\u003ch2 id=\"gpt-1\"\u003eGPT-1\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eIt was unclear which training objective could best learn text representations that transfer well across tasks. (Intuitively: for different NLP tasks, we don\u0026rsquo;t know which objective works best.)\u003c/p\u003e\n\u003ch3 id=\"method\"\u003eMethod\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eA semi-supervised setup\u003c/li\u003e\n\u003cli\u003eTransformer (a more structured memory for handling long-range dependencies) \u001e strong transfer performance\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"1-unsupervised-pre-training\"\u003e1. Unsupervised pre-training\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eObjective:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eGiven an unsupervised corpus of tokens $U = \\{u_1, \\cdots , u_n\\}$,\u003c/p\u003e\n\u003cp\u003e$$\nL_1(U) = \\sum_i \\log{P(u_i | u_{i-k}, \\cdots, u_{i-1}; \\Theta)},\\quad i \\in \\{1,\\cdots, n\\}\n$$\u003c/p\u003e","title":"GPT Series"},{"content":"LoRA LoRA is a parameter-efficient fine-tuning method for large language models, introduced in the paper LoRA: Low-Rank Adaptation of Large Language Models. With LoRA, the number of trainable parameters can be reduced to around $10^{-4}$ of the full model; GPU memory usage can drop by about 2/3, and it does not add extra inference latency.\nKey ideas Low-rank decomposition: instead of updating the full pretrained weights, learn a low-rank update Parameter efficiency: drastically reduces the number of trainable parameters Keep base weights frozen: the original pretrained weights stay unchanged; only the added low-rank matrices are trained How it works 1. The idea of parameter-efficient fine-tuning Full fine-tuning\nFor a language model, we initialize the model with pretrained parameters $\\Phi_0$ and update parameters by maximizing the conditional language modeling likelihood, i.e., learn $\\Phi_0 + \\Delta \\Phi$:\n$$ \\max_{\\Phi} \\sum_{(x,y)\\in\\mathcal{Z}} \\sum^{\\lvert y \\rvert}_{t=1} \\log{(P^{\\Phi}(y_t|x,y \u0026lt; t))} $$\nThe main downside is that the learned update $\\Delta \\Phi$ has the same dimensionality as the original parameters $\\Phi_0$, which is expensive in compute and memory. This is typically called full fine-tuning.\nParameter-efficient fine-tuning\nA common idea is to represent the update with a much smaller set of parameters: $\\Delta \\Phi = \\Delta \\Phi(\\Theta)$, where $\\lvert \\Theta \\rvert \\ll \\lvert \\Phi_0 \\rvert$. The optimization objective becomes:\n$$ \\max_{\\Theta} \\sum_{(x,y)\\in\\mathcal{Z}} \\sum^{\\lvert y \\rvert}_{t=1} \\log{(P^{\\Phi_0 + \\Delta \\Phi(\\Theta)}(y_t|x,y \u0026lt; t))} $$\nThis family of methods is called parameter-efficient fine-tuning (PEFT). There are many approaches (e.g., Adapters, prefix-tuning, etc.). LoRA encodes the update using low-rank matrices; compared with some alternatives, it does not add inference latency and is easy to optimize.\n2. 数学表达 The work by Aghajanyan et al. suggests that pretrained models can have a small intrinsic dimension: there may exist a very low-dimensional subspace such that fine-tuning within it can achieve comparable performance to full-space fine-tuning.\nGiven an original weight matrix $W_{\\theta} \\in \\mathbb{R}^{d \\times k}$, LoRA updates it as:\n$$ W = W_{\\theta} + BA $$\nwhere:\n$B \\in \\mathbb{R}^{d \\times r}$，初始化为0矩阵 $A \\in \\mathbb{R}^{r \\times k}$，初始化为高斯分布矩阵 $r \\ll \\min{(k, d)}$，r 是秩 (rank),通常远小于 d 和 k 初始化$W = W_{\\theta} + \\mathbf{0} \\times A = W_{\\theta}$ In words:\n$B \\in \\mathbb{R}^{d \\times r}$ is initialized as a zero matrix $A \\in \\mathbb{R}^{r \\times k}$ is initialized from a Gaussian distribution $r \\ll \\min{(k, d)}$ is the rank, typically much smaller than $d$ and $k$ initially $W = W_{\\theta} + \\mathbf{0} \\times A = W_{\\theta}$ 3. Why low-rank helps Fewer parameters: from $d \\times k$ down to $r \\times (d+k)$ Better memory efficiency: enables training with smaller GPU memory Better compute efficiency: reduces overall compute cost References LoRA: Low-Rank Adaptation of Large Language Models LORA微调系列(一)：LORA和它的基本原理 ","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/lora/","summary":"\u003ch2 id=\"lora\"\u003eLoRA\u003c/h2\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/cspaulia-blog/posts/lora/lora_overview.png\" alt=\"lora_overview\" width=\"60%\" loading=\"lazy\" /\u003e\n\n\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eLoRA is a parameter-efficient fine-tuning method for large language models, introduced in the paper\n\u003ca href=\"https://arxiv.org/pdf/2106.09685\"\u003eLoRA: Low-Rank Adaptation of Large Language Models\u003c/a\u003e.\nWith LoRA, the number of trainable parameters can be reduced to around $10^{-4}$ of the full model; GPU memory usage can drop by about 2/3, and it does not add extra inference latency.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"key-ideas\"\u003eKey ideas\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eLow-rank decomposition\u003c/strong\u003e: instead of updating the full pretrained weights, learn a low-rank update\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eParameter efficiency\u003c/strong\u003e: drastically reduces the number of trainable parameters\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKeep base weights frozen\u003c/strong\u003e: the original pretrained weights stay unchanged; only the added low-rank matrices are trained\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"how-it-works\"\u003eHow it works\u003c/h3\u003e\n\u003ch4 id=\"1-the-idea-of-parameter-efficient-fine-tuning\"\u003e1. The idea of parameter-efficient fine-tuning\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFull fine-tuning\u003c/strong\u003e\u003c/p\u003e","title":"LoRA: Low-Rank Adaptation for LLM Fine-tuning"},{"content":"Customize post_meta display Last edited time (Lastmod)\nAdd this to config.yaml:\nfrontmatter: date: - date - publishDate - lastmod lastmod: - :git - :fileModTime - lastmod - date - publishDate The last modified time is resolved in the order listed in frontmatter.lastmod:\n:git: uses git commit time; requires enableGitInfo = true in config.yml (I didn’t get this working) :fileModTime: uses the local file’s last modified time lastmod: set directly in the page front matter date: set directly in the page front matter publishDate: the publish time Edit post_meta.html\n\u0026lt;!-- layouts/partials/post_meta.html --\u0026gt; {{ $scratch := newScratch }} {{ if not .Date.IsZero }} {{ $scratch.Add \u0026#34;meta\u0026#34; (slice (printf `\u0026lt;span class=\u0026#34;post-meta-item\u0026#34;\u0026gt;\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; width=\u0026#34;16\u0026#34; height=\u0026#34;16\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34; fill=\u0026#34;none\u0026#34; stroke=\u0026#34;currentColor\u0026#34; stroke-width=\u0026#34;2\u0026#34; stroke-linecap=\u0026#34;round\u0026#34; stroke-linejoin=\u0026#34;round\u0026#34;\u0026gt;\u0026lt;rect x=\u0026#34;3\u0026#34; y=\u0026#34;4\u0026#34; width=\u0026#34;18\u0026#34; height=\u0026#34;18\u0026#34; rx=\u0026#34;2\u0026#34; ry=\u0026#34;2\u0026#34;\u0026gt;\u0026lt;/rect\u0026gt;\u0026lt;line x1=\u0026#34;16\u0026#34; y1=\u0026#34;2\u0026#34; x2=\u0026#34;16\u0026#34; y2=\u0026#34;6\u0026#34;\u0026gt;\u0026lt;/line\u0026gt;\u0026lt;line x1=\u0026#34;8\u0026#34; y1=\u0026#34;2\u0026#34; x2=\u0026#34;8\u0026#34; y2=\u0026#34;6\u0026#34;\u0026gt;\u0026lt;/line\u0026gt;\u0026lt;line x1=\u0026#34;3\u0026#34; y1=\u0026#34;10\u0026#34; x2=\u0026#34;21\u0026#34; y2=\u0026#34;10\u0026#34;\u0026gt;\u0026lt;/line\u0026gt;\u0026lt;/svg\u0026gt; %s\u0026lt;/span\u0026gt;` (.Date | time.Format (default \u0026#34;January 2, 2006\u0026#34; site.Params.DateFormat)))) }} {{ end }} {{ if not .Lastmod.IsZero }} {{ $scratch.Add \u0026#34;meta\u0026#34; (slice (printf `\u0026lt;span class=\u0026#34;post-meta-item\u0026#34;\u0026gt;\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; width=\u0026#34;16\u0026#34; height=\u0026#34;16\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34; fill=\u0026#34;none\u0026#34; stroke=\u0026#34;currentColor\u0026#34; stroke-width=\u0026#34;2\u0026#34; stroke-linecap=\u0026#34;round\u0026#34; stroke-linejoin=\u0026#34;round\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M12 19l7-7 3 3-7 7-3-3z\u0026#34;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;path d=\u0026#34;M18 13l-1.5-7.5L2 2l3.5 14.5L13 18l5-5z\u0026#34;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;path d=\u0026#34;M2 2l7.586 7.586\u0026#34;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;circle cx=\u0026#34;11\u0026#34; cy=\u0026#34;11\u0026#34; r=\u0026#34;2\u0026#34;\u0026gt;\u0026lt;/circle\u0026gt;\u0026lt;/svg\u0026gt; %s\u0026lt;/span\u0026gt;` (.Lastmod | time.Format (default \u0026#34;January 2, 2006\u0026#34; site.Params.DateFormat)))) }} {{ end }} {{ if (.Param \u0026#34;ShowWordCount\u0026#34;) }} {{ $scratch.Add \u0026#34;meta\u0026#34; (slice (printf `\u0026lt;span class=\u0026#34;post-meta-item\u0026#34;\u0026gt;\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; width=\u0026#34;16\u0026#34; height=\u0026#34;16\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34; fill=\u0026#34;none\u0026#34; stroke=\u0026#34;currentColor\u0026#34; stroke-width=\u0026#34;2\u0026#34; stroke-linecap=\u0026#34;round\u0026#34; stroke-linejoin=\u0026#34;round\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z\u0026#34;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;path d=\u0026#34;M14 2v6h6\u0026#34;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;line x1=\u0026#34;8\u0026#34; y1=\u0026#34;13\u0026#34; x2=\u0026#34;16\u0026#34; y2=\u0026#34;13\u0026#34;\u0026gt;\u0026lt;/line\u0026gt;\u0026lt;line x1=\u0026#34;8\u0026#34; y1=\u0026#34;17\u0026#34; x2=\u0026#34;16\u0026#34; y2=\u0026#34;17\u0026#34;\u0026gt;\u0026lt;/line\u0026gt;\u0026lt;line x1=\u0026#34;8\u0026#34; y1=\u0026#34;9\u0026#34; x2=\u0026#34;12\u0026#34; y2=\u0026#34;9\u0026#34;\u0026gt;\u0026lt;/line\u0026gt;\u0026lt;/svg\u0026gt; %s\u0026lt;/span\u0026gt;` (i18n \u0026#34;words\u0026#34; .WordCount | default (printf \u0026#34;%d words\u0026#34; .WordCount)))) }} {{ end }} {{ if (.Param \u0026#34;ShowReadingTime\u0026#34;) }} {{ $scratch.Add \u0026#34;meta\u0026#34; (slice (printf `\u0026lt;span class=\u0026#34;post-meta-item\u0026#34;\u0026gt;\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; width=\u0026#34;16\u0026#34; height=\u0026#34;16\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34; fill=\u0026#34;none\u0026#34; stroke=\u0026#34;currentColor\u0026#34; stroke-width=\u0026#34;2\u0026#34; stroke-linecap=\u0026#34;round\u0026#34; stroke-linejoin=\u0026#34;round\u0026#34;\u0026gt;\u0026lt;circle cx=\u0026#34;12\u0026#34; cy=\u0026#34;12\u0026#34; r=\u0026#34;10\u0026#34;\u0026gt;\u0026lt;/circle\u0026gt;\u0026lt;polyline points=\u0026#34;12 6 12 12 16 14\u0026#34;\u0026gt;\u0026lt;/polyline\u0026gt;\u0026lt;/svg\u0026gt; %s\u0026lt;/span\u0026gt;` (i18n \u0026#34;read_time\u0026#34; .ReadingTime | default (printf \u0026#34;%d min\u0026#34; .ReadingTime)))) }} {{ end }} {{ with (partial \u0026#34;author.html\u0026#34; .) }} {{ $scratch.Add \u0026#34;meta\u0026#34; (slice (printf `\u0026lt;span class=\u0026#34;post-meta-item\u0026#34;\u0026gt;\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; width=\u0026#34;16\u0026#34; height=\u0026#34;16\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34; fill=\u0026#34;none\u0026#34; stroke=\u0026#34;currentColor\u0026#34; stroke-width=\u0026#34;2\u0026#34; stroke-linecap=\u0026#34;round\u0026#34; stroke-linejoin=\u0026#34;round\u0026#34;\u0026gt;\u0026lt;path d=\u0026#34;M20 21v-2a4 4 0 0 0-4-4H8a4 4 0 0 0-4 4v2\u0026#34;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;circle cx=\u0026#34;12\u0026#34; cy=\u0026#34;7\u0026#34; r=\u0026#34;4\u0026#34;\u0026gt;\u0026lt;/circle\u0026gt;\u0026lt;/svg\u0026gt; %s\u0026lt;/span\u0026gt;` .)) }} {{ end }} {{ with ($scratch.Get \u0026#34;meta\u0026#34;) }} {{ delimit . \u0026#34; ｜ \u0026#34; | safeHTML }} {{ end }} Add CSS to align icons\n/* assests/css/extended/blank.css */ .post-meta-item { display: inline-flex; align-items: center; gap: 4px; } .post-meta-item svg { stroke: var(--secondary); } Hide on list pages (optional)\nIf you don’t want ReadingTime to show on list pages, copy layouts/partials/post_meta.html to layouts/partials/post_meta_list.html and remove the following block from post_meta_list.html:\n\u0026lt;!-- layouts/partials/post_meta_list.html --\u0026gt; \u0026lt;!-- remove --\u0026gt; {{ if (.Param \u0026#34;ShowReadingTime\u0026#34;) }} ... {{ end }} Then in layouts/_default/list.html, change:\n\u0026lt;!-- layouts/_default/list.html --\u0026gt; {{- partial \u0026#34;post_meta.html\u0026#34; . -}} to:\n\u0026lt;!-- layouts/_default/list.html --\u0026gt; {{- partial \u0026#34;post_meta_list.html\u0026#34; . -}} Code block improvements Code block indentation Add the following to blank.css:\n/* assets/css/extended/blank.css */ .post-content pre { /* code block indentation */ margin-left: 2em; /* indentation */ } .post-content li pre { /* extra indentation for code blocks inside lists */ margin-left: 4em; } Syntax highlighting settings in PaperMod Configure syntax highlighting correctly in config.yaml:\nparams: # ...existing code... assets: disableHLJS: false # enable highlight.js # Set syntax highlighting engine syntax_highlighter: \u0026#34;highlight.js\u0026#34; Example fenced code block with highlighted lines:\nprint(\u0026#34;Line 1\u0026#34;) print(\u0026#34;Line 2\u0026#34;) print(\u0026#34;**This line will be highlighted**\u0026#34;) print(\u0026#34;Line 4\u0026#34;) Limit code block height Limit height and enable scrolling\n/* assets/css/extended/blank.css */ .post-content pre { max-height: 400px; /* max height */ overflow: auto; /* scroll when overflow */ } Scrollbar size\n.post-content pre::-webkit-scrollbar { width: 10px; height: 10px; } Shrink post cover images and move them to the side Copy list.html\nCopy themes/PaperMod/layouts/_default/list.html to layouts/_default/list.html, and replace:\n\u0026lt;!-- layouts/_default/list.html --\u0026gt; \u0026lt;article class=\u0026#34;{{ $class }}\u0026#34;\u0026gt; {{- $isHidden := (.Param \u0026#34;cover.hiddenInList\u0026#34;) | default (.Param \u0026#34;cover.hidden\u0026#34;) | default false }} {{- partial \u0026#34;cover.html\u0026#34; (dict \u0026#34;cxt\u0026#34; . \u0026#34;IsSingle\u0026#34; false \u0026#34;isHidden\u0026#34; $isHidden) }} \u0026lt;header class=\u0026#34;entry-header\u0026#34;\u0026gt; \u0026lt;h2 class=\u0026#34;entry-hint-parent\u0026#34;\u0026gt; {{- .Title }} {{- if .Draft }} \u0026lt;span class=\u0026#34;entry-hint\u0026#34; title=\u0026#34;Draft\u0026#34;\u0026gt; \u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; height=\u0026#34;20\u0026#34; viewBox=\u0026#34;0 -960 960 960\u0026#34; fill=\u0026#34;currentColor\u0026#34;\u0026gt; \u0026lt;path d=\u0026#34;M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z\u0026#34; /\u0026gt; \u0026lt;/svg\u0026gt; \u0026lt;/span\u0026gt; {{- end }} \u0026lt;/h2\u0026gt; \u0026lt;/header\u0026gt; {{- if (ne (.Param \u0026#34;hideSummary\u0026#34;) true) }} \u0026lt;div class=\u0026#34;entry-content\u0026#34;\u0026gt; \u0026lt;p\u0026gt;{{ .Summary | plainify | htmlUnescape }}{{ if .Truncated }}...{{ end }}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; {{- end }} {{- if not (.Param \u0026#34;hideMeta\u0026#34;) }} \u0026lt;footer class=\u0026#34;entry-footer\u0026#34;\u0026gt; {{- partial \u0026#34;post_meta.html\u0026#34; . -}} \u0026lt;/footer\u0026gt; {{- end }} \u0026lt;a class=\u0026#34;entry-link\u0026#34; aria-label=\u0026#34;post link to {{ .Title | plainify }}\u0026#34; href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/article\u0026gt; with:\n\u0026lt;!-- layouts/_default/list.html --\u0026gt; \u0026lt;article class=\u0026#34;{{ $class }}\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;post-info\u0026#34;\u0026gt; \u0026lt;header class=\u0026#34;entry-header\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;{{ .Title }}\u0026lt;/h2\u0026gt; \u0026lt;/header\u0026gt; {{- if .Description }} \u0026lt;section class=\u0026#34;entry-content\u0026#34;\u0026gt; \u0026lt;p\u0026gt;{{ .Description }}\u0026lt;/p\u0026gt; \u0026lt;/section\u0026gt; {{- else if (ne (.Param \u0026#34;hideSummary\u0026#34;) true) }} \u0026lt;section class=\u0026#34;entry-content\u0026#34;\u0026gt; \u0026lt;p\u0026gt;{{ .Summary | plainify | htmlUnescape }}{{ if .Truncated }}...{{ end }}\u0026lt;/p\u0026gt; \u0026lt;/section\u0026gt; {{- end }} {{- if not (.Param \u0026#34;hideMeta\u0026#34;) }} \u0026lt;footer class=\u0026#34;entry-footer\u0026#34;\u0026gt; {{- partial \u0026#34;post_meta.html\u0026#34; . -}} \u0026lt;/footer\u0026gt; {{- end }} \u0026lt;/div\u0026gt; {{- $isHidden := (.Param \u0026#34;cover.hiddenInList\u0026#34;) | default (.Param \u0026#34;cover.hidden\u0026#34;) | default false }} {{- partial \u0026#34;cover.html\u0026#34; (dict \u0026#34;cxt\u0026#34; . \u0026#34;IsHome\u0026#34; true \u0026#34;isHidden\u0026#34; $isHidden) }} \u0026lt;a class=\u0026#34;entry-link\u0026#34; aria-label=\u0026#34;post link to {{ .Title | plainify }}\u0026#34; href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/article\u0026gt; Add custom styles\n/* assets/css/extended/blank.css */ .post-entry { display: flex; flex-direction: row; align-items: center; } .entry-cover { overflow: hidden; /* padding-left: 18px; */ height: 100%; width: 50%; margin-bottom: unset; border-radius: 12px; } .entry-cover img { border-radius: 12px; } .post-info { display: inline-block; overflow: hidden; width: 90%; } Hover zoom animation for side cover\n/* assets/css/extended/blank.css */ .post-entry img{ transition: all 0.3s ease-out; transform:scale(1,1); } .post-entry:hover img{ transition: all 0.3s ease-out; transform:scale(1.02,1.02); } Custom Post Footer Copy single.html\nFind single.html under themes/PaperMod/layouts/_default and copy it to layouts/_default/single.html.\nModify the post-footer section\nFind:\n\u0026lt;!-- layouts/_default/single.html --\u0026gt; \u0026lt;footer class=\u0026#34;post-footer\u0026#34;\u0026gt; ... \u0026lt;/footer\u0026gt; Replace it with:\n\u0026lt;!-- layouts/_default/single.html --\u0026gt; \u0026lt;footer class=\u0026#34;post-footer\u0026#34;\u0026gt; {{- $tags := .Language.Params.Taxonomies.tag | default \u0026#34;tags\u0026#34; }} \u0026lt;p style=\u0026#34;font-size: medium; margin-bottom: 5px; font-weight: bold;\u0026#34;\u0026gt;Tags:\u0026lt;/p\u0026gt; \u0026lt;ul class=\u0026#34;post-tags\u0026#34;\u0026gt; {{- range ($.GetTerms $tags) }} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .LinkTitle }}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {{- end }} \u0026lt;/ul\u0026gt; {{- $categories := .Language.Params.Taxonomies.categories | default \u0026#34;categories\u0026#34; }} \u0026lt;p style=\u0026#34;font-size: medium; margin-bottom: 5px; font-weight: bold;\u0026#34;\u0026gt;Categories:\u0026lt;/p\u0026gt; \u0026lt;ul class=\u0026#34;post-tags\u0026#34;\u0026gt; {{- range ($.GetTerms $categories) }} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .LinkTitle }}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {{- end }} \u0026lt;/ul\u0026gt; {{- if (.Param \u0026#34;ShowPostNavLinks\u0026#34;) }} {{- partial \u0026#34;post_nav_links.html\u0026#34; . }} {{- end }} {{- if (and site.Params.ShowShareButtons (ne .Params.disableShare true)) }} {{- partial \u0026#34;share_icons.html\u0026#34; . -}} {{- end }} \u0026lt;/footer\u0026gt; Result Add reference links at the end of posts Add a bit of style in blank.css\n/* assets/css/extended/blank.css */ /* light mode */ .zhihu-ref { background: #f6f7fa; border-radius: 8px; padding: 1.2em 1.5em 1.2em 1.5em; margin-top: 2em; font-size: 1em; box-shadow: 0 2px 8px rgba(0,0,0,0.03); border-left: 4px solid #0084ff; transition: background 0.3s, border-color 0.3s; } .zhihu-ref-title { font-weight: bold; color: #175199; margin-bottom: 0.5em; font-size: 1.1em; } .zhihu-ref a { color: #175199; } .zhihu-ref a:hover { color: #0084ff; } /* dark mode (PaperMod uses .dark) */ .dark .zhihu-ref { background: #23272e; border-left: 4px solid #3ea6ff; } .dark .zhihu-ref-title, .dark .zhihu-ref a { color: #3ea6ff; } .dark .zhihu-ref a:hover { color: #8cc8ff; } Markdown usage:\n\u0026lt;hr\u0026gt; \u0026lt;div class=\u0026#34;references\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;References\u0026lt;/h3\u0026gt; \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;https://gohugo.io/documentation/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Hugo Documentation\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;https://github.com/adityatelange/hugo-PaperMod\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;PaperMod Theme\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;https://www.markdownguide.org/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Markdown Guide\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;/div\u0026gt; Result:\nSide Table of Contents (TOC) Create toc.html under layouts/partials and add:\n\u0026lt;!-- layouts/partials/toc.html --\u0026gt; {{- $headers := findRE \u0026#34;\u0026lt;h[1-6].*?\u0026gt;(.|\\n])+?\u0026lt;/h[1-6]\u0026gt;\u0026#34; .Content -}} {{- $has_headers := ge (len $headers) 1 -}} {{- if $has_headers -}} \u0026lt;aside id=\u0026#34;toc-container\u0026#34; class=\u0026#34;toc-container wide\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;toc\u0026#34;\u0026gt; \u0026lt;details {{if (.Param \u0026#34;TocOpen\u0026#34;) }} open{{ end }}\u0026gt; \u0026lt;summary accesskey=\u0026#34;c\u0026#34; title=\u0026#34;(Alt + C)\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;details\u0026#34;\u0026gt;{{- i18n \u0026#34;toc\u0026#34; | default \u0026#34;Table of Contents\u0026#34; }}\u0026lt;/span\u0026gt; \u0026lt;/summary\u0026gt; \u0026lt;div class=\u0026#34;inner\u0026#34;\u0026gt; {{- $largest := 6 -}} {{- range $headers -}} {{- $headerLevel := index (findRE \u0026#34;[1-6]\u0026#34; . 1) 0 -}} {{- $headerLevel := len (seq $headerLevel) -}} {{- if lt $headerLevel $largest -}} {{- $largest = $headerLevel -}} {{- end -}} {{- end -}} {{- $firstHeaderLevel := len (seq (index (findRE \u0026#34;[1-6]\u0026#34; (index $headers 0) 1) 0)) -}} {{- $.Scratch.Set \u0026#34;bareul\u0026#34; slice -}} \u0026lt;ul\u0026gt; {{- range seq (sub $firstHeaderLevel $largest) -}} \u0026lt;ul\u0026gt; {{- $.Scratch.Add \u0026#34;bareul\u0026#34; (sub (add $largest .) 1) -}} {{- end -}} {{- range $i, $header := $headers -}} {{- $headerLevel := index (findRE \u0026#34;[1-6]\u0026#34; . 1) 0 -}} {{- $headerLevel := len (seq $headerLevel) -}} {{/* get id=\u0026#34;xyz\u0026#34; */}} {{- $id := index (findRE \u0026#34;(id=\\\u0026#34;(.*?)\\\u0026#34;)\u0026#34; $header 9) 0 }} {{- /* strip id=\\\u0026#34;\\\u0026#34; to leave xyz, no way to get regex capturing groups in hugo */ -}} {{- $cleanedID := replace (replace $id \u0026#34;id=\\\u0026#34;\u0026#34; \u0026#34;\u0026#34;) \u0026#34;\\\u0026#34;\u0026#34; \u0026#34;\u0026#34; }} {{- $header := replaceRE \u0026#34;\u0026lt;h[1-6].*?\u0026gt;((.|\\n])+?)\u0026lt;/h[1-6]\u0026gt;\u0026#34; \u0026#34;$1\u0026#34; $header -}} {{- if ne $i 0 -}} {{- $prevHeaderLevel := index (findRE \u0026#34;[1-6]\u0026#34; (index $headers (sub $i 1)) 1) 0 -}} {{- $prevHeaderLevel := len (seq $prevHeaderLevel) -}} {{- if gt $headerLevel $prevHeaderLevel -}} {{- range seq $prevHeaderLevel (sub $headerLevel 1) -}} \u0026lt;ul\u0026gt; {{/* the first should not be recorded */}} {{- if ne $prevHeaderLevel . -}} {{- $.Scratch.Add \u0026#34;bareul\u0026#34; . -}} {{- end -}} {{- end -}} {{- else -}} \u0026lt;/li\u0026gt; {{- if lt $headerLevel $prevHeaderLevel -}} {{- range seq (sub $prevHeaderLevel 1) -1 $headerLevel -}} {{- if in ($.Scratch.Get \u0026#34;bareul\u0026#34;) . -}} \u0026lt;/ul\u0026gt; {{/* manually do pop item */}} {{- $tmp := $.Scratch.Get \u0026#34;bareul\u0026#34; -}} {{- $.Scratch.Delete \u0026#34;bareul\u0026#34; -}} {{- $.Scratch.Set \u0026#34;bareul\u0026#34; slice}} {{- range seq (sub (len $tmp) 1) -}} {{- $.Scratch.Add \u0026#34;bareul\u0026#34; (index $tmp (sub . 1)) -}} {{- end -}} {{- else -}} \u0026lt;/ul\u0026gt; \u0026lt;/li\u0026gt; {{- end -}} {{- end -}} {{- end -}} {{- end -}} {{- end }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;#{{- $cleanedID -}}\u0026#34; aria-label=\u0026#34;{{- $header | plainify -}}\u0026#34;\u0026gt;{{- $header | safeHTML -}}\u0026lt;/a\u0026gt; {{- else }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;#{{- $cleanedID -}}\u0026#34; aria-label=\u0026#34;{{- $header | plainify -}}\u0026#34;\u0026gt;{{- $header | safeHTML -}}\u0026lt;/a\u0026gt; {{- end -}} {{- end -}} {{- end -}} \u0026lt;!-- {{- $firstHeaderLevel := len (seq (index (findRE \u0026#34;[1-6]\u0026#34; (index $headers 0) 1) 0)) -}} --\u0026gt; {{- $firstHeaderLevel := $largest }} {{- $lastHeaderLevel := len (seq (index (findRE \u0026#34;[1-6]\u0026#34; (index $headers (sub (len $headers) 1)) 1) 0)) }} \u0026lt;/li\u0026gt; {{- range seq (sub $lastHeaderLevel $firstHeaderLevel) -}} {{- if in ($.Scratch.Get \u0026#34;bareul\u0026#34;) (add . $firstHeaderLevel) }} \u0026lt;/ul\u0026gt; {{- else }} \u0026lt;/ul\u0026gt; \u0026lt;/li\u0026gt; {{- end -}} {{- end }} \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/details\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/aside\u0026gt; \u0026lt;script\u0026gt; let activeElement; let elements; window.addEventListener(\u0026#39;DOMContentLoaded\u0026#39;, function (event) { checkTocPosition(); elements = document.querySelectorAll(\u0026#39;h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]\u0026#39;); // Make the first header active activeElement = elements[0]; const id = encodeURI(activeElement.getAttribute(\u0026#39;id\u0026#39;)).toLowerCase(); document.querySelector(`.inner ul li a[href=\u0026#34;#${id}\u0026#34;]`).classList.add(\u0026#39;active\u0026#39;); }, false); window.addEventListener(\u0026#39;resize\u0026#39;, function(event) { checkTocPosition(); }, false); window.addEventListener(\u0026#39;scroll\u0026#39;, () =\u0026gt; { // Check if there is an object in the top half of the screen or keep the last item active activeElement = Array.from(elements).find((element) =\u0026gt; { if ((getOffsetTop(element) - window.pageYOffset) \u0026gt; 0 \u0026amp;\u0026amp; (getOffsetTop(element) - window.pageYOffset) \u0026lt; window.innerHeight/2) { return element; } }) || activeElement elements.forEach(element =\u0026gt; { const id = encodeURI(element.getAttribute(\u0026#39;id\u0026#39;)).toLowerCase(); if (element === activeElement){ document.querySelector(`.inner ul li a[href=\u0026#34;#${id}\u0026#34;]`).classList.add(\u0026#39;active\u0026#39;); } else { document.querySelector(`.inner ul li a[href=\u0026#34;#${id}\u0026#34;]`).classList.remove(\u0026#39;active\u0026#39;); } }) }, false); const main = parseInt(getComputedStyle(document.body).getPropertyValue(\u0026#39;--article-width\u0026#39;), 10); const toc = parseInt(getComputedStyle(document.body).getPropertyValue(\u0026#39;--toc-width\u0026#39;), 10); const gap = parseInt(getComputedStyle(document.body).getPropertyValue(\u0026#39;--gap\u0026#39;), 10); function checkTocPosition() { const width = document.body.scrollWidth; if (width - main - (toc * 2) - (gap * 4) \u0026gt; 0) { document.getElementById(\u0026#34;toc-container\u0026#34;).classList.add(\u0026#34;wide\u0026#34;); } else { document.getElementById(\u0026#34;toc-container\u0026#34;).classList.remove(\u0026#34;wide\u0026#34;); } } function getOffsetTop(element) { if (!element.getClientRects().length) { return 0; } let rect = element.getBoundingClientRect(); let win = element.ownerDocument.defaultView; return rect.top + win.pageYOffset; } \u0026lt;/script\u0026gt; {{- end }} Update CSS:\n/* assets/css/extended/blank.css */ :root { --nav-width: 1380px; --article-width: 650px; --toc-width: 300px; } .toc { margin: 0 2px 40px 2px; border: 1px solid var(--border); background: var(--entry); border-radius: var(--radius); padding: 0.4em; } .toc-container.wide { position: absolute; height: 100%; border-right: 1px solid var(--border); left: calc((var(--toc-width) + var(--gap)) * -1); top: calc(var(--gap) * 2); width: var(--toc-width); } .wide .toc { position: sticky; top: var(--gap); border: unset; background: unset; border-radius: unset; width: 100%; margin: 0 2px 40px 2px; } .toc details summary { cursor: zoom-in; margin-inline-start: 20px; padding: 12px 0; } .toc details[open] summary { font-weight: 500; } .toc-container.wide .toc .inner { margin: 0; } .active { font-size: 110%; font-weight: 600; } .toc ul { list-style-type: circle; } .toc .inner { margin: 0 0 0 20px; padding: 0px 15px 15px 20px; font-size: 16px; } .toc li ul { margin-inline-start: calc(var(--gap) * 0.5); list-style-type: none; } .toc li { list-style: none; font-size: 0.95rem; padding-bottom: 5px; } .toc li a:hover { color: var(--secondary); } Sort posts by LastMod Posts page Update layouts/_default/list.html:\n\u0026lt;!-- layouts/_default/list.html --\u0026gt; {{- if .IsHome }} {{- $pages = where site.RegularPages \u0026#34;Type\u0026#34; \u0026#34;in\u0026#34; site.Params.mainSections }} {{- $pages = where $pages \u0026#34;Params.hiddenInHomeList\u0026#34; \u0026#34;!=\u0026#34; \u0026#34;true\u0026#34; }} {{- end }} {{- $pages := $pages.ByLastmod.Reverse }} \u0026lt;!-- add this line --\u0026gt; {{- $paginator := .Paginate $pages }} Archives page Copy themes/PaperMod/layouts/_default/archives.html to layouts/_default, and change:\n\u0026lt;!-- layouts/_default/archives.html --\u0026gt; {{- range $pages.GroupByPublishDate \u0026#34;2006\u0026#34; }} ... {{- range .Pages.GroupByDate \u0026#34;January\u0026#34; }} ... {{- range .Pages }} ... {{- end }} ... {{- end }} ... {{- end }} to:\n\u0026lt;!-- layouts/_default/archives.html --\u0026gt; {{- range $pages.GroupByParamDate \u0026#34;lastmod\u0026#34; \u0026#34;2006\u0026#34; }} ... {{- range .Pages.GroupByParamDate \u0026#34;lastmod\u0026#34; \u0026#34;January\u0026#34; }} ... {{- range .Pages.ByLastmod.Reverse }} ... {{- end }} ... {{- end }} ... {{- end }} References 小M平碎碎念-小M平部落格整形手術 CSDN-Hugo博客PaperMod主题目录放在侧边 ","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/hugo-papermod/","summary":"\u003ch2 id=\"customize-post_meta-display\"\u003eCustomize \u003ccode\u003epost_meta\u003c/code\u003e display\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eLast edited time (\u003ccode\u003eLastmod\u003c/code\u003e)\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAdd this to \u003ccode\u003econfig.yaml\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nt\"\u003efrontmatter\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003edate\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003edate\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003epublishDate\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003elastmod\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e    \u003c/span\u003e\u003cspan class=\"nt\"\u003elastmod\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"l\"\u003egit\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"l\"\u003efileModTime\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003elastmod\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003edate\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"w\"\u003e        \u003c/span\u003e- \u003cspan class=\"l\"\u003epublishDate\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe last modified time is resolved in the order listed in \u003ccode\u003efrontmatter.lastmod\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e:git\u003c/code\u003e: uses git commit time; requires \u003ccode\u003eenableGitInfo = true\u003c/code\u003e in \u003ccode\u003econfig.yml\u003c/code\u003e (I didn’t get this working)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e:fileModTime\u003c/code\u003e: uses the local file’s last modified time\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003elastmod\u003c/code\u003e: set directly in the page front matter\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edate\u003c/code\u003e: set directly in the page front matter\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epublishDate\u003c/code\u003e: the publish time\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEdit \u003ccode\u003epost_meta.html\u003c/code\u003e\u003c/strong\u003e\u003c/p\u003e","title":"The Evolution of PaperMod"},{"content":"I. CLIP Contrastive pre-training: Collect $N$ image–text pairs from the internet (OpenAI used 400M image–text pairs) as positive pairs.\nFor each image, pair it with the other $N-1$ texts to form negative pairs (text does not match the image).\nFeed the $N$ images into the image encoder and the corresponding texts into the text encoder. Compute dot products between image and text features to obtain a similarity matrix.\nTreat each row as an $N$-way classification problem. For row $i$, the correct label is $i$ (make the $(i,i)$ entry the largest in that row). Apply cross-entropy between row $i$ and label $i$.\nSimilarly, treat each column as an $N$-way classification problem and apply cross-entropy so that the $(i,i)$ entry is the largest in column $i$.\nBuild a classifier from labels: Turn text labels into prompts/sentences and feed them into the text encoder to get text features. Zero-shot prediction: Match the label text features with image features by similarity to make predictions. II. Using CLIP for Semantic Segmentation 2.1 LSeg Relationship to CLIP: Use CLIP’s aligned feature space: map semantic labels and pixel features into the same space and predict segmentation by similarity.\nText encoder: the same as CLIP, and frozen during training \u0026#x2744;\u0026#xfe0f;.\nDifferences from CLIP: Method Training Text encoder params Similarity computation CLIP Contrastive Trainable Similarity between image–text pairs LSeg Supervised Frozen Similarity between image features and text features CLIP input: multiple image–text pairs LSeg input: one image + labels (can be viewed as descriptive text for the image) 2.2 GroupViT Relationship to CLIP and LSeg: Method Training Similarity computation CLIP Contrastive Similarity between image–text pairs LSeg Supervised Similarity between image features and text features GroupViT Contrastive Similarity between image–text pairs LSeg is trained with supervision on top of CLIP’s aligned feature space. GroupViT adjusts the vision encoder architecture in CLIP to better fit segmentation, while keeping CLIP-style contrastive learning. Architecture details: Model input: image patches + learnable group tokens Segmentation: assign patch features to learnable group tokens via Group Block learnable group tokens: similar to cluster centroids III. Using CLIP for Object Detection 3.1 ViLD Vanilla Detector = Head + Classifier + supervised cross-entropy ViLD-text = Head + similarity matching + supervised cross-entropy Similarity-matching pipeline (CLIP-style): Feed $n$ labels (via prompt engineering) into the text encoder to get $n$ text embeddings. To cover regions that do not match any label, introduce a background embedding. Match region embeddings with text embeddings and the background embedding to obtain $n$ class scores plus one background score. This replaces the Classifier and is frozen during training \u0026#x2744;\u0026#xfe0f;. ViLD-image = teacher network + student network + L1 distillation teacher network: CLIP image encoder student network: Vanilla Detector To reduce training cost, pre-extract $m$ region embeddings using a pretrained detector. ViLD = ViLD-text + ViLD-image IV. Using CLIP for Visual Grounding 4.1 GLIP Essentially supervised training. Compute similarity between Regions and Words to classify/caption Regions. Training requires knowing the alignment between Regions and Words in captions. To obtain it: Detection datasets: build captions from bounding-box annotations (e.g., Banana → “There is a banana.”) Caption datasets: use a GLIP model trained on detection data to find Regions–Words alignments and construct pseudo labels. V. Using CLIP for Image Generation 5.1 CLIPasso Motivation Observation: prior sketch generation methods often work only for a specific category.\nGoal: leverage CLIP’s strong generalization to generate sketches for arbitrary categories.\nPipeline Generate a sketch: Use the image encoder to obtain a heatmap. Sample points based on the heatmap. Aggregate points with learnable parameters to form Bézier curves, producing a sketch. Constrain generation with CLIP: Feed the generated sketch and the original image into two different CLIP image encoders. $L_g$ constrains geometric consistency (closer is better). $L_s$ constrains semantic consistency (closer is better). VI. Using CLIP for Video Retrieval 6.1 CLIP4Clip Motivation CLIP is designed for image–text pairs. For video retrieval, the task is matching one text query against multiple frames and finding the most relevant frames. CLIP4Clip explores three matching strategies: Parameter-free type: no extra parameters (e.g., mean pooling; ignores temporal order) Sequential type: temporal modules such as LSTM and Transformer Tight type: a Transformer Encoder jointly learns text and video features and outputs similarity 6.2 ActionCLIP Similar to CLIP4Clip.\nVII. Using CLIP for Speech Recognition 7.1 AudioCLIP Add an audio encoder and follow CLIP-style objectives: audio–image contrastive learning and audio–text contrastive learning.\nVIII. Using CLIP for 3D Understanding 8.1 PointCLIP Project the point cloud into a 2D space. Build prompts: Point Cloud Depth Map of a [CLASS] IX. Using CLIP for Depth Estimation 9.1 DepthCLIP Build 7 text prompts: \u0026ldquo;This object is [distance class]\u0026rdquo;, where [distance class] is: \u0026lsquo;giant\u0026rsquo; \u0026rsquo;extremely close' \u0026lsquo;close\u0026rsquo; \u0026rsquo;not in distance' \u0026lsquo;a little remote\u0026rsquo; \u0026lsquo;far\u0026rsquo; \u0026lsquo;unseen\u0026rsquo; The network is similar to LSeg and performs classification with Softmax. References CLIP 改进工作串讲（上）【论文精读·42】 CLIP 改进工作串讲（下）【论文精读·42】 ","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/clip/","summary":"\u003ch2 id=\"i-clip\"\u003eI. CLIP\u003c/h2\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/cspaulia-blog/posts/clip/clip.png\" alt=\"clip\" loading=\"lazy\" /\u003e\n\n\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eContrastive pre-training:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eCollect $N$ image–text pairs from the internet (OpenAI used 400M image–text pairs) as positive pairs.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFor each image, pair it with the other $N-1$ texts to form negative pairs (text does not match the image).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFeed the $N$ images into the image encoder and the corresponding texts into the text encoder. Compute dot products between image and text features to obtain a similarity matrix.\u003c/p\u003e","title":"CLIP and Its Follow-up Works"},{"content":"Basic screen commands Start a new screen session: screen -S \u0026lt;name\u0026gt; List all screen sessions: screen -ls Reattach a detached session: screen -r \u0026lt;session-id\u0026gt; Detach (leave the session running in background): Press Ctrl+a, then press d.\nShow the current session name (id.name): echo $STY Close the current session: Run exit (or press Ctrl+d) inside the session.\nKill a session from outside: screen -X -S \u0026lt;session_name\u0026gt; quit Clean up dead sessions: screen -wipe ","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/screen/","summary":"\u003ch2 id=\"basic-screen-commands\"\u003eBasic \u003ccode\u003escreen\u003c/code\u003e commands\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eStart a new \u003ccode\u003escreen\u003c/code\u003e session:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003escreen -S \u0026lt;name\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eList all \u003ccode\u003escreen\u003c/code\u003e sessions:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003escreen -ls\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eReattach a detached session:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003escreen -r \u0026lt;session-id\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eDetach (leave the session running in background):\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePress \u003ccode\u003eCtrl+a\u003c/code\u003e, then press \u003ccode\u003ed\u003c/code\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eShow the current session name (id.name):\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eecho\u003c/span\u003e \u003cspan class=\"nv\"\u003e$STY\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eClose the current session:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eRun \u003ccode\u003eexit\u003c/code\u003e (or press \u003ccode\u003eCtrl+d\u003c/code\u003e) inside the session.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKill a session from outside:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003escreen -X -S \u0026lt;session_name\u0026gt; quit\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eClean up dead sessions:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003escreen -wipe\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Using `screen` to Manage Terminal Sessions"},{"content":"ReLU $$ \\text{ReLU}(x) = \\max(0, x) $$\nGELU (Gaussian Error Linear Unit) $$ \\text{GELU}(x) = x \\cdot \\Phi(x) $$\n$\\Phi(x)$ is the cumulative distribution function (CDF) of the standard normal distribution:\n$$ \\Phi(x) = \\frac{1}{2}[1+\\text{erf}(\\frac{x}{\\sqrt{2}})] $$\n$\\text{erf}$ is the Gauss error function. Therefore, GELU can be written as\n$$ \\text{GELU}(x) = \\frac{x}{2}[1+\\text{erf}(\\frac{x}{\\sqrt{2}})] $$\nIt is commonly approximated by\n$$ \\text{GELU}(x) = 0.5x(1+\\tanh(\\sqrt{\\frac{2}{\\pi}}(x+0.044715x^3))) $$\nIntuition GELU can be understood as:\nInstead of “hard-clipping” negative inputs to 0 like ReLU, it uses a probability-weighted gate so the output transitions smoothly.\nIn other words, it passes the input $x$ through a “Gaussian gate”:\nWhen $x$ is large, $\\Phi(x) \\approx 1$, so the output is $\\approx x$ When $x$ is very negative, $\\Phi(x) \\approx 0$, so the output is $\\approx 0$ Near $x=0$, $\\Phi(x)$ smoothly transitions between 0 and 1 So GELU is “softer” than ReLU and can retain some information from negative inputs.\nAdvantages Smooth derivative: the derivative of GELU is continuous, which often helps gradient flow and avoids the non-differentiability of ReLU at $x=0$. Strong empirical performance: GELU is widely used in NLP and vision models and often improves results compared to ReLU. Potentially faster convergence: the smooth nonlinearity can make optimization more stable. Swish $$ \\text{Swish}(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}} $$\nThe derivative is smooth and behaves like a gate.\nSwish can be adjusted with a smoothing parameter $\\beta$:\n$$ \\text{Swish}(x) = x \\cdot \\sigma(\\beta x) = \\frac{x}{1 + e^{-\\beta x}} $$\nGLU(Gated Linear Unit) $$ \\begin{align} [x_1,x_2]\u0026amp;=xW+b \\\\ \\text{GLU}(x_1, x_2) \u0026amp;= x_1 \\odot \\sigma(x_2) \\end{align} $$\nWhere:\nThe input $x$ is projected (or convolved) and split into two parts $x_1, x_2$. $\\sigma$ is the sigmoid function, acting as a gate. $\\odot$ denotes element-wise multiplication (Hadamard product). $x_1$ is the content stream and $x_2$ is the control/gating stream. Intuition: content stream + control stream For a plain linear layer:\n$$ y=xW+b $$\nAll inputs are treated uniformly.\nIn GLU, we add a learnable gate to dynamically modulate the content stream:\n$$ y=content \\times gate $$\nIf $\\sigma(x_2)$ is close to 1, the content $x_1$ passes through. If $\\sigma(x_2)$ is close to 0, the content $x_1$ is suppressed. For intermediate values, the content is partially passed. You can think of it as:\nGLU adds a learnable “switch” that controls how strongly information flows.\nCode import torch import torch.nn.functional as F def GLU(x): x1, x2 = x.chunk(2, dim=-1) return x1 * torch.sigmoid(x2) Advantages Selective information flow: the gate learns which features should pass. Better gradient behavior: sigmoid has non-zero derivative in many regions. Improved interpretability: gate values (often in $[0,1]$) can be visualized as importance weights. ReGLU(ReLU-GLU) $$ \\text{ReGLU}(x) = x_1 \\odot \\text{ReLU}(x_2) $$\nThis makes the gating sparse and can be cheaper to compute.\nimport torch from torch import nn class ReGLU(nn.Module): def __init__(self, d_in, d_out): super().__init__() self.w_gate = nn.Linear(d_in, d_out, bias=False) self.w_up = nn.Linear(d_in, d_out, bias=False) self.w_down = nn.Linear(d_out, d_in, bias=False) def forward(self, x): gate = F.relu(self.w_gate(x)) up = self.w_up(x) return self.w_down(gate * up) GEGLU(Gaussian Error GLU) $$ \\text{GEGLU}(x) = x_1 \\odot \\text{GELU}(x_2) $$\nThis keeps the gating smooth while preserving the GLU structure.\nimport torch from torch import nn class GEGLU(nn.Module): def __init__(self, d_in, d_out): super().__init__() self.w_gate = nn.Linear(d_in, d_out, bias=False) self.w_up = nn.Linear(d_in, d_out, bias=False) self.w_down = nn.Linear(d_out, d_in, bias=False) def forward(self, x): gate = F.gelu(self.w_gate(x)) up = self.w_up(x) return self.w_down(gate * up) SwiGLU(Swish-GLU) $$ \\text{SwiGLU}(x) = x_1 \\odot \\text{Swish}(x_2) $$\nimport torch from torch import nn class SwiGLU(nn.Module): def __init__(self, d_in, d_out, beta=1.0): super().__init__() self.beta = beta self.w_gate = nn.Linear(d_in, d_out, bias=False) self.w_up = nn.Linear(d_in, d_out, bias=False) self.w_down = nn.Linear(d_out, d_in, bias=False) def forward(self, x): gate = self.w_gate(x) gate = gate * torch.sigmoid(self.beta * gate) # Swish up = self.w_up(x) return self.w_down(gate * up) ","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/activation/","summary":"\u003ch3 id=\"relu\"\u003eReLU\u003c/h3\u003e\n\u003cp\u003e$$\n\\text{ReLU}(x) = \\max(0, x)\n$$\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"ReLU\" loading=\"lazy\" src=\"/cspaulia-blog/posts/activation/activation_relu.png\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"gelu-gaussian-error-linear-unit\"\u003eGELU (Gaussian Error Linear Unit)\u003c/h3\u003e\n\u003cp\u003e$$\n\\text{GELU}(x) = x \\cdot \\Phi(x)\n$$\u003c/p\u003e\n\u003cp\u003e$\\Phi(x)$ is the cumulative distribution function (CDF) of the standard normal distribution:\u003c/p\u003e\n\u003cp\u003e$$\n\\Phi(x) = \\frac{1}{2}[1+\\text{erf}(\\frac{x}{\\sqrt{2}})]\n$$\u003c/p\u003e\n\u003cp\u003e$\\text{erf}$ is the Gauss error function. Therefore, GELU can be written as\u003c/p\u003e\n\u003cp\u003e$$\n\\text{GELU}(x) = \\frac{x}{2}[1+\\text{erf}(\\frac{x}{\\sqrt{2}})]\n$$\u003c/p\u003e\n\u003cp\u003eIt is commonly approximated by\u003c/p\u003e\n\u003cp\u003e$$\n\\text{GELU}(x) = 0.5x(1+\\tanh(\\sqrt{\\frac{2}{\\pi}}(x+0.044715x^3)))\n$$\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"GELU_Derivative\" loading=\"lazy\" src=\"/cspaulia-blog/posts/activation/activation_gelu.png\"\u003e\u003c/p\u003e\n\u003ch4 id=\"intuition\"\u003eIntuition\u003c/h4\u003e\n\u003cp\u003eGELU can be understood as:\u003c/p\u003e","title":"A Collection of Activation Functions"},{"content":"Cross-Attention Overview Cross-attention is an attention mechanism that fuses two embedding sequences:\nThe two sequences must have the same embedding dimension. They can come from different modalities (e.g., text, images, audio). One sequence provides the queries (Q) and determines the output length. The other sequence provides the keys (K) and values (V). Cross-attention vs. self-attention The key difference is the input:\nSelf-attention uses a single sequence to produce Q/K/V. Cross-attention uses two sequences: one for Q and the other for K/V. Cross-attention algorithm Given two sequences $S_1$ and $S_2$ Compute $K$ and $V$ from $S_1$ Compute $Q$ from $S_2$ Compute the attention matrix from $Q$ and $K$ Apply attention to $V$ The output sequence length matches $S_2$ $$ \\pmb{\\text{softmax}}((W_Q S_2)(W_K S_1)^\\mathrm{T})W_V S_1 $$\n","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/cross_attention/","summary":"\u003ch2 id=\"cross-attention\"\u003eCross-Attention\u003c/h2\u003e\n\u003ch3 id=\"overview\"\u003eOverview\u003c/h3\u003e\n\u003cp\u003eCross-attention is an attention mechanism that fuses two embedding sequences:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe two sequences must have the \u003cstrong\u003esame embedding dimension\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eThey can come from different modalities (e.g., text, images, audio).\u003c/li\u003e\n\u003cli\u003eOne sequence provides the \u003cstrong\u003equeries (Q)\u003c/strong\u003e and determines the \u003cstrong\u003eoutput length\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eThe other sequence provides the \u003cstrong\u003ekeys (K)\u003c/strong\u003e and \u003cstrong\u003evalues (V)\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"cross-attention-vs-self-attention\"\u003eCross-attention vs. self-attention\u003c/h3\u003e\n\u003cp\u003eThe key difference is the input:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSelf-attention\u003c/strong\u003e uses a single sequence to produce Q/K/V.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCross-attention\u003c/strong\u003e uses two sequences: one for Q and the other for K/V.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"cross attention\" loading=\"lazy\" src=\"/cspaulia-blog/posts/cross_attention/cross_attention.png\"\u003e\u003c/p\u003e","title":"Cross-Attention Mechanism"},{"content":"Layer Normalization In the figure above, $N$ denotes the sample axis, $C$ the channel axis, and $F$ the number of features per channel. Batch Normalization (BN, right) normalizes using features from the same channel across different samples. Layer Normalization (LN, left) normalizes using features from different channels within the same sample.\n1. Issues with BN 1.1 BN and batch size BN computes normalization statistics based on the number of samples in a batch. When the batch is very small (e.g., only 4 samples), the mean and variance estimated from those samples may not represent the global data distribution well, so BN can perform poorly.\n1.2 BN and RNNs Within a batch, sequence lengths often differ. For later time steps (e.g., $t\u0026gt;4$ in the figure), only a small number of sequences may still have valid tokens. Statistics computed from so few samples are not representative of the overall distribution, so BN tends to work poorly in this setting.\nAlso, at inference time, if we encounter a test sequence longer than any training sequence, we may not have the corresponding saved normalization statistics for those time steps, which makes BN hard to apply.\n2. LayerNorm in detail 2.1 LN in an MLP Consider LN in an MLP. Let $H$ be the number of hidden units in a layer, and $l$ be the layer index. LN computes the normalization statistics $\\mu$ and $\\sigma$ as:\n$$ \\mu^{l} = \\frac{1}{H} \\sum_{i=1}^{H} a^l_i ~~~~~~~ \\sigma^{l} = \\sqrt{\\frac{1}{H} \\sum_{i=1}^{H}(a^l_i-\\mu^l)^2} $$\nNote that these statistics do not depend on batch size; they only depend on the number of hidden units. If $H$ is sufficiently large, the estimated statistics can still be stable. The normalized activation is:\n$$ \\hat{a}^l = \\frac{a^l-\\mu^l}{\\sqrt{(\\sigma^l)^2+\\epsilon}} \\tag{1} $$\nwhere $\\epsilon$ is a small constant to avoid division by zero.\nLN also uses learnable parameters to preserve representational capacity: gain $g$ and bias $b$. With activation function $f$, the LN output is:\n$$ h^l = f(g^l \\odot \\hat{a}^l + b^l) \\tag{2} $$\nCombining (1) and (2) and omitting the layer index $l$:\n$$ h=f(\\frac{g}{\\sqrt{\\sigma^2+\\epsilon}} \\odot (a-\\mu) + b) $$\n2.2 LN in an RNN For an RNN at time step $t$, the input is the previous hidden state $h^t$ at time step $t-1$ and the current input $\\text{x}_t$. It can be written as:\n$$ ext{a}^t = W_{hh}h^{t-1}+W_{xh}\\text{x}^{t} $$\nThen we can apply the same normalization procedure on $\\text{a}^t$ as above:\n$$ h^t=f(\\frac{g}{\\sqrt{\\sigma^2+\\epsilon}} \\odot (a^t-\\mu^t) + b) ~~~~~~ \\mu^{t} = \\frac{1}{H} \\sum_{i=1}^{H} a^t_i ~~~~~~~ \\sigma^{l} = \\sqrt{\\frac{1}{H} \\sum_{i=1}^{H}(a^t_i-\\mu^t)^2} $$\n","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/norm/","summary":"\u003ch3 id=\"layer-normalization\"\u003eLayer Normalization\u003c/h3\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/cspaulia-blog/posts/norm/LNvsBN.jpg\" alt=\"LN vs BN\" loading=\"lazy\" /\u003e\n\n\u003c/p\u003e\n\u003cp\u003eIn the figure above, $N$ denotes the sample axis, $C$ the channel axis, and $F$ the number of features per channel.\nBatch Normalization (BN, right) normalizes using features from \u003cstrong\u003ethe same channel across different samples\u003c/strong\u003e.\nLayer Normalization (LN, left) normalizes using features from \u003cstrong\u003edifferent channels within the same sample\u003c/strong\u003e.\u003c/p\u003e\n\u003ch4 id=\"1-issues-with-bn\"\u003e1. Issues with BN\u003c/h4\u003e\n\u003ch5 id=\"11-bn-and-batch-size\"\u003e1.1 BN and batch size\u003c/h5\u003e\n\u003cp\u003eBN computes normalization statistics based on the \u003cstrong\u003enumber of samples\u003c/strong\u003e in a batch.\nWhen the batch is very small (e.g., only 4 samples), the mean and variance estimated from those samples may not represent the global data distribution well, so BN can perform poorly.\u003c/p\u003e","title":"100 Normalization Methods (Work in Progress)"},{"content":"Push local files to GitHub 1. Create a repository on GitHub (remote) Create a new repository on GitHub.\n2. Install / configure Git Configure your username and email:\ngit config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;your_email@example.com\u0026#34; Verify the current config:\ngit config --global user.name git config --global user.email 3. Push files (local -\u0026gt; remote) Initialize a Git repo: git init Add files: # add a folder git add csj_project/ # or add everything git add . Commit: git commit -m \u0026#34;Initial commit\u0026#34; Copy the repository URL (SSH or HTTPS) from GitHub.\nAdd the remote:\ngit remote add origin \u0026lt;repo-url\u0026gt; Example:\ngit remote add origin https://github.com/CSPaulia/\u0026lt;repo-name\u0026gt;.git Push to GitHub: # if your default branch is master git push -u origin master # if your default branch is main # git push -u origin main ","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/github-tips/","summary":"\u003ch2 id=\"push-local-files-to-github\"\u003ePush local files to GitHub\u003c/h2\u003e\n\u003ch3 id=\"1-create-a-repository-on-github-remote\"\u003e1. Create a repository on GitHub (remote)\u003c/h3\u003e\n\u003cp\u003eCreate a new repository on GitHub.\u003c/p\u003e\n\u003ch3 id=\"2-install--configure-git\"\u003e2. Install / configure Git\u003c/h3\u003e\n\u003cp\u003eConfigure your username and email:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit config --global user.name \u003cspan class=\"s2\"\u003e\u0026#34;Your Name\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit config --global user.email \u003cspan class=\"s2\"\u003e\u0026#34;your_email@example.com\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eVerify the current config:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit config --global user.name\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit config --global user.email\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"3-push-files-local---remote\"\u003e3. Push files (local -\u0026gt; remote)\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eInitialize a Git repo:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit init\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"2\"\u003e\n\u003cli\u003eAdd files:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# add a folder\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit add csj_project/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# or add everything\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit add .\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"3\"\u003e\n\u003cli\u003eCommit:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit commit -m \u003cspan class=\"s2\"\u003e\u0026#34;Initial commit\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col start=\"4\"\u003e\n\u003cli\u003e\n\u003cp\u003eCopy the repository URL (SSH or HTTPS) from GitHub.\u003c/p\u003e","title":"How to Use Git to Push Local Files to a GitHub Repository"},{"content":"Rebuttal Workflow List out all reviewer comments and categorize them.\nSchedule a discussion with your advisor and lab mates/senior students to talk through the comments and decide on responses.\nList all additional experiments you need to run, and estimate how much compute they will require.\nWrite the rebuttal document.\nEvidence preparation Theoretical justification: provide detailed derivations; show the full process to senior students and your advisor. Experimental justification: double-check (and re-check) experimental results. Citation justification: clearly point to where the evidence is (e.g., paste the arXiv link). Writing style requirements Address reviewers in the second person (to reduce distance). Be sincere, well-reasoned, logically clear, and human. Drafting process Write the Chinese responses first. Translate with DeepL. Polish with GPT. After finishing, ask your advisor and senior students to review it, then submit a unified response.\nIf reviewers respond again, keep discussing:\nContinue one-on-one communication with senior students and your advisor. Update the rebuttal document. ","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/rebuttal/","summary":"\u003ch2 id=\"rebuttal-workflow\"\u003eRebuttal Workflow\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eList out\u003c/strong\u003e all reviewer comments and categorize them.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSchedule a discussion with your advisor and lab mates/senior students to \u003cstrong\u003etalk through\u003c/strong\u003e the comments and decide on responses.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eList all additional experiments you need to run\u003c/strong\u003e, and estimate how much \u003cstrong\u003ecompute\u003c/strong\u003e they will require.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWrite the rebuttal document.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEvidence preparation\n\u003cul\u003e\n\u003cli\u003eTheoretical justification: provide detailed derivations; show the full process to senior students and your advisor.\u003c/li\u003e\n\u003cli\u003eExperimental justification: double-check (and re-check) experimental results.\u003c/li\u003e\n\u003cli\u003eCitation justification: clearly point to where the evidence is (e.g., paste the arXiv link).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eWriting style requirements\n\u003cul\u003e\n\u003cli\u003eAddress reviewers in the second person (to reduce distance).\u003c/li\u003e\n\u003cli\u003eBe sincere, well-reasoned, logically clear, and human.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDrafting process\n\u003cul\u003e\n\u003cli\u003eWrite the Chinese responses first.\u003c/li\u003e\n\u003cli\u003eTranslate with DeepL.\u003c/li\u003e\n\u003cli\u003ePolish with GPT.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAfter finishing, \u003cstrong\u003eask your advisor and senior students to review\u003c/strong\u003e it, then submit a unified response.\u003c/p\u003e","title":"Rebuttal Writing Tips"},{"content":"Loss functions for classification Cross Entropy $$ \\text{H}_p(q) = \\sum_x q(x) \\log_2(\\frac{1}{p(x)}) = - \\sum_x q(x) \\log_2(p(x)) $$\nCross entropy provides a way to measure the difference between two probability distributions. The more different $p$ and $q$ are, the larger the cross entropy of $p$ with respect to $q$ becomes compared to the entropy of $p$.\nIn practice:\n$$ \\text{L} = - \\sum_x q(y|x) \\log_2(p(y|x)) = - \\frac{1}{N} \\sum_x \\sum_c y_{xc} \\log_2(p(y_c|x)) $$\nwhere $N$ is the number of samples, $x$ is a sample, $c$ is the class index, and $y_{xc}$ is the one-hot label for sample $x$. Only when $c$ matches the ground-truth class $\\hat{c}$ do we have $y_{x\\hat{c}} = 1$ (i.e., $q(y_{\\hat{c}}|x)=1/N$); otherwise $y_{xc}=0$.\nExample:\nPrediction (after softmax) Ground truth [0.1, 0.2, 0.7] [0, 0, 1] [0.3, 0.4, 0.3] [0, 1, 0] [0.1, 0.2, 0.7] [1, 0, 0] Loss values:\n$$ \\text{sample 1 Loss} = - (0 \\times \\log{0.1} + 0 \\times \\log{0.2} + 1 \\times \\log{0.7}) = 0.36 $$\n$$ \\text{sample 2 Loss} = - (0 \\times \\log{0.3} + 1 \\times \\log{0.4} + 0 \\times \\log{0.3}) = 0.92 $$\n$$ \\text{sample 3 Loss} = - (1 \\times \\log{0.1} + 0 \\times \\log{0.2} + 0 \\times \\log{0.7}) = 2.30 $$\n$$ \\text{L} = \\frac{0.36+0.92+2.3}{3} = 1.19 $$\nKL Divergence $$ \\text{D}_{\\text{KL}}(q | p) = - \\sum_i q(x) \\log_2(p(x)) + \\sum_x p(x) \\log_2(p(x)) = \\text{H}_p(q) - \\text{H}(p) $$\nThis subtracts the entropy of $p$ from cross entropy, measuring the distance between two distributions.\nIn neural network training, $p$ is often the label distribution and $\\text{H}(p)$ is constant, so KL divergence becomes equivalent to cross entropy. Cross entropy is used more widely because it avoids explicitly computing entropy and is simpler.\nBinary Cross Entropy Model prediction:\n$$ P_\\theta(y=1)=\\theta ~~~~~~~ P_\\theta(y=0)=1 - \\theta $$\nCombine the two:\n$$ p_\\theta(y) = \\theta^y(1-\\theta)^{(1-y)} $$\nThe log-likelihood over $N$ data points:\n$$ l(\\theta) = \\log \\prod^N_{i=1} p_\\theta(y_i) = \\log \\prod^N_{i=1}\\theta^y(1-\\theta)^{(1-y)} = \\sum_{i=1}^N [y_i\\log \\theta + (1-y_i)\\log(1-\\theta)] $$\nThis is exactly the cross entropy between $y_i$ and $\\theta$, i.e., $H_y(\\theta)$.\nBalanced Cross Entropy To address class imbalance, we can add a weight to cross entropy. For binary classification, the BCE loss is:\n$$ \\text{L} = - \\sum_{i=1}^N [y_i\\log p + (1-y_i)\\log(1-p)] $$\nRewrite it as:\n$$ \\text{L} = \\begin{cases} -\\log(p) \u0026amp; \\text{if}~y=1 \\ -\\log(1-p) \u0026amp; \\text{otherwise} \\end{cases} $$\nDefine:\n$$ p_t= \\begin{cases} \u0026amp; p \u0026amp; \\text{if}~y=1 \\ \u0026amp; 1-p \u0026amp; \\text{otherwise} \\end{cases} $$\nso:\n$$ \\text{L} = -\\log(p_t) $$\nAdd a class weight:\n$$ \\text{L} = -\\alpha_t\\log(p_t) $$\nwhere $\\alpha_t=\\alpha$ when $y=1$, and $\\alpha_t=1-\\alpha$ when $y=0$. If there are $n$ negative samples ($y=0$) and $m$ positive samples ($y=1$), then $\\frac{\\alpha}{1-\\alpha}=\\frac{n}{m}$.\nBalanced cross entropy helps with imbalance, but not with easy vs hard examples. For that, see Focal Loss.\nFocal Loss $$ \\text{FL}(p_t) = (1-p_t)^\\gamma\\log(p_t) $$\nHere $p_t$ is the predicted probability of the ground-truth class. Since $-\\log(p_t)$ is the same as cross entropy, a smaller $p_t$ (worse prediction) increases $(1-p_t)^\\gamma$, making the loss focus more on hard examples. In other words, focal loss increases the contribution of hard samples to both loss and gradients.\nThe earlier $\\alpha_t$ term is a class weight. For imbalanced datasets, we usually assign a larger weight to the minority class. In multi-class problems, $\\alpha_t$ is typically a vector of length equal to the number of classes; a common heuristic is to set it proportional to the inverse frequency of each class.\nLovasz Loss Deriving Lovasz Loss IoU (intersection-over-union, also called the Jaccard index) is a common metric in segmentation. A natural question is: can we directly optimize IoU as a loss?\nIoU definition:\n$$ J_c(y^*, \\widetilde{y}) = \\frac{ | { y^* = c } \\cap { \\widetilde{y} = c } | }{ | { y^* = c } \\cup { \\widetilde{y} = c } | } $$\nwhere $y^{*}$ is the ground-truth label, $\\widetilde{y}$ is the prediction, and $\\vert \\cdot \\vert$ is set cardinality. Because $J_c$ is in $[0,1]$, we can define a discrete loss:\n$$ \\Delta_{J_c}(y^{*},\\widetilde{y})=1-J_c(y^{*},\\widetilde{y}) $$\nBut this loss is discrete and not directly differentiable. We need a continuous extension.\nRewrite $\\Delta_{J_c}$ as:\n$$ \\Delta_{J_c} = 1-J_c(y^{*},\\widetilde{y}) = \\frac{\\vert M_c \\vert}{\\vert {y^{*}=c} \\cup M_c \\vert} \\tag{1} $$\nwhere $M_c(y^{*},\\widetilde{y}) = {y^{*}=c,\\widetilde{y}\\neq c} \\cup {y^{*} \\neq c,\\widetilde{y}=c}$ captures mismatched pixels between prediction and ground truth. The domain of $M_c$ is ${0,1}^p$ (i.e., $M_c \\in {0,1}^p$), where $p$ is the number of pixels.\nBecause (1) is a submodular function, we can apply a smooth extension.\nDefinition 1 A set function $\\Delta:{0,1}^p \\rightarrow \\mathbb{R}$ is submodular if for all sets $A,B \\in {0,1}^p$:\n$$ \\Delta(A) + \\Delta(B) \\geq \\Delta(A \\cup B) + \\Delta(A \\cap B) $$\nDefinition 2 (Lovasz extension) Given a set function $\\Delta:{0,1}^p \\rightarrow \\mathbb{R}$ with $\\Delta(\\pmb{0})=0$, its Lovasz extension is:\n$$ \\overline{\\Delta} = \\sum_{i=1}^p m_i g_i(\\pmb{m}) \\tag{2} $$\n$$ g_i(m) = \\Delta({\\pi_1,\\cdots,\\pi_i}) - \\Delta({\\pi_1,\\cdots,\\pi_{i-1}}) $$\nHere $\\pi$ is an index array obtained by sorting the elements of $\\pmb{m}$ in descending order (e.g., $x_{\\pi_1} \\geq x_{\\pi_2} \\geq \\cdots \\geq x_{\\pi_p}$).\nNow $\\overline{\\Delta}$ becomes a continuous, piecewise-linear function. We can differentiate it w.r.t. the error vector $m$, and the derivative is $g(m)$.\nLovasz Loss for multi-class segmentation Let $F_i(c)$ be the unnormalized score (logit) for pixel $i$ being class $c$. After softmax, the probability is:\n$$ f_i(c) = \\frac{e^{F_i(c)}}{\\sum_{c\u0026rsquo; \\in C} e^{F_i(c\u0026rsquo;)}} $$\nThen the $m_i(c)$ in (2) can be defined as:\n$$ m_i(c) = \\begin{cases} \u0026amp; 1-f_i(c) \u0026amp; \\text{if}~c=y_i^{*} \\ \u0026amp; f_i(c) \u0026amp; \\text{otherwise} \\end{cases} $$\nThe per-class loss:\n$$ loss(\\pmb{f}(c)) = \\overline{\\Delta_{J_c}}(\\pmb{m}(c)) $$\nAnd averaging over classes (consistent with mIoU):\n$$ loss(\\pmb{f}) = \\frac{1}{\\vert C \\vert} \\sum_{c \\in C} \\overline{\\Delta_{J_c}}(\\pmb{m}(c)) $$\nImplementation sketch for multi-class Lovasz Loss Step 1 Compute prediction errors\nsigns = 2. * predictions.float() - 1. errors = (1. - logits * Variable(signs)) errors_sorted, perm = torch.sort(errors, dim=0, descending=True) This yields $m_i$ in (2).\nStep 2 Compute IoU (Jaccard) curve\ngts = gt_sorted.sum() intersection = gts - gt_sorted.float().cumsum(0) union = gts + (1 - gt_sorted).float().cumsum(0) jaccard = 1. - intersection / union This corresponds to (1), i.e., the IoU curve.\nStep 3 Convert IoU curve into Lovasz gradient\njaccard[1:p] = jaccard[1:p] - jaccard[0:-1] This yields $g_i(\\pmb{m})$ in (2).\nStep 4 Compute the final loss\nloss = torch.dot(F.relu(errors_sorted), Variable(grad)) This corresponds to the Lovasz extension value in (2).\n","permalink":"https://cspaulia.github.io/cspaulia-blog/en/posts/loss/","summary":"\u003ch2 id=\"loss-functions-for-classification\"\u003eLoss functions for classification\u003c/h2\u003e\n\u003ch3 id=\"cross-entropy\"\u003eCross Entropy\u003c/h3\u003e\n\u003cp\u003e$$\n\\text{H}_p(q) = \\sum_x q(x) \\log_2(\\frac{1}{p(x)}) = - \\sum_x q(x) \\log_2(p(x))\n$$\u003c/p\u003e\n\u003cp\u003eCross entropy provides a way to measure the difference between two probability distributions. The more different $p$ and $q$ are, the larger the cross entropy of $p$ with respect to $q$ becomes compared to the entropy of $p$.\u003c/p\u003e\n\u003cp\u003eIn practice:\u003c/p\u003e\n\u003cp\u003e$$\n\\text{L} = - \\sum_x q(y|x) \\log_2(p(y|x))\n= - \\frac{1}{N} \\sum_x \\sum_c y_{xc} \\log_2(p(y_c|x))\n$$\u003c/p\u003e","title":"100 Loss Functions (Work in Progress)"},{"content":"","permalink":"https://cspaulia.github.io/cspaulia-blog/en/series/","summary":"series","title":"Series"}]