<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Training Primitives and Resource Accounting | cspaulia-blog</title><meta name=keywords content="PyTorch,FLOPs,Memory,Training"><meta name=description content="A summary of training primitives and resource accounting in deep learning."><meta name=author content="CSPaulia"><link rel=canonical href=https://cspaulia.github.io/cspaulia-blog/en/posts/primitives/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/cspaulia-blog/assets/css/stylesheet.4a40da687e9ad320449d5d267445e114ee7b6620a3d534bde2b12baa408f07f5.css integrity="sha256-SkDaaH6a0yBEnV0mdEXhFO57ZiCj1TS94rErqkCPB/U=" rel="preload stylesheet" as=style><link rel=icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://cspaulia.github.io/cspaulia-blog/posts/primitives/><link rel=alternate hreflang=en href=https://cspaulia.github.io/cspaulia-blog/en/posts/primitives/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1,trust:!0,strict:!1})})</script><meta property="og:url" content="https://cspaulia.github.io/cspaulia-blog/en/posts/primitives/"><meta property="og:site_name" content="cspaulia-blog"><meta property="og:title" content="Training Primitives and Resource Accounting"><meta property="og:description" content="A summary of training primitives and resource accounting in deep learning."><meta property="og:locale" content="en-US"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-28T10:00:00+08:00"><meta property="article:modified_time" content="2026-01-30T16:43:57+08:00"><meta property="article:tag" content="PyTorch"><meta property="article:tag" content="FLOPs"><meta property="article:tag" content="Memory"><meta property="article:tag" content="Training"><meta property="og:image" content="https://cspaulia.github.io/cspaulia-blog/primitive.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cspaulia.github.io/cspaulia-blog/primitive.jpg"><meta name=twitter:title content="Training Primitives and Resource Accounting"><meta name=twitter:description content="A summary of training primitives and resource accounting in deep learning."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://cspaulia.github.io/cspaulia-blog/en/posts/"},{"@type":"ListItem","position":2,"name":"Training Primitives and Resource Accounting","item":"https://cspaulia.github.io/cspaulia-blog/en/posts/primitives/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Training Primitives and Resource Accounting","name":"Training Primitives and Resource Accounting","description":"A summary of training primitives and resource accounting in deep learning.","keywords":["PyTorch","FLOPs","Memory","Training"],"articleBody":"Key resources in deep learning Memory (GB): stores parameters, gradients, optimizer states, activations, etc. Compute (FLOPs): number of floating-point operations required for training. 1. Tensor basics and memory management 1.1. Creating and storing tensors A tensor is the basic unit for storing parameters, gradients, optimizer states, data, and activations. PyTorch supports many ways to create tensors (e.g., torch.zeros, torch.ones, torch.randn). x = torch.tensor([[1., 2, 3], [4, 5, 6]]) # @inspect x x = torch.zeros(4, 8) # 4x8 matrix of all zeros @inspect x x = torch.ones(4, 8) # 4x8 matrix of all ones @inspect x x = torch.randn(4, 8) # 4x8 matrix of iid Normal(0, 1) samples @inspect x You can also allocate memory first and then initialize values. x = torch.empty(4, 8) # 4x8 matrix of uninitialized values @inspect x nn.init.trunc_normal_(x, mean=0, std=1, a=-2, b=2) # @inspect x Tensor memory usage is determined by the number of elements and the dtype. 1.2. Common dtypes Parameters, gradients, activations, and optimizer states are almost always stored as floating-point values.\n1.2.1 float32 (single precision) Default dtype, 4 bytes, wide dynamic range.\nMemory is determined by (i) the number of values and (ii) the dtype.\nx = torch.zeros(4, 8) # @inspect x assert x.dtype == torch.float32 # Default type assert x.numel() == 4 * 8 assert x.element_size() == 4 # Float is 4 bytes assert get_memory_usage(x) == 4 * 8 * 4 # 128 bytes 1.2.2 float16 (half precision) 2 bytes. Saves memory but has a smaller dynamic range and is more prone to underflow.\nx = torch.zeros(4, 8, dtype=torch.float16) # @inspect x assert x.element_size() == 2 # Float is 2 bytes Smaller dynamic range (easy to underflow):\nx = torch.tensor([1e-8], dtype=torch.float16) # @inspect x assert x == 0 # Underflow! 1.2.3 bfloat16 2 bytes. Same dynamic range as float32, slightly lower precision.\nLess likely to underflow:\nx = torch.tensor([1e-8], dtype=torch.bfloat16) # @inspect x assert x != 0 # No underflow! Compare dynamic range and memory usage across dtypes:\nfloat32_info = torch.finfo(torch.float32) # @inspect float32_info float16_info = torch.finfo(torch.float16) # @inspect float16_info bfloat16_info = torch.finfo(torch.bfloat16) # @inspect bfloat16_info Output:\nfloat32 info=\"finfo(resolution=1e-06,min=-3.40282e+38,max=3.40282e+38, eps=1.19209e-07,smallest normal=1.17549e-38,tiny=1.17549e-38dtype=float32)\" float1l6 info=\"finfo(resolution=0.001,min=-65504,max=65504, eps=0.000976562,sma1lest norma1=6.10352e-05,tiny=6.10352e-05,dtype=float16)\" bfloat16 info=\"finfo(resolution=0.01, min=-3.38953e+38,max=3.38953e+38, eps=0.0078125,smallest normal=1.17549e38, tiny=1.17549e-38,dtype=bfloat16)\" 1.2.4 fp8 FP8 primer (NVIDIA docs)\n1 byte. Extreme compression, designed for newer hardware (e.g., H100).\nH100 supports two FP8 formats: E4M3 (range [-448, 448]) and E5M2 ([-57344, 57344]).\n1.2.5 Mixed-precision training TODO: expand mixed-precision training notes\nUsing different dtypes comes with trade-offs:\nHigher precision: more accurate and stable, but needs more memory and more compute. Lower precision: less accurate/stable, but reduces memory and compute requirements. A common mixed-precision recipe:\nUse bfloat16/fp8 for forward activations. Keep parameters and gradients in float32. Mixed-precision training paper/PyTorch AMP docs/NVIDIA mixed-precision training docs\n2. Compute resources 2.1. Tensor operations 2.1.1. Storage and views A tensor is a memory pointer + metadata (describing how to index into storage, e.g., strides). PyTorch stride definition\nFor a tensor:\nx = torch.tensor([ [0., 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], ]) To move to the next row (dim 0), you skip 4 elements in storage:\nassert x.stride(0) == 4 To move to the next column (dim 1), you skip 1 element in storage:\nassert x.stride(1) == 1 Indexing into storage using strides:\nr, c = 1, 2 index = r * x.stride(0) + c * x.stride(1) # @inspect index assert index == 6 2.1.2. Tensor slicing Many tensor operations return a different view of the same underlying storage (no copy), so modifying one view can affect another.\nx = torch.tensor([[1., 2, 3], [4, 5, 6]]) # @inspect x Operation 1: get row 0\ndef same_storage(x: torch.Tensor, y: torch.Tensor): return x.untyped_storage().data_ptr() == y.untyped_storage().data_ptr() y = x[0] # @inspect y assert torch.equal(y, torch.tensor([1., 2, 3])) assert same_storage(x, y) Operation 2: get column 1\ny = x[:, 1] # @inspect y assert torch.equal(y, torch.tensor([2, 5])) assert same_storage(x, y) Operation 3: reshape a 2×3 matrix into a 3×2 matrix (view)\ny = x.view(3, 2) # @inspect y assert torch.equal(y, torch.tensor([[1, 2], [3, 4], [5, 6]])) assert same_storage(x, y) Operation 4: transpose the matrix\ny = x.transpose(1, 0) # @inspect y assert torch.equal(y, torch.tensor([[1, 4], [2, 5], [3, 6]])) assert same_storage(x, y) Operation 5: modifying x also modifies y\nx[0][0] = 100 # @inspect x, @inspect y assert y[0][0] == 100 Operation 6: storage contiguity (contiguous)\nSome transforms (e.g., transpose, some view patterns) make a tensor non-contiguous in memory. A non-contiguous tensor often cannot be reshaped with view without copying.\nx = torch.tensor([[1., 2, 3], [4, 5, 6]]) # @inspect x y = x.transpose(1, 0) # @inspect y assert not y.is_contiguous() try: y.view(2, 3) assert False except RuntimeError as e: assert \"view size is not compatible with input tensor's size and stride\" in str(e) You can force a tensor to be contiguous, but this allocates new storage.\ny = x.transpose(1, 0).contiguous().view(2, 3) # @inspect y assert not same_storage(x, y) 2.1.3. Tensor elementwise operations (elementwise) These operations apply a function to each element and return a tensor of the same shape.\nx = torch.tensor([1, 4, 9]) assert torch.equal(x.pow(2), torch.tensor([1, 16, 81])) assert torch.equal(x.sqrt(), torch.tensor([1, 2, 3])) assert torch.equal(x.rsqrt(), torch.tensor([1, 1 / 2, 1 / 3])) # i -\u003e 1/sqrt(x_i) assert torch.equal(x + x, torch.tensor([2, 8, 18])) assert torch.equal(x * 2, torch.tensor([2, 8, 18])) assert torch.equal(x / 0.5, torch.tensor([2, 8, 18])) triu constructs the upper-triangular part of a matrix, which is useful for building causal attention masks.\nx = torch.ones(3, 3).triu() # @inspect x assert torch.equal(x, torch.tensor([ [1, 1, 1], [0, 1, 1], [0, 0, 1]], )) 2.1.4. Tensor multiplication x = torch.ones(16, 32) w = torch.ones(32, 2) y = x @ w assert y.size() == torch.Size([16, 2]) In practice, we apply this multiplication per example in the batch and per token position in the sequence.\nx = torch.ones(4, 8, 16, 32) ## [batch, sequence, H, W] w = torch.ones(32, 2) y = x @ w assert y.size() == torch.Size([4, 8, 16, 2]) 2.2. Tensor einops 2.2.1. Why use einops x = torch.ones(2, 2, 3) # batch, sequence, hidden @inspect x y = torch.ones(2, 2, 3) # batch, sequence, hidden @inspect y z = x @ y.transpose(-2, -1) # batch, sequence, sequence @inspect z What do dimensions -2 and -1 mean?\nTensor dimensions are easy to mix up.\neinops is a Python library for naming tensor dimensions and transforming tensors with readable patterns.\neinops docs\n2.2.2. Naming dimensions with jaxtyping How to define tensor dimensions.\nOld approach\nx = torch.ones(2, 2, 1, 3) # batch seq heads hidden @inspect x New approach (jaxtyping)\nx: Float[torch.Tensor, \"batch seq heads hidden\"] = torch.ones(2, 2, 1, 3) # @inspect x 2.2.3. einops operations Operation 1: einsum einsum is a general-purpose tensor contraction API with a compact, documented notation.\nDefine two tensors\nx: Float[torch.Tensor, \"batch seq1 hidden\"] = torch.ones(2, 3, 4) # @inspect x y: Float[torch.Tensor, \"batch seq2 hidden\"] = torch.ones(2, 3, 4) # @inspect y Old approach\nz = x @ y.transpose(-2, -1) # batch, sequence, sequence @inspect z New approach (einops.einsum)\nz = einsum(x, y, \"batch seq1 hidden, batch seq2 hidden -\u003e batch seq1 seq2\") # @inspect z z = einsum(x, y, \"... seq1 hidden, ... seq2 hidden -\u003e ... seq1 seq2\") # @inspect z Any dimensions that do not appear in the output pattern are reduced (summed over).\nOperation 2: reduce You can reduce a tensor along one or more axes, e.g. sum, mean, max, min.\nDefine a tensor\nx: Float[torch.Tensor, \"batch seq hidden\"] = torch.ones(2, 3, 4) # @inspect x Old approach\ny = x.mean(dim=-1) # @inspect y New approach (einops.reduce)\ny = reduce(x, \"... hidden -\u003e ...\", \"sum\") # @inspect y Operation 3: rearrange Sometimes a single axis is a flattened representation of multiple logical axes, and you want to operate on just one of them.\nDefine a tensor\nx: Float[torch.Tensor, \"batch seq total_hidden\"] = torch.ones(2, 3, 8) # @inspect x Here total_hidden is the flattened representation of heads * hidden1.\nw: Float[torch.Tensor, \"hidden1 hidden2\"] = torch.ones(4, 4) Split total_hidden into heads and hidden1:\nx = rearrange(x, \"... (heads hidden1) -\u003e ... heads hidden1\", heads=2) # @inspect x Merge heads and hidden2 back into a single axis:\nx = rearrange(x, \"... heads hidden2 -\u003e ... (heads hidden2)\") # @inspect x 2.3. Tensor operation FLOPs A floating-point operation (FLOP) is a basic arithmetic op such as addition ($x + y$) or multiplication ($x \\cdot y$).\n⚠ WarningTwo extremely confusing abbreviations (they sound the same!):\nFLOPs: the number of floating-point operations (a measure of compute) FLOP/s: floating-point operations per second (also written as FLOPS), a measure of hardware throughput × Training GPT-3 (2020) requires 3.14e23 FLOPs\nTraining GPT-4 (2023) requires about 2e25 FLOPs\nA100 peak throughput: 312 teraFLOP/s for torch.bfloat16 or torch.float16; 19.5 teraFLOP/s for torch.float32\nH100 peak throughput: 1979 teraFLOP/s for torch.bfloat16 or torch.float16 (often ~50% lower in practice); 67.5 teraFLOP/s for torch.float32\nWith 8× H100 GPUs, two weeks gives:\ntotal_flops = 8 * (60 * 60 * 24 * 7) * h100_flop_per_sec # @inspect total_flops Output:\ntotal_flops = 4.788e+21 2.3.1. FLOPs calculation Linear layer / matrix multiply: for a $B \\times D$ matrix times a $D \\times K$ matrix, the FLOPs are:\nx = torch.ones(B, D, device=device) w = torch.randn(D, K, device=device) y = x @ w actual_num_flops = 2 * B * D * K # @inspect actual_num_flops For each triple $(i, j, k)$, you do one multiply $(x[i][j] * w[j][k])$ and one add, which is why the factor is 2.\nElementwise ops: an $m \\times n$ tensor costs $O(mn)$ FLOPs.\nAddition: adding two $m \\times n$ tensors costs $mn$ FLOPs.\nIn practice, matrix multiplication dominates FLOPs in deep learning. A good first-order estimate is to count the matmul-like operations.\n2.3.2. Model FLOPs utilization (MFU) Definition: (actual FLOP/s) / (promised FLOP/s), ignoring communication overhead.\nIn practice, MFU $\\ge 0.5$ is already great (and can be higher when matmuls dominate).\n2.3.3. Summary Matrix multiplication dominates: $(2 \\times m \\times n \\times p)$ FLOPs.\nFLOP/s depends on hardware (H100 » A100) and dtype (bfloat16 » float32).\nMFU: (actual FLOP/s) / (promised FLOP/s).\n2.4. Gradients and backpropagation 2.4.1. Gradient basics Assume we have a simple linear model:\n$$ y = 0.5 \\cdot (x \\times w - 5)^2 $$\nForward pass: compute the loss\nx = torch.tensor([1., 2, 3]) w = torch.tensor([1., 1, 1], requires_grad=True) # Want gradient pred_y = x @ w loss = 0.5 * (pred_y - 5).pow(2) Backward pass: compute gradients\nloss.backward() assert loss.grad is None assert pred_y.grad is None assert x.grad is None assert torch.equal(w.grad, torch.tensor([1, 2, 3])) 2.4.2. Gradient FLOPs To reason about gradient FLOPs, consider a simple two-layer linear example:\nx = torch.ones(B, D, device=device) w1 = torch.randn(D, D, device=device, requires_grad=True) w2 = torch.randn(D, K, device=device, requires_grad=True) h1 = x @ w1 h2 = h1 @ w2 loss = h2.pow(2).mean() Recall the forward-pass FLOPs:\nMultiply x[i][j] * w1[j][k] Add to h1[i][k] Multiply h1[i][j] * w2[j][k] Add to h2[i][k] num_forward_flops = (2 * B * D * D) + (2 * B * D * K) # @inspect num_forward_flops Backprop path: loss –\u003e h2 –\u003e w2 –\u003e h1 –\u003e w1 –\u003e x\nFor parameter $w2$, the chain rule gives:\n$$ \\text{w2.grad} = \\frac{\\partial loss}{\\partial w2} = \\frac{\\partial loss}{\\partial h2} \\cdot \\frac{\\partial h2}{\\partial w2} $$ $$ w2.grad[j,k] = \\frac{\\partial loss}{\\partial w2[j, k]} = \\sum_{i=0}^{N-1} \\frac{\\partial loss}{\\partial h2[i, k]} \\cdot \\frac{\\partial h2[i, k]}{\\partial w2[j, k]} = \\sum_{i=0}^{N-1} h2.grad[i,k] \\cdot h1[i,j] $$\nFor each triple $(i, j, k)$, you do one multiply and one add, so:\nnum_backward_flops += 2 * B * D * K # @inspect num_backward_flops There are four gradient computations in this toy graph, so the total backward FLOPs are:\nnum_backward_flops = (2 + 2) * B * D * K + (2 + 2) * B * D * D # @inspect num_backward_flops i Summary Forward pass: 2 × (# data points) × (# parameters) FLOPs Backward pass: 4 × (# data points) × (# parameters) FLOPs Total: 6 × (# data points) × (# parameters) FLOPs × 3. Models 3.1. Model parameters Model parameters are stored as nn.Parameter objects in PyTorch.\ninput_dim = 16384 output_dim = 32 w = nn.Parameter(torch.randn(input_dim, output_dim)) assert isinstance(w, torch.Tensor) # Behaves like a tensor assert type(w.data) == torch.Tensor # Access the underlying tensor 3.1.1. Parameter initialization Assume we randomly initialize the weight matrix w and multiply it with x.\nx = nn.Parameter(torch.randn(input_dim)) output = x @ w # @inspect output assert output.size() == torch.Size([output_dim]) Output:\noutput = [ 18.919979095458984, ... ] Since $output[k] = x \\times w[:, k]$, the magnitude of each output element grows with input_dim.\nIf input_dim is too large, gradients can blow up, making training unstable.\nWe want the initialization scale to be roughly independent of input_dim, so we scale by $1/\\sqrt{\\text{input_dim}}$.\nw = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim)) output = x @ w # @inspect output Output:\noutput = [ -1.5302726030349731, ... ] This is essentially Xavier initialization.\nFor extra safety, we truncate the normal distribution to [-3, 3] to avoid extreme outliers.\nw = nn.Parameter(nn.init.trunc_normal_(torch.empty(input_dim, output_dim), std=1 / np.sqrt(input_dim), a=-3, b=3)) 3.1.2. Building a model Using a simple linear model as an example:\nclass Linear(nn.Module): \"\"\"Simple linear layer.\"\"\" def __init__(self, input_dim: int, output_dim: int): super().__init__() self.weight = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim)) def forward(self, x: torch.Tensor) -\u003e torch.Tensor: return x @ self.weight class Cruncher(nn.Module): def __init__(self, dim: int, num_layers: int): super().__init__() self.layers = nn.ModuleList([ Linear(dim, dim) for i in range(num_layers) ]) self.final = Linear(dim, 1) def forward(self, x: torch.Tensor) -\u003e torch.Tensor: # Apply linear layers B, D = x.size() for layer in self.layers: x = layer(x) # Apply final head x = self.final(x) assert x.size() == torch.Size([B, 1]) # Remove the last dimension x = x.squeeze(-1) assert x.size() == torch.Size([B]) return x B = 8 # Batch size x = torch.randn(B, D, device=device) y = model(x) assert y.size() == torch.Size([B]) Model parameters:\nparam_sizes = [ (name, param.numel()) for name, param in model.state_dict().items() ] assert param_sizes == [ (\"layers.0.weight\", D * D), (\"layers.1.weight\", D * D), (\"final.weight\", D), ] num_parameters = get_num_parameters(model) assert num_parameters == (D * D) + (D * D) + D 3.2. Model training 3.2.1. Randomness Randomness appears in many places: parameter init, dropout, data shuffling, etc. For reproducibility, set seeds explicitly whenever you rely on randomness. Determinism is especially useful for debugging, because it makes issues easier to reproduce. In practice you should set seeds in three places (PyTorch, NumPy, Python’s random). # Torch seed = 0 torch.manual_seed(seed) # NumPy import numpy as np np.random.seed(seed) # Python import random random.seed(seed) 3.2.2. Data loading In language models, data can be represented as sequences of integers (tokens).\nYou can serialize sequences with a NumPy array:\norig_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.int32) orig_data.tofile(\"data.npy\") You can load the data with NumPy.\nIf you don’t want to load the entire dataset into memory (some datasets are huge; e.g., LLaMA can be ~2.8TB), you can use memmap to map only the accessed parts.\ndata = np.memmap(\"data.npy\", dtype=np.int32) assert np.array_equal(data, orig_data) A dataloader generates a batch of training data:\ndef get_batch(data: np.array, batch_size: int, sequence_length: int, device: str) -\u003e torch.Tensor: # Sample batch_size random positions into data. start_indices = torch.randint(len(data) - sequence_length, (batch_size,)) assert start_indices.size() == torch.Size([batch_size]) # Index into the data. x = torch.tensor([data[start:start + sequence_length] for start in start_indices]) assert x.size() == torch.Size([batch_size, sequence_length]) # Pinned memory if torch.cuda.is_available(): x = x.pin_memory() x = x.to(device, non_blocking=True) return x B = 2 # Batch size L = 4 # Length of sequence x = get_batch(data, batch_size=B, sequence_length=L, device=get_device()) assert x.size() == torch.Size([B, L]) By default, CPU tensors live in paged memory. You can explicitly pin them via x = x.pin_memory().\nThis allows two tasks to overlap:\nFetch the next batch on the CPU Process the current x on the GPU 3.2.3. Optimizer We’ll use the familiar linear example again.\nB = 2 D = 4 num_layers = 2 model = Cruncher(dim=D, num_layers=num_layers).to(get_device()) Define an AdaGrad optimizer\nmomentum = SGD + exponential averaging of grad AdaGrad = SGD + averaging by grad^2 RMSProp = AdaGrad + exponentially averaging of grad^2 Adam = RMSProp + momentum AdaGrad\nclass AdaGrad(torch.optim.Optimizer): def __init__(self, params: Iterable[nn.Parameter], lr: float = 0.01): super(AdaGrad, self).__init__(params, dict(lr=lr)) def step(self): for group in self.param_groups: lr = group[\"lr\"] for p in group[\"params\"]: # Optimizer state state = self.state[p] grad = p.grad.data # Get squared gradients g2 = sum_{i","wordCount":"3070","inLanguage":"en","image":"https://cspaulia.github.io/cspaulia-blog/primitive.jpg","datePublished":"2025-07-28T10:00:00+08:00","dateModified":"2026-01-30T16:43:57+08:00","author":{"@type":"Person","name":"CSPaulia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://cspaulia.github.io/cspaulia-blog/en/posts/primitives/"},"publisher":{"@type":"Organization","name":"cspaulia-blog","logo":{"@type":"ImageObject","url":"https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://cspaulia.github.io/cspaulia-blog/en/ accesskey=h title="Home (Alt + H)"><img src=https://cspaulia.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://cspaulia.github.io/cspaulia-blog/ title=中文 aria-label=中文>中文</a></li></ul></div></div><ul id=menu><li><a href=https://cspaulia.github.io/cspaulia-blog/en/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/series/ title=Series><span>Series</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://cspaulia.github.io/cspaulia-blog/en/>Home</a>&nbsp;»&nbsp;<a href=https://cspaulia.github.io/cspaulia-blog/en/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Training Primitives and Resource Accounting</h1><div class=post-description>A summary of training primitives and resource accounting in deep learning.</div><div class=post-meta><span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg> July 28, 2025</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 19l7-7 3 3-7 7-3-3z"/><path d="M18 13l-1.5-7.5L2 2l3.5 14.5L13 18l5-5z"/><path d="M2 2l7.586 7.586"/><circle cx="11" cy="11" r="2"/></svg> January 30, 2026</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><path d="M14 2v6h6"/><line x1="8" y1="13" x2="16" y2="13"/><line x1="8" y1="17" x2="16" y2="17"/><line x1="8" y1="9" x2="12" y2="9"/></svg> 3070 words</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg> 15 min</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg> CSPaulia</span>
&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://cspaulia.github.io/cspaulia-blog/posts/primitives/>中文</a></li></ul><span id=busuanzi_container_page_pv>｜ <span id=busuanzi_value_page_pv></span> views</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#key-resources-in-deep-learning aria-label="Key resources in deep learning">Key resources in deep learning</a></li><li><a href=#1-tensor-basics-and-memory-management aria-label="1. Tensor basics and memory management">1. Tensor basics and memory management</a><ul><li><a href=#11-creating-and-storing-tensors aria-label="1.1. Creating and storing tensors">1.1. Creating and storing tensors</a></li><li><a href=#12-common-dtypes aria-label="1.2. Common dtypes">1.2. Common dtypes</a></li></ul></li><li><a href=#2-compute-resources aria-label="2. Compute resources">2. Compute resources</a><ul><li><a href=#21-tensor-operations aria-label="2.1. Tensor operations">2.1. Tensor operations</a></li><li><a href=#22-tensor-einops aria-label="2.2. Tensor einops">2.2. Tensor <code>einops</code></a></li><li><a href=#23-tensor-operation-flops aria-label="2.3. Tensor operation FLOPs">2.3. Tensor operation FLOPs</a></li><li><a href=#24-gradients-and-backpropagation aria-label="2.4. Gradients and backpropagation">2.4. Gradients and backpropagation</a></li></ul></li><li><a href=#3-models aria-label="3. Models">3. Models</a><ul><li><a href=#31-model-parameters aria-label="3.1. Model parameters">3.1. Model parameters</a></li><li><a href=#32-model-training aria-label="3.2. Model training">3.2. Model training</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h2[id],h3[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=key-resources-in-deep-learning>Key resources in deep learning<a hidden class=anchor aria-hidden=true href=#key-resources-in-deep-learning>#</a></h2><ul><li><strong>Memory</strong> (GB): stores parameters, gradients, optimizer states, activations, etc.</li><li><strong>Compute</strong> (FLOPs): number of floating-point operations required for training.</li></ul><hr><h2 id=1-tensor-basics-and-memory-management>1. Tensor basics and memory management<a hidden class=anchor aria-hidden=true href=#1-tensor-basics-and-memory-management>#</a></h2><h3 id=11-creating-and-storing-tensors>1.1. Creating and storing tensors<a hidden class=anchor aria-hidden=true href=#11-creating-and-storing-tensors>#</a></h3><ul><li>A tensor is the basic unit for storing parameters, gradients, optimizer states, data, and activations.</li><li>PyTorch supports many ways to create tensors (e.g., <code>torch.zeros</code>, <code>torch.ones</code>, <code>torch.randn</code>).<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>1.</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>]])</span>  <span class=c1># @inspect x</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>)</span>  <span class=c1># 4x8 matrix of all zeros @inspect x</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>)</span>  <span class=c1># 4x8 matrix of all ones @inspect x</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>)</span>  <span class=c1># 4x8 matrix of iid Normal(0, 1) samples @inspect x</span>
</span></span></code></pre></div>You can also allocate memory first and then initialize values.<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>empty</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>)</span>  <span class=c1># 4x8 matrix of uninitialized values @inspect x</span>
</span></span><span class=line><span class=cl><span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>trunc_normal_</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>mean</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>std</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>a</span><span class=o>=-</span><span class=mi>2</span><span class=p>,</span> <span class=n>b</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>  <span class=c1># @inspect x</span>
</span></span></code></pre></div></li><li>Tensor memory usage is determined by <strong>the number of elements</strong> and <strong>the dtype</strong>.</li></ul><hr><h3 id=12-common-dtypes>1.2. Common dtypes<a hidden class=anchor aria-hidden=true href=#12-common-dtypes>#</a></h3><p>Parameters, gradients, activations, and optimizer states are almost always stored as floating-point values.</p><h4 id=121-float32-single-precision>1.2.1 float32 (single precision)<a hidden class=anchor aria-hidden=true href=#121-float32-single-precision>#</a></h4><p>Default dtype, 4 bytes, wide dynamic range.</p><p align=center><img src=/cspaulia-blog/posts/primitives/fp32.png alt=fp32 loading=lazy></p><p>Memory is determined by (i) the number of values and (ii) the dtype.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>)</span>  <span class=c1># @inspect x</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>dtype</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>float32</span>  <span class=c1># Default type</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=o>==</span> <span class=mi>4</span> <span class=o>*</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>element_size</span><span class=p>()</span> <span class=o>==</span> <span class=mi>4</span>  <span class=c1># Float is 4 bytes</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>get_memory_usage</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>==</span> <span class=mi>4</span> <span class=o>*</span> <span class=mi>8</span> <span class=o>*</span> <span class=mi>4</span>  <span class=c1># 128 bytes</span>
</span></span></code></pre></div><h4 id=122-float16-half-precision>1.2.2 float16 (half precision)<a hidden class=anchor aria-hidden=true href=#122-float16-half-precision>#</a></h4><p>2 bytes. Saves memory but has a smaller dynamic range and is more prone to underflow.</p><p align=center><img src=/cspaulia-blog/posts/primitives/fp16.png alt=fp16 loading=lazy></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>  <span class=c1># @inspect x</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>element_size</span><span class=p>()</span> <span class=o>==</span> <span class=mi>2</span> <span class=c1># Float is 2 bytes</span>
</span></span></code></pre></div><p>Smaller dynamic range (easy to underflow):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>1e-8</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>  <span class=c1># @inspect x</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span> <span class=o>==</span> <span class=mi>0</span>  <span class=c1># Underflow!</span>
</span></span></code></pre></div><h4 id=123-bfloat16>1.2.3 bfloat16<a hidden class=anchor aria-hidden=true href=#123-bfloat16>#</a></h4><p>2 bytes. Same dynamic range as float32, slightly lower precision.</p><p align=center><img src=/cspaulia-blog/posts/primitives/bf16.png alt=bf16 loading=lazy></p><p>Less likely to underflow:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>1e-8</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>  <span class=c1># @inspect x</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span> <span class=o>!=</span> <span class=mi>0</span>  <span class=c1># No underflow!</span>
</span></span></code></pre></div><p>Compare dynamic range and memory usage across dtypes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>float32_info</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>finfo</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>  <span class=c1># @inspect float32_info</span>
</span></span><span class=line><span class=cl><span class=n>float16_info</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>finfo</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span>  <span class=c1># @inspect float16_info</span>
</span></span><span class=line><span class=cl><span class=n>bfloat16_info</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>finfo</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>  <span class=c1># @inspect bfloat16_info</span>
</span></span></code></pre></div><p><strong>Output</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>float32</span> <span class=n>info</span><span class=o>=</span><span class=s2>&#34;finfo(resolution=1e-06,min=-3.40282e+38,max=3.40282e+38, eps=1.19209e-07,smallest normal=1.17549e-38,tiny=1.17549e-38dtype=float32)&#34;</span>
</span></span><span class=line><span class=cl><span class=n>float1l6</span> <span class=n>info</span><span class=o>=</span><span class=s2>&#34;finfo(resolution=0.001,min=-65504,max=65504, eps=0.000976562,sma1lest norma1=6.10352e-05,tiny=6.10352e-05,dtype=float16)&#34;</span>
</span></span><span class=line><span class=cl><span class=n>bfloat16</span> <span class=n>info</span><span class=o>=</span><span class=s2>&#34;finfo(resolution=0.01, min=-3.38953e+38,max=3.38953e+38, eps=0.0078125,smallest normal=1.17549e38, tiny=1.17549e-38,dtype=bfloat16)&#34;</span>
</span></span></code></pre></div><h4 id=124-fp8>1.2.4 fp8<a hidden class=anchor aria-hidden=true href=#124-fp8>#</a></h4><p><a href=https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html>FP8 primer (NVIDIA docs)</a></p><p>1 byte. Extreme compression, designed for newer hardware (e.g., H100).</p><p align=center><img src=fp8.png alt=fp8></p><p>H100 supports two FP8 formats: E4M3 (range [-448, 448]) and E5M2 ([-57344, 57344]).</p><h4 id=125-mixed-precision-training>1.2.5 Mixed-precision training<a hidden class=anchor aria-hidden=true href=#125-mixed-precision-training>#</a></h4><p><input disabled type=checkbox> TODO: expand mixed-precision training notes</p><p>Using different dtypes comes with trade-offs:</p><ul><li>Higher precision: <strong>more accurate and stable</strong>, but needs <strong>more memory</strong> and <strong>more compute</strong>.</li><li>Lower precision: <strong>less accurate/stable</strong>, but reduces <strong>memory</strong> and <strong>compute</strong> requirements.</li></ul><p>A common mixed-precision recipe:</p><ul><li>Use bfloat16/fp8 for forward activations.</li><li>Keep parameters and gradients in float32.</li></ul><p><a href=https://arxiv.org/pdf/1710.03740>Mixed-precision training paper</a>/<a href=https://pytorch.org/docs/stable/amp.html>PyTorch AMP docs</a>/<a href=https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/>NVIDIA mixed-precision training docs</a></p><hr><h2 id=2-compute-resources>2. Compute resources<a hidden class=anchor aria-hidden=true href=#2-compute-resources>#</a></h2><h3 id=21-tensor-operations>2.1. Tensor operations<a hidden class=anchor aria-hidden=true href=#21-tensor-operations>#</a></h3><h4 id=211-storage-and-views>2.1.1. Storage and views<a hidden class=anchor aria-hidden=true href=#211-storage-and-views>#</a></h4><ul><li>A tensor is a <strong>memory pointer</strong> + <strong>metadata</strong> (describing how to index into storage, e.g., strides).</li></ul><p align=center><img src=2D_tensor_strides.png alt=2D_tensor_strides></p><p><a href=https://docs.pytorch.org/docs/stable/generated/torch.Tensor.stride.html>PyTorch stride definition</a></p><p>For a tensor:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mf>0.</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>,</span> <span class=mi>7</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>8</span><span class=p>,</span> <span class=mi>9</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>11</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>12</span><span class=p>,</span> <span class=mi>13</span><span class=p>,</span> <span class=mi>14</span><span class=p>,</span> <span class=mi>15</span><span class=p>],</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span></code></pre></div><p>To move to the next row (dim 0), you skip 4 elements in storage:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>stride</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=o>==</span> <span class=mi>4</span>
</span></span></code></pre></div><p>To move to the next column (dim 1), you skip 1 element in storage:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>stride</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span>
</span></span></code></pre></div><p>Indexing into storage using strides:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>r</span><span class=p>,</span> <span class=n>c</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>index</span> <span class=o>=</span> <span class=n>r</span> <span class=o>*</span> <span class=n>x</span><span class=o>.</span><span class=n>stride</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=o>+</span> <span class=n>c</span> <span class=o>*</span> <span class=n>x</span><span class=o>.</span><span class=n>stride</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># @inspect index</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>index</span> <span class=o>==</span> <span class=mi>6</span>
</span></span></code></pre></div><hr><h4 id=212-tensor-slicing>2.1.2. Tensor slicing<a hidden class=anchor aria-hidden=true href=#212-tensor-slicing>#</a></h4><p>Many tensor operations return a different <strong>view</strong> of the same underlying storage (no copy), so modifying one view can affect another.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>1.</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>]])</span>  <span class=c1># @inspect x</span>
</span></span></code></pre></div><p><strong>Operation 1</strong>: get row 0</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>same_storage</span><span class=p>(</span><span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>y</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>x</span><span class=o>.</span><span class=n>untyped_storage</span><span class=p>()</span><span class=o>.</span><span class=n>data_ptr</span><span class=p>()</span> <span class=o>==</span> <span class=n>y</span><span class=o>.</span><span class=n>untyped_storage</span><span class=p>()</span><span class=o>.</span><span class=n>data_ptr</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>  <span class=c1># @inspect y</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>torch</span><span class=o>.</span><span class=n>equal</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>same_storage</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Operation 2</strong>: get column 1</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span>  <span class=c1># @inspect y</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>torch</span><span class=o>.</span><span class=n>equal</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span> <span class=mi>5</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>same_storage</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Operation 3</strong>: reshape a 2×3 matrix into a 3×2 matrix (view)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>  <span class=c1># @inspect y</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>torch</span><span class=o>.</span><span class=n>equal</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>]]))</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>same_storage</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Operation 4</strong>: transpose the matrix</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>  <span class=c1># @inspect y</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>torch</span><span class=o>.</span><span class=n>equal</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>5</span><span class=p>],</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>6</span><span class=p>]]))</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>same_storage</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Operation 5</strong>: modifying <code>x</code> also modifies <code>y</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=mi>100</span>  <span class=c1># @inspect x, @inspect y</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>y</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span> <span class=o>==</span> <span class=mi>100</span>
</span></span></code></pre></div><p><strong>Operation 6</strong>: storage contiguity (<code>contiguous</code>)</p><p>Some transforms (e.g., <code>transpose</code>, some <code>view</code> patterns) make a tensor non-contiguous in memory. A non-contiguous tensor often cannot be reshaped with <code>view</code> without copying.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>1.</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>]])</span>  <span class=c1># @inspect x</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>  <span class=c1># @inspect y</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=ow>not</span> <span class=n>y</span><span class=o>.</span><span class=n>is_contiguous</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=k>except</span> <span class=ne>RuntimeError</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=s2>&#34;view size is not compatible with input tensor&#39;s size and stride&#34;</span> <span class=ow>in</span> <span class=nb>str</span><span class=p>(</span><span class=n>e</span><span class=p>)</span>
</span></span></code></pre></div><p>You can force a tensor to be contiguous, but this allocates new storage.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># @inspect y</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=ow>not</span> <span class=n>same_storage</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></div><hr><h4 id=213-tensor-elementwise-operations-elementwise>2.1.3. Tensor elementwise operations (elementwise)<a hidden class=anchor aria-hidden=true href=#213-tensor-elementwise-operations-elementwise>#</a></h4><p>These operations apply a function to each element and return a tensor of the same shape.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>9</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>torch</span><span class=o>.</span><span class=n>equal</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>81</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>torch</span><span class=o>.</span><span class=n>equal</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(),</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>torch</span><span class=o>.</span><span class=n>equal</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>rsqrt</span><span class=p>(),</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span> <span class=o>/</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span> <span class=o>/</span> <span class=mi>3</span><span class=p>]))</span>  <span class=c1># i -&gt; 1/sqrt(x_i)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>torch</span><span class=o>.</span><span class=n>equal</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=n>x</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>18</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>torch</span><span class=o>.</span><span class=n>equal</span><span class=p>(</span><span class=n>x</span> <span class=o>*</span> <span class=mi>2</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>18</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>torch</span><span class=o>.</span><span class=n>equal</span><span class=p>(</span><span class=n>x</span> <span class=o>/</span> <span class=mf>0.5</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>18</span><span class=p>]))</span>
</span></span></code></pre></div><p><code>triu</code> constructs the upper-triangular part of a matrix, which is useful for building causal attention masks.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span><span class=o>.</span><span class=n>triu</span><span class=p>()</span>  <span class=c1># @inspect x</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>torch</span><span class=o>.</span><span class=n>equal</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>]],</span>
</span></span><span class=line><span class=cl><span class=p>))</span>
</span></span></code></pre></div><hr><h4 id=214-tensor-multiplication>2.1.4. Tensor multiplication<a hidden class=anchor aria-hidden=true href=#214-tensor-multiplication>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>w</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>y</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>16</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span></span></code></pre></div><p>In practice, we apply this multiplication per example in the batch and per token position in the sequence.</p><p align=center><img src=batch-sequence.png alt=batch-sequence width=75%></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>32</span><span class=p>)</span> <span class=c1>## [batch, sequence, H, W]</span>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>w</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>y</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>16</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span></span></code></pre></div><hr><h3 id=22-tensor-einops>2.2. Tensor <code>einops</code><a hidden class=anchor aria-hidden=true href=#22-tensor-einops>#</a></h3><h4 id=221-why-use-einops>2.2.1. Why use <code>einops</code><a hidden class=anchor aria-hidden=true href=#221-why-use-einops>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># batch, sequence, hidden  @inspect x</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># batch, sequence, hidden  @inspect y</span>
</span></span><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>y</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># batch, sequence, sequence  @inspect z</span>
</span></span></code></pre></div><blockquote><p>What do dimensions <code>-2</code> and <code>-1</code> mean?</p></blockquote><p>Tensor dimensions are easy to mix up.</p><p><code>einops</code> is a Python library for naming tensor dimensions and transforming tensors with readable patterns.</p><p><a href=https://einops.rocks/1-einops-basics/>einops docs</a></p><hr><h4 id=222-naming-dimensions-with-jaxtyping>2.2.2. Naming dimensions with <code>jaxtyping</code><a hidden class=anchor aria-hidden=true href=#222-naming-dimensions-with-jaxtyping>#</a></h4><p>How to define tensor dimensions.</p><p><strong>Old approach</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># batch seq heads hidden  @inspect x</span>
</span></span></code></pre></div><p><strong>New approach (<code>jaxtyping</code>)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq heads hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>  <span class=c1># @inspect x</span>
</span></span></code></pre></div><hr><h4 id=223-einops-operations>2.2.3. <code>einops</code> operations<a hidden class=anchor aria-hidden=true href=#223-einops-operations>#</a></h4><h5 id=operation-1-einsum>Operation 1: <code>einsum</code><a hidden class=anchor aria-hidden=true href=#operation-1-einsum>#</a></h5><p><code>einsum</code> is a general-purpose tensor contraction API with a compact, documented notation.</p><p><strong>Define two tensors</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq1 hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>  <span class=c1># @inspect x</span>
</span></span><span class=line><span class=cl><span class=n>y</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq2 hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>  <span class=c1># @inspect y</span>
</span></span></code></pre></div><p><strong>Old approach</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>y</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># batch, sequence, sequence  @inspect z</span>
</span></span></code></pre></div><p><strong>New approach (<code>einops.einsum</code>)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>einsum</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=s2>&#34;batch seq1 hidden, batch seq2 hidden -&gt; batch seq1 seq2&#34;</span><span class=p>)</span>  <span class=c1># @inspect z</span>
</span></span><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=n>einsum</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=s2>&#34;... seq1 hidden, ... seq2 hidden -&gt; ... seq1 seq2&#34;</span><span class=p>)</span>  <span class=c1># @inspect z</span>
</span></span></code></pre></div><p>Any dimensions that do not appear in the output pattern are reduced (summed over).</p><h5 id=operation-2-reduce>Operation 2: <code>reduce</code><a hidden class=anchor aria-hidden=true href=#operation-2-reduce>#</a></h5><p>You can reduce a tensor along one or more axes, e.g. <code>sum</code>, <code>mean</code>, <code>max</code>, <code>min</code>.</p><p><strong>Define a tensor</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>  <span class=c1># @inspect x</span>
</span></span></code></pre></div><p><strong>Old approach</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># @inspect y</span>
</span></span></code></pre></div><p><strong>New approach (<code>einops.reduce</code>)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>reduce</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=s2>&#34;... hidden -&gt; ...&#34;</span><span class=p>,</span> <span class=s2>&#34;sum&#34;</span><span class=p>)</span>  <span class=c1># @inspect y</span>
</span></span></code></pre></div><h5 id=operation-3-rearrange>Operation 3: <code>rearrange</code><a hidden class=anchor aria-hidden=true href=#operation-3-rearrange>#</a></h5><p>Sometimes a single axis is a flattened representation of multiple logical axes, and you want to operate on just one of them.</p><p><strong>Define a tensor</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;batch seq total_hidden&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>8</span><span class=p>)</span>  <span class=c1># @inspect x</span>
</span></span></code></pre></div><p>Here <code>total_hidden</code> is the flattened representation of <code>heads * hidden1</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>w</span><span class=p>:</span> <span class=n>Float</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=s2>&#34;hidden1 hidden2&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span></code></pre></div><p>Split <code>total_hidden</code> into <code>heads</code> and <code>hidden1</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>rearrange</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=s2>&#34;... (heads hidden1) -&gt; ... heads hidden1&#34;</span><span class=p>,</span> <span class=n>heads</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>  <span class=c1># @inspect x</span>
</span></span></code></pre></div><p>Merge <code>heads</code> and <code>hidden2</code> back into a single axis:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>rearrange</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=s2>&#34;... heads hidden2 -&gt; ... (heads hidden2)&#34;</span><span class=p>)</span>  <span class=c1># @inspect x</span>
</span></span></code></pre></div><hr><h3 id=23-tensor-operation-flops>2.3. Tensor operation FLOPs<a hidden class=anchor aria-hidden=true href=#23-tensor-operation-flops>#</a></h3><p>A floating-point operation (FLOP) is a basic arithmetic op such as addition ($x + y$) or multiplication ($x \cdot y$).</p><div class="alert alert-warning"><div class=alert-icon>⚠</div><div class=alert-content><div class=alert-title>Warning</div><div class=alert-message><p>Two extremely confusing abbreviations (they sound the same!):</p><ul><li>FLOPs: the number of floating-point operations (a measure of compute)</li><li>FLOP/s: floating-point operations per second (also written as FLOPS), a measure of hardware throughput</li></ul></div></div><button class=alert-close onclick=closeAlert(this)>&#215;</button></div><blockquote><p>Training <a href=https://lambda.ai/blog/demystifying-gpt-3>GPT-3 (2020)</a> requires 3.14e23 FLOPs</p><p>Training <a href=https://patmcguinness.substack.com/p/gpt-4-details-revealed>GPT-4 (2023)</a> requires about 2e25 FLOPs</p><p><a href=https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf>A100</a> peak throughput: 312 teraFLOP/s for torch.bfloat16 or torch.float16; 19.5 teraFLOP/s for torch.float32</p><p><a href=https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet>H100</a> peak throughput: 1979 teraFLOP/s for torch.bfloat16 or torch.float16 (often ~50% lower in practice); 67.5 teraFLOP/s for torch.float32</p><p>With 8× H100 GPUs, two weeks gives:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>total_flops</span> <span class=o>=</span> <span class=mi>8</span> <span class=o>*</span> <span class=p>(</span><span class=mi>60</span> <span class=o>*</span> <span class=mi>60</span> <span class=o>*</span> <span class=mi>24</span> <span class=o>*</span> <span class=mi>7</span><span class=p>)</span> <span class=o>*</span> <span class=n>h100_flop_per_sec</span>  <span class=c1># @inspect total_flops</span>
</span></span></code></pre></div><p>Output:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>total_flops = 4.788e+21
</span></span></code></pre></div></blockquote><hr><h4 id=231-flops-calculation>2.3.1. FLOPs calculation<a hidden class=anchor aria-hidden=true href=#231-flops-calculation>#</a></h4><ul><li><p><strong>Linear layer / matrix multiply</strong>: for a $B \times D$ matrix times a $D \times K$ matrix, the FLOPs are:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>D</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>D</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>w</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>actual_num_flops</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>B</span> <span class=o>*</span> <span class=n>D</span> <span class=o>*</span> <span class=n>K</span>  <span class=c1># @inspect actual_num_flops</span>
</span></span></code></pre></div><p>For each triple $(i, j, k)$, you do one multiply $(x[i][j] * w[j][k])$ and one add, which is why the factor is 2.</p></li><li><p><strong>Elementwise ops</strong>: an $m \times n$ tensor costs $O(mn)$ FLOPs.</p></li><li><p><strong>Addition</strong>: adding two $m \times n$ tensors costs $mn$ FLOPs.</p></li></ul><p>In practice, matrix multiplication dominates FLOPs in deep learning. A good first-order estimate is to count the matmul-like operations.</p><hr><h4 id=232-model-flops-utilization-mfu>2.3.2. Model FLOPs utilization (MFU)<a hidden class=anchor aria-hidden=true href=#232-model-flops-utilization-mfu>#</a></h4><p><strong>Definition</strong>: (actual FLOP/s) / (promised FLOP/s), ignoring communication overhead.</p><blockquote><p>In practice, MFU $\ge 0.5$ is already great (and can be higher when matmuls dominate).</p></blockquote><hr><h4 id=233-summary>2.3.3. Summary<a hidden class=anchor aria-hidden=true href=#233-summary>#</a></h4><ul><li><p>Matrix multiplication dominates: $(2 \times m \times n \times p)$ FLOPs.</p></li><li><p>FLOP/s depends on hardware (H100 &#187; A100) and dtype (bfloat16 &#187; float32).</p></li><li><p>MFU: (actual FLOP/s) / (promised FLOP/s).</p></li></ul><hr><h3 id=24-gradients-and-backpropagation>2.4. Gradients and backpropagation<a hidden class=anchor aria-hidden=true href=#24-gradients-and-backpropagation>#</a></h3><h4 id=241-gradient-basics>2.4.1. Gradient basics<a hidden class=anchor aria-hidden=true href=#241-gradient-basics>#</a></h4><p>Assume we have a simple linear model:</p><p>$$
y = 0.5 \cdot (x \times w - 5)^2
$$</p><p><strong>Forward pass</strong>: compute the loss</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>  <span class=c1># Want gradient</span>
</span></span><span class=line><span class=cl><span class=n>pred_y</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>w</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=p>(</span><span class=n>pred_y</span> <span class=o>-</span> <span class=mi>5</span><span class=p>)</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Backward pass</strong>: compute gradients</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>loss</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>pred_y</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>torch</span><span class=o>.</span><span class=n>equal</span><span class=p>(</span><span class=n>w</span><span class=o>.</span><span class=n>grad</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>]))</span>
</span></span></code></pre></div><hr><h4 id=242-gradient-flops>2.4.2. Gradient FLOPs<a hidden class=anchor aria-hidden=true href=#242-gradient-flops>#</a></h4><p>To reason about gradient FLOPs, consider a simple two-layer linear example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>D</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>w1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>D</span><span class=p>,</span> <span class=n>D</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>w2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>D</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>h1</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>w1</span>
</span></span><span class=line><span class=cl><span class=n>h2</span> <span class=o>=</span> <span class=n>h1</span> <span class=o>@</span> <span class=n>w2</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>h2</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span></code></pre></div><blockquote><p>Recall the forward-pass FLOPs:</p><ul><li>Multiply x[i][j] * w1[j][k]</li><li>Add to h1[i][k]</li><li>Multiply h1[i][j] * w2[j][k]</li><li>Add to h2[i][k]</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_forward_flops</span> <span class=o>=</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>B</span> <span class=o>*</span> <span class=n>D</span> <span class=o>*</span> <span class=n>D</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>B</span> <span class=o>*</span> <span class=n>D</span> <span class=o>*</span> <span class=n>K</span><span class=p>)</span>  <span class=c1># @inspect num_forward_flops</span>
</span></span></code></pre></div></blockquote><p>Backprop path: loss &ndash;> h2 &ndash;> w2 &ndash;> h1 &ndash;> w1 &ndash;> x</p><p>For parameter $w2$, the chain rule gives:</p><p>$$
\text{w2.grad} = \frac{\partial loss}{\partial w2} = \frac{\partial loss}{\partial h2} \cdot \frac{\partial h2}{\partial w2}
$$
$$
w2.grad[j,k] = \frac{\partial loss}{\partial w2[j, k]} = \sum_{i=0}^{N-1} \frac{\partial loss}{\partial h2[i, k]} \cdot \frac{\partial h2[i, k]}{\partial w2[j, k]} = \sum_{i=0}^{N-1} h2.grad[i,k] \cdot h1[i,j]
$$</p><p>For each triple $(i, j, k)$, you do one multiply and one add, so:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_backward_flops</span> <span class=o>+=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>B</span> <span class=o>*</span> <span class=n>D</span> <span class=o>*</span> <span class=n>K</span>  <span class=c1># @inspect num_backward_flops</span>
</span></span></code></pre></div><p>There are four gradient computations in this toy graph, so the total backward FLOPs are:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_backward_flops</span> <span class=o>=</span> <span class=p>(</span><span class=mi>2</span> <span class=o>+</span> <span class=mi>2</span><span class=p>)</span> <span class=o>*</span> <span class=n>B</span> <span class=o>*</span> <span class=n>D</span> <span class=o>*</span> <span class=n>K</span> <span class=o>+</span> <span class=p>(</span><span class=mi>2</span> <span class=o>+</span> <span class=mi>2</span><span class=p>)</span> <span class=o>*</span> <span class=n>B</span> <span class=o>*</span> <span class=n>D</span> <span class=o>*</span> <span class=n>D</span>  <span class=c1># @inspect num_backward_flops</span>
</span></span></code></pre></div><p align=center><img src=back_flops.gif alt=back_flops></p><div class="alert alert-info"><div class=alert-icon>i</div><div class=alert-content><div class=alert-title>Summary</div><div class=alert-message><ul><li>Forward pass: 2 × (# data points) × (# parameters) FLOPs</li><li>Backward pass: 4 × (# data points) × (# parameters) FLOPs</li><li>Total: 6 × (# data points) × (# parameters) FLOPs</li></ul></div></div><button class=alert-close onclick=closeAlert(this)>&#215;</button></div><hr><h2 id=3-models>3. Models<a hidden class=anchor aria-hidden=true href=#3-models>#</a></h2><h3 id=31-model-parameters>3.1. Model parameters<a hidden class=anchor aria-hidden=true href=#31-model-parameters>#</a></h3><p>Model parameters are stored as <code>nn.Parameter</code> objects in PyTorch.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>input_dim</span> <span class=o>=</span> <span class=mi>16384</span>
</span></span><span class=line><span class=cl><span class=n>output_dim</span> <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>w</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span>  <span class=c1># Behaves like a tensor</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=nb>type</span><span class=p>(</span><span class=n>w</span><span class=o>.</span><span class=n>data</span><span class=p>)</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span>  <span class=c1># Access the underlying tensor</span>
</span></span></code></pre></div><h4 id=311-parameter-initialization>3.1.1. Parameter initialization<a hidden class=anchor aria-hidden=true href=#311-parameter-initialization>#</a></h4><p>Assume we randomly initialize the weight matrix <code>w</code> and multiply it with <code>x</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>input_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>w</span>  <span class=c1># @inspect output</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>output</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=n>output_dim</span><span class=p>])</span>
</span></span></code></pre></div><p>Output:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>output = [
</span></span><span class=line><span class=cl>  18.919979095458984,
</span></span><span class=line><span class=cl>  ...
</span></span><span class=line><span class=cl>]
</span></span></code></pre></div><p>Since $output[k] = x \times w[:, k]$, the magnitude of each output element grows with <code>input_dim</code>.</p><p>If <code>input_dim</code> is too large, gradients can blow up, making training unstable.</p><p>We want the initialization scale to be roughly independent of <code>input_dim</code>, so we scale by $1/\sqrt{\text{input_dim}}$.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>input_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>w</span>  <span class=c1># @inspect output</span>
</span></span></code></pre></div><p>Output:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>output = [
</span></span><span class=line><span class=cl>  -1.5302726030349731,
</span></span><span class=line><span class=cl>  ...
</span></span><span class=line><span class=cl>]
</span></span></code></pre></div><p>This is essentially <a href=https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf>Xavier initialization</a>.</p><p>For extra safety, we truncate the normal distribution to [-3, 3] to avoid extreme outliers.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>trunc_normal_</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>empty</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>),</span> <span class=n>std</span><span class=o>=</span><span class=mi>1</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>input_dim</span><span class=p>),</span> <span class=n>a</span><span class=o>=-</span><span class=mi>3</span><span class=p>,</span> <span class=n>b</span><span class=o>=</span><span class=mi>3</span><span class=p>))</span>
</span></span></code></pre></div><hr><h4 id=312-building-a-model>3.1.2. Building a model<a hidden class=anchor aria-hidden=true href=#312-building-a-model>#</a></h4><p>Using a simple linear model as an example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Linear</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34;Simple linear layer.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      <span class=bp>self</span><span class=o>.</span><span class=n>weight</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>input_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=k>return</span> <span class=n>x</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Cruncher</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
</span></span><span class=line><span class=cl>          <span class=n>Linear</span><span class=p>(</span><span class=n>dim</span><span class=p>,</span> <span class=n>dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>          <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=bp>self</span><span class=o>.</span><span class=n>final</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>dim</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=c1># Apply linear layers</span>
</span></span><span class=line><span class=cl>      <span class=n>B</span><span class=p>,</span> <span class=n>D</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>          <span class=n>x</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=c1># Apply final head</span>
</span></span><span class=line><span class=cl>      <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>final</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=n>B</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=c1># Remove the last dimension</span>
</span></span><span class=line><span class=cl>      <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=n>B</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>B</span> <span class=o>=</span> <span class=mi>8</span>  <span class=c1># Batch size</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>D</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>y</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=n>B</span><span class=p>])</span>
</span></span></code></pre></div><p>Model parameters:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>param_sizes</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>name</span><span class=p>,</span> <span class=n>param</span><span class=o>.</span><span class=n>numel</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span><span class=o>.</span><span class=n>items</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>param_sizes</span> <span class=o>==</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;layers.0.weight&#34;</span><span class=p>,</span> <span class=n>D</span> <span class=o>*</span> <span class=n>D</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;layers.1.weight&#34;</span><span class=p>,</span> <span class=n>D</span> <span class=o>*</span> <span class=n>D</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=s2>&#34;final.weight&#34;</span><span class=p>,</span> <span class=n>D</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>num_parameters</span> <span class=o>=</span> <span class=n>get_num_parameters</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>num_parameters</span> <span class=o>==</span> <span class=p>(</span><span class=n>D</span> <span class=o>*</span> <span class=n>D</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=n>D</span> <span class=o>*</span> <span class=n>D</span><span class=p>)</span> <span class=o>+</span> <span class=n>D</span>
</span></span></code></pre></div><hr><h3 id=32-model-training>3.2. Model training<a hidden class=anchor aria-hidden=true href=#32-model-training>#</a></h3><h4 id=321-randomness>3.2.1. Randomness<a hidden class=anchor aria-hidden=true href=#321-randomness>#</a></h4><ul><li>Randomness appears in many places: parameter init, dropout, data shuffling, etc.</li><li>For reproducibility, set seeds explicitly whenever you rely on randomness.</li><li>Determinism is especially useful for debugging, because it makes issues easier to reproduce.</li><li>In practice you should set seeds in three places (PyTorch, NumPy, Python&rsquo;s <code>random</code>).</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Torch</span>
</span></span><span class=line><span class=cl><span class=n>seed</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># NumPy</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Python</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span></span></code></pre></div><hr><h4 id=322-data-loading>3.2.2. Data loading<a hidden class=anchor aria-hidden=true href=#322-data-loading>#</a></h4><p>In language models, data can be represented as sequences of integers (tokens).</p><p>You can serialize sequences with a NumPy array:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>orig_data</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>,</span> <span class=mi>7</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>9</span><span class=p>,</span> <span class=mi>10</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>orig_data</span><span class=o>.</span><span class=n>tofile</span><span class=p>(</span><span class=s2>&#34;data.npy&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>You can load the data with NumPy.</p><p>If you don&rsquo;t want to load the entire dataset into memory (some datasets are huge; e.g., LLaMA can be ~2.8TB), you can use <code>memmap</code> to map only the accessed parts.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>memmap</span><span class=p>(</span><span class=s2>&#34;data.npy&#34;</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>np</span><span class=o>.</span><span class=n>array_equal</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>orig_data</span><span class=p>)</span>
</span></span></code></pre></div><p>A dataloader generates a batch of training data:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_batch</span><span class=p>(</span><span class=n>data</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>sequence_length</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>device</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=c1># Sample batch_size random positions into data.</span>
</span></span><span class=line><span class=cl>  <span class=n>start_indices</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>data</span><span class=p>)</span> <span class=o>-</span> <span class=n>sequence_length</span><span class=p>,</span> <span class=p>(</span><span class=n>batch_size</span><span class=p>,))</span>
</span></span><span class=line><span class=cl>  <span class=k>assert</span> <span class=n>start_indices</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=n>batch_size</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Index into the data.</span>
</span></span><span class=line><span class=cl>  <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>data</span><span class=p>[</span><span class=n>start</span><span class=p>:</span><span class=n>start</span> <span class=o>+</span> <span class=n>sequence_length</span><span class=p>]</span> <span class=k>for</span> <span class=n>start</span> <span class=ow>in</span> <span class=n>start_indices</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>sequence_length</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Pinned memory</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>pin_memory</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>,</span> <span class=n>non_blocking</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>B</span> <span class=o>=</span> <span class=mi>2</span>  <span class=c1># Batch size</span>
</span></span><span class=line><span class=cl><span class=n>L</span> <span class=o>=</span> <span class=mi>4</span>  <span class=c1># Length of sequence</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>get_batch</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>B</span><span class=p>,</span> <span class=n>sequence_length</span><span class=o>=</span><span class=n>L</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>get_device</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=n>B</span><span class=p>,</span> <span class=n>L</span><span class=p>])</span>
</span></span></code></pre></div><p>By default, CPU tensors live in paged memory. You can explicitly pin them via <code>x = x.pin_memory()</code>.</p><p>This allows two tasks to overlap:</p><ul><li>Fetch the next batch on the CPU</li><li>Process the current <code>x</code> on the GPU</li></ul><hr><h4 id=323-optimizer>3.2.3. Optimizer<a hidden class=anchor aria-hidden=true href=#323-optimizer>#</a></h4><p>We&rsquo;ll use the familiar linear example again.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>B</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>D</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=n>num_layers</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Cruncher</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=n>D</span><span class=p>,</span> <span class=n>num_layers</span><span class=o>=</span><span class=n>num_layers</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>get_device</span><span class=p>())</span>
</span></span></code></pre></div><p><strong>Define</strong> an AdaGrad optimizer</p><ul><li>momentum = SGD + exponential averaging of grad</li><li>AdaGrad = SGD + averaging by grad^2</li><li>RMSProp = AdaGrad + exponentially averaging of grad^2</li><li>Adam = RMSProp + momentum</li></ul><p><a href=https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf>AdaGrad</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>AdaGrad</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Optimizer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>params</span><span class=p>:</span> <span class=n>Iterable</span><span class=p>[</span><span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>],</span> <span class=n>lr</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.01</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>AdaGrad</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=nb>dict</span><span class=p>(</span><span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>group</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>param_groups</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>lr</span> <span class=o>=</span> <span class=n>group</span><span class=p>[</span><span class=s2>&#34;lr&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>group</span><span class=p>[</span><span class=s2>&#34;params&#34;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>                <span class=c1># Optimizer state</span>
</span></span><span class=line><span class=cl>                <span class=n>state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>state</span><span class=p>[</span><span class=n>p</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>grad</span> <span class=o>=</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># Get squared gradients g2 = sum_{i&lt;t} g_i^2</span>
</span></span><span class=line><span class=cl>                <span class=n>g2</span> <span class=o>=</span> <span class=n>state</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;g2&#34;</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>grad</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># Update optimizer state</span>
</span></span><span class=line><span class=cl>                <span class=n>g2</span> <span class=o>+=</span> <span class=n>torch</span><span class=o>.</span><span class=n>square</span><span class=p>(</span><span class=n>grad</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>state</span><span class=p>[</span><span class=s2>&#34;g2&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>g2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># Update parameters</span>
</span></span><span class=line><span class=cl>                <span class=n>p</span><span class=o>.</span><span class=n>data</span> <span class=o>-=</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>grad</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>g2</span> <span class=o>+</span> <span class=mf>1e-5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>AdaGrad</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>state</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span>  <span class=c1># @inspect state</span>
</span></span></code></pre></div><p>Compute gradients</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>D</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>get_device</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>4.</span><span class=p>,</span> <span class=mf>5.</span><span class=p>],</span> <span class=n>device</span><span class=o>=</span><span class=n>get_device</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>pred_y</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>mse_loss</span><span class=p>(</span><span class=nb>input</span><span class=o>=</span><span class=n>pred_y</span><span class=p>,</span> <span class=n>target</span><span class=o>=</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span></code></pre></div><p>Run one optimizer step</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>state</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span>  <span class=c1># @inspect state</span>
</span></span></code></pre></div><p>Free memory (optional)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>(</span><span class=n>set_to_none</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><h5 id=memory-accounting>Memory accounting<a hidden class=anchor aria-hidden=true href=#memory-accounting>#</a></h5><ul><li><p>Memory for parameters</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_num_parameters</span><span class=p>(</span><span class=n>model</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=nb>sum</span><span class=p>(</span><span class=n>param</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>num_parameters</span> <span class=o>=</span> <span class=p>(</span><span class=n>D</span> <span class=o>*</span> <span class=n>D</span> <span class=o>*</span> <span class=n>num_layers</span><span class=p>)</span> <span class=o>+</span> <span class=n>D</span>  <span class=c1># @inspect num_parameters</span>
</span></span><span class=line><span class=cl><span class=k>assert</span> <span class=n>num_parameters</span> <span class=o>==</span> <span class=n>get_num_parameters</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span></code></pre></div><p>Output</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_parameters</span> <span class=o>=</span> <span class=mi>36</span>
</span></span></code></pre></div></li><li><p>Memory for activations</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_activations</span> <span class=o>=</span> <span class=n>B</span> <span class=o>*</span> <span class=n>D</span> <span class=o>*</span> <span class=n>num_layers</span>  <span class=c1># @inspect num_activations</span>
</span></span></code></pre></div><p>Output</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_parameters</span> <span class=o>=</span> <span class=mi>16</span>
</span></span></code></pre></div></li><li><p>Memory for gradients</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_gradients</span> <span class=o>=</span> <span class=n>num_parameters</span>  <span class=c1># @inspect num_gradients</span>
</span></span></code></pre></div><p>Output</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_parameters</span> <span class=o>=</span> <span class=mi>36</span>
</span></span></code></pre></div></li><li><p>Memory for optimizer state</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_optimizer_states</span> <span class=o>=</span> <span class=n>num_parameters</span>  <span class=c1># @inspect num_optimizer_states</span>
</span></span></code></pre></div><p>Output</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_parameters</span> <span class=o>=</span> <span class=mi>36</span>
</span></span></code></pre></div></li><li><p>Total memory (assuming float32 storage, 4 bytes)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>total_memory</span> <span class=o>=</span> <span class=mi>4</span> <span class=o>*</span> <span class=p>(</span><span class=n>num_parameters</span> <span class=o>+</span> <span class=n>num_activations</span> <span class=o>+</span> <span class=n>num_gradients</span> <span class=o>+</span> <span class=n>num_optimizer_states</span><span class=p>)</span>  <span class=c1># @inspect total_memory</span>
</span></span></code></pre></div><p>Output</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_parameters</span> <span class=o>=</span> <span class=mi>496</span>
</span></span></code></pre></div></li></ul><hr><h4 id=324-training-loop>3.2.4. Training loop<a hidden class=anchor aria-hidden=true href=#324-training-loop>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span><span class=n>name</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>get_batch</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=n>D</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=n>B</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>num_train_steps</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>lr</span><span class=p>:</span> <span class=nb>float</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>Cruncher</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=n>D</span><span class=p>,</span> <span class=n>num_layers</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>get_device</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_train_steps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Get data</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>get_batch</span><span class=p>(</span><span class=n>B</span><span class=o>=</span><span class=n>B</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Forward (compute loss)</span>
</span></span><span class=line><span class=cl>        <span class=n>pred_y</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>mse_loss</span><span class=p>(</span><span class=n>pred_y</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Backward (compute gradients)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># Update parameters</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>(</span><span class=n>set_to_none</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>train</span><span class=p>(</span><span class=s2>&#34;simple&#34;</span><span class=p>,</span> <span class=n>get_batch</span><span class=p>,</span> <span class=n>D</span><span class=o>=</span><span class=n>D</span><span class=p>,</span> <span class=n>num_layers</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>B</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>num_train_steps</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</span></span></code></pre></div><hr><h4 id=325-checkpointing>3.2.5. Checkpointing<a hidden class=anchor aria-hidden=true href=#325-checkpointing>#</a></h4><p>During training, periodically saving the model and optimizer state to disk is very useful.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>checkpoint</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;model&#34;</span><span class=p>:</span> <span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;optimizer&#34;</span><span class=p>:</span> <span class=n>optimizer</span><span class=o>.</span><span class=n>state_dict</span><span class=p>(),</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>checkpoint</span><span class=p>,</span> <span class=s2>&#34;model_checkpoint.pt&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>Load a checkpoint</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>loaded_checkpoint</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>&#34;model_checkpoint.pt&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=326-mixed-precision-training>3.2.6. Mixed-precision training<a hidden class=anchor aria-hidden=true href=#326-mixed-precision-training>#</a></h4><ul><li>Choosing the dtype (float32, bfloat16, fp8) involves trade-offs:<ul><li>Higher precision: more accurate/stable, more memory, more compute</li><li>Lower precision: less accurate/stable, less memory, less compute</li></ul></li><li>How do we get the best of both?<ul><li>Typical approach: default to float32, but use {bfloat16, fp8} where possible</li><li>A common plan:<ol><li>Use {bfloat16, fp8} in the forward pass (activations)</li><li>Keep the rest in float32 (parameters, gradients)</li></ol></li><li><a href=https://arxiv.org/pdf/1710.03740.pdf>Mixed-precision training</a></li><li><a href=https://pytorch.org/docs/stable/amp.html>PyTorch automatic mixed precision (AMP)</a></li><li>NVIDIA&rsquo;s <a href=https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/>Transformer Engine</a> supports FP8 for linear layers and is widely used in training; see <a href=https://arxiv.org/pdf/2310.18313>FP8</a></li></ul></li></ul><hr><div class=zhihu-ref><div class=zhihu-ref-title>References</div><ol><li><a href="https://stanford-cs336.github.io/spring2025-lectures/?trace=var%2Ftraces%2Flecture_02.json" target=_blank>stanford-cs336 lecture 2</a></li></ol></div></div><br><div><div class=copyright-card><div class=copyright-card-main><div class=copyright-info><h3 class=copyright-title>Training Primitives and Resource Accounting</h3><p class=copyright-link><a href=https://cspaulia.github.io/cspaulia-blog/en/posts/primitives/>https://cspaulia.github.io/cspaulia-blog/en/posts/primitives/</a></p><div class=copyright-meta><span><strong>Author</strong><br>CSPaulia</span>
<span><strong>Published at</strong><br>July 28, 2025</span>
<span><strong>Copyright</strong><br><a rel=license href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank>CC BY-NC-SA 4.0</a></span></div></div><div class=copyright-icon>©</div></div></div></div><footer class=post-footer><p style=font-size:medium;margin-bottom:5px;font-weight:700>categories</p><ul class=post-tags><li><a href=https://cspaulia.github.io/cspaulia-blog/en/categories/large-language-model/>Large Language Model</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/categories/nlp/>NLP</a></li></ul><p style=font-size:medium;margin-bottom:5px;font-weight:700>tags</p><ul class=post-tags><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/pytorch/>PyTorch</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/flops/>FLOPs</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/memory/>Memory</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/training/>Training</a></li></ul><nav class=paginav><a class=prev href=https://cspaulia.github.io/cspaulia-blog/en/posts/sft_rlhf/><span class=title>« Prev</span><br><span>SFT and RLHF</span>
</a><a class=next href=https://cspaulia.github.io/cspaulia-blog/en/posts/tokenization/><span class=title>Next »</span><br><span>Tokenization</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Training Primitives and Resource Accounting on x" href="https://x.com/intent/tweet/?text=Training%20Primitives%20and%20Resource%20Accounting&amp;url=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fen%2fposts%2fprimitives%2f&amp;hashtags=PyTorch%2cFLOPs%2cMemory%2cTraining"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Training Primitives and Resource Accounting on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fen%2fposts%2fprimitives%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Training Primitives and Resource Accounting on telegram" href="https://telegram.me/share/url?text=Training%20Primitives%20and%20Resource%20Accounting&amp;url=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fen%2fposts%2fprimitives%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://cspaulia.github.io/cspaulia-blog/en/>cspaulia-blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>