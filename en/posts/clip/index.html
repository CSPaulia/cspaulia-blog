<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>CLIP and Its Follow-up Works | cspaulia-blog</title><meta name=keywords content="CLIP"><meta name=description content="CLIP and its follow-up works"><meta name=author content="CSPaulia"><link rel=canonical href=https://cspaulia.github.io/cspaulia-blog/en/posts/clip/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/cspaulia-blog/assets/css/stylesheet.4a40da687e9ad320449d5d267445e114ee7b6620a3d534bde2b12baa408f07f5.css integrity="sha256-SkDaaH6a0yBEnV0mdEXhFO57ZiCj1TS94rErqkCPB/U=" rel="preload stylesheet" as=style><link rel=icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://cspaulia.github.io/cspaulia-blog/posts/clip/><link rel=alternate hreflang=en href=https://cspaulia.github.io/cspaulia-blog/en/posts/clip/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1,trust:!0,strict:!1})})</script><meta property="og:url" content="https://cspaulia.github.io/cspaulia-blog/en/posts/clip/"><meta property="og:site_name" content="cspaulia-blog"><meta property="og:title" content="CLIP and Its Follow-up Works"><meta property="og:description" content="CLIP and its follow-up works"><meta property="og:locale" content="en-US"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-03T10:46:03+08:00"><meta property="article:modified_time" content="2026-01-30T13:44:37+08:00"><meta property="article:tag" content="CLIP"><meta property="og:image" content="https://cspaulia.github.io/cspaulia-blog/clip_cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cspaulia.github.io/cspaulia-blog/clip_cover.png"><meta name=twitter:title content="CLIP and Its Follow-up Works"><meta name=twitter:description content="CLIP and its follow-up works"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://cspaulia.github.io/cspaulia-blog/en/posts/"},{"@type":"ListItem","position":2,"name":"CLIP and Its Follow-up Works","item":"https://cspaulia.github.io/cspaulia-blog/en/posts/clip/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CLIP and Its Follow-up Works","name":"CLIP and Its Follow-up Works","description":"CLIP and its follow-up works","keywords":["CLIP"],"articleBody":"I. CLIP Contrastive pre-training: Collect $N$ image–text pairs from the internet (OpenAI used 400M image–text pairs) as positive pairs.\nFor each image, pair it with the other $N-1$ texts to form negative pairs (text does not match the image).\nFeed the $N$ images into the image encoder and the corresponding texts into the text encoder. Compute dot products between image and text features to obtain a similarity matrix.\nTreat each row as an $N$-way classification problem. For row $i$, the correct label is $i$ (make the $(i,i)$ entry the largest in that row). Apply cross-entropy between row $i$ and label $i$.\nSimilarly, treat each column as an $N$-way classification problem and apply cross-entropy so that the $(i,i)$ entry is the largest in column $i$.\nBuild a classifier from labels: Turn text labels into prompts/sentences and feed them into the text encoder to get text features. Zero-shot prediction: Match the label text features with image features by similarity to make predictions. II. Using CLIP for Semantic Segmentation 2.1 LSeg Relationship to CLIP: Use CLIP’s aligned feature space: map semantic labels and pixel features into the same space and predict segmentation by similarity.\nText encoder: the same as CLIP, and frozen during training ❄️.\nDifferences from CLIP: Method Training Text encoder params Similarity computation CLIP Contrastive Trainable Similarity between image–text pairs LSeg Supervised Frozen Similarity between image features and text features CLIP input: multiple image–text pairs LSeg input: one image + labels (can be viewed as descriptive text for the image) 2.2 GroupViT Relationship to CLIP and LSeg: Method Training Similarity computation CLIP Contrastive Similarity between image–text pairs LSeg Supervised Similarity between image features and text features GroupViT Contrastive Similarity between image–text pairs LSeg is trained with supervision on top of CLIP’s aligned feature space. GroupViT adjusts the vision encoder architecture in CLIP to better fit segmentation, while keeping CLIP-style contrastive learning. Architecture details: Model input: image patches + learnable group tokens Segmentation: assign patch features to learnable group tokens via Group Block learnable group tokens: similar to cluster centroids III. Using CLIP for Object Detection 3.1 ViLD Vanilla Detector = Head + Classifier + supervised cross-entropy ViLD-text = Head + similarity matching + supervised cross-entropy Similarity-matching pipeline (CLIP-style): Feed $n$ labels (via prompt engineering) into the text encoder to get $n$ text embeddings. To cover regions that do not match any label, introduce a background embedding. Match region embeddings with text embeddings and the background embedding to obtain $n$ class scores plus one background score. This replaces the Classifier and is frozen during training ❄️. ViLD-image = teacher network + student network + L1 distillation teacher network: CLIP image encoder student network: Vanilla Detector To reduce training cost, pre-extract $m$ region embeddings using a pretrained detector. ViLD = ViLD-text + ViLD-image IV. Using CLIP for Visual Grounding 4.1 GLIP Essentially supervised training. Compute similarity between Regions and Words to classify/caption Regions. Training requires knowing the alignment between Regions and Words in captions. To obtain it: Detection datasets: build captions from bounding-box annotations (e.g., Banana → “There is a banana.”) Caption datasets: use a GLIP model trained on detection data to find Regions–Words alignments and construct pseudo labels. V. Using CLIP for Image Generation 5.1 CLIPasso Motivation Observation: prior sketch generation methods often work only for a specific category.\nGoal: leverage CLIP’s strong generalization to generate sketches for arbitrary categories.\nPipeline Generate a sketch: Use the image encoder to obtain a heatmap. Sample points based on the heatmap. Aggregate points with learnable parameters to form Bézier curves, producing a sketch. Constrain generation with CLIP: Feed the generated sketch and the original image into two different CLIP image encoders. $L_g$ constrains geometric consistency (closer is better). $L_s$ constrains semantic consistency (closer is better). VI. Using CLIP for Video Retrieval 6.1 CLIP4Clip Motivation CLIP is designed for image–text pairs. For video retrieval, the task is matching one text query against multiple frames and finding the most relevant frames. CLIP4Clip explores three matching strategies: Parameter-free type: no extra parameters (e.g., mean pooling; ignores temporal order) Sequential type: temporal modules such as LSTM and Transformer Tight type: a Transformer Encoder jointly learns text and video features and outputs similarity 6.2 ActionCLIP Similar to CLIP4Clip.\nVII. Using CLIP for Speech Recognition 7.1 AudioCLIP Add an audio encoder and follow CLIP-style objectives: audio–image contrastive learning and audio–text contrastive learning.\nVIII. Using CLIP for 3D Understanding 8.1 PointCLIP Project the point cloud into a 2D space. Build prompts: Point Cloud Depth Map of a [CLASS] IX. Using CLIP for Depth Estimation 9.1 DepthCLIP Build 7 text prompts: “This object is [distance class]”, where [distance class] is: ‘giant’ ’extremely close' ‘close’ ’not in distance' ‘a little remote’ ‘far’ ‘unseen’ The network is similar to LSeg and performs classification with Softmax. References CLIP 改进工作串讲（上）【论文精读·42】 CLIP 改进工作串讲（下）【论文精读·42】 ","wordCount":"956","inLanguage":"en","image":"https://cspaulia.github.io/cspaulia-blog/clip_cover.png","datePublished":"2025-06-03T10:46:03+08:00","dateModified":"2026-01-30T13:44:37+08:00","author":{"@type":"Person","name":"CSPaulia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://cspaulia.github.io/cspaulia-blog/en/posts/clip/"},"publisher":{"@type":"Organization","name":"cspaulia-blog","logo":{"@type":"ImageObject","url":"https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://cspaulia.github.io/cspaulia-blog/en/ accesskey=h title="Home (Alt + H)"><img src=https://cspaulia.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://cspaulia.github.io/cspaulia-blog/ title=中文 aria-label=中文>中文</a></li></ul></div></div><ul id=menu><li><a href=https://cspaulia.github.io/cspaulia-blog/en/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/series/ title=Series><span>Series</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://cspaulia.github.io/cspaulia-blog/en/>Home</a>&nbsp;»&nbsp;<a href=https://cspaulia.github.io/cspaulia-blog/en/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">CLIP and Its Follow-up Works</h1><div class=post-description>CLIP and its follow-up works</div><div class=post-meta><span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg> June 3, 2025</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 19l7-7 3 3-7 7-3-3z"/><path d="M18 13l-1.5-7.5L2 2l3.5 14.5L13 18l5-5z"/><path d="M2 2l7.586 7.586"/><circle cx="11" cy="11" r="2"/></svg> January 30, 2026</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><path d="M14 2v6h6"/><line x1="8" y1="13" x2="16" y2="13"/><line x1="8" y1="17" x2="16" y2="17"/><line x1="8" y1="9" x2="12" y2="9"/></svg> 956 words</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg> 2 min</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg> CSPaulia</span>
&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://cspaulia.github.io/cspaulia-blog/posts/clip/>中文</a></li></ul><span id=busuanzi_container_page_pv>｜ <span id=busuanzi_value_page_pv></span> views</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#i-clip aria-label="I. CLIP">I. CLIP</a></li><li><a href=#ii-using-clip-for-semantic-segmentation aria-label="II. Using CLIP for Semantic Segmentation">II. Using CLIP for Semantic Segmentation</a><ul><li><a href=#21-lseg aria-label="2.1 LSeg">2.1 LSeg</a></li><li><a href=#22-groupvit aria-label="2.2 GroupViT">2.2 GroupViT</a></li></ul></li><li><a href=#iii-using-clip-for-object-detection aria-label="III. Using CLIP for Object Detection">III. Using CLIP for Object Detection</a><ul><li><a href=#31-vild aria-label="3.1 ViLD">3.1 ViLD</a></li></ul></li><li><a href=#iv-using-clip-for-visual-grounding aria-label="IV. Using CLIP for Visual Grounding">IV. Using CLIP for Visual Grounding</a><ul><li><a href=#41-glip aria-label="4.1 GLIP">4.1 GLIP</a></li></ul></li><li><a href=#v-using-clip-for-image-generation aria-label="V. Using CLIP for Image Generation">V. Using CLIP for Image Generation</a><ul><li><a href=#51-clipasso aria-label="5.1 CLIPasso">5.1 CLIPasso</a></li></ul></li><li><a href=#vi-using-clip-for-video-retrieval aria-label="VI. Using CLIP for Video Retrieval">VI. Using CLIP for Video Retrieval</a><ul><li><a href=#61-clip4clip aria-label="6.1 CLIP4Clip">6.1 CLIP4Clip</a></li><li><a href=#62-actionclip aria-label="6.2 ActionCLIP">6.2 ActionCLIP</a></li></ul></li><li><a href=#vii-using-clip-for-speech-recognition aria-label="VII. Using CLIP for Speech Recognition">VII. Using CLIP for Speech Recognition</a><ul><li><a href=#71-audioclip aria-label="7.1 AudioCLIP">7.1 AudioCLIP</a></li></ul></li><li><a href=#viii-using-clip-for-3d-understanding aria-label="VIII. Using CLIP for 3D Understanding">VIII. Using CLIP for 3D Understanding</a><ul><li><a href=#81-pointclip aria-label="8.1 PointCLIP">8.1 PointCLIP</a></li></ul></li><li><a href=#ix-using-clip-for-depth-estimation aria-label="IX. Using CLIP for Depth Estimation">IX. Using CLIP for Depth Estimation</a><ul><li><a href=#91-depthclip aria-label="9.1 DepthCLIP">9.1 DepthCLIP</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h2[id],h3[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=i-clip>I. CLIP<a hidden class=anchor aria-hidden=true href=#i-clip>#</a></h2><p align=center><img src=/cspaulia-blog/posts/clip/clip.png alt=clip loading=lazy></p><ol><li>Contrastive pre-training:</li></ol><ul><li><p>Collect $N$ image–text pairs from the internet (OpenAI used 400M image–text pairs) as positive pairs.</p></li><li><p>For each image, pair it with the other $N-1$ texts to form negative pairs (text does not match the image).</p></li><li><p>Feed the $N$ images into the image encoder and the corresponding texts into the text encoder. Compute dot products between image and text features to obtain a similarity matrix.</p></li><li><p>Treat each row as an $N$-way classification problem. For row $i$, the correct label is $i$ (make the $(i,i)$ entry the largest in that row). Apply cross-entropy between row $i$ and label $i$.</p></li><li><p>Similarly, treat each column as an $N$-way classification problem and apply cross-entropy so that the $(i,i)$ entry is the largest in column $i$.</p></li></ul><ol start=2><li>Build a classifier from labels:</li></ol><ul><li>Turn text labels into prompts/sentences and feed them into the text encoder to get text features.</li></ul><ol start=3><li>Zero-shot prediction:</li></ol><ul><li>Match the label text features with image features by similarity to make predictions.</li></ul><h2 id=ii-using-clip-for-semantic-segmentation>II. Using CLIP for Semantic Segmentation<a hidden class=anchor aria-hidden=true href=#ii-using-clip-for-semantic-segmentation>#</a></h2><h3 id=21-lseg>2.1 LSeg<a hidden class=anchor aria-hidden=true href=#21-lseg>#</a></h3><p align=center><img src=/cspaulia-blog/posts/clip/lseg.png alt=lseg loading=lazy></p><ol><li>Relationship to CLIP:</li></ol><ul><li><p>Use CLIP’s aligned feature space: map semantic labels and pixel features into the same space and predict segmentation by similarity.</p></li><li><p>Text encoder: the same as CLIP, and frozen during training &#x2744;&#xfe0f;.</p></li></ul><ol start=2><li>Differences from CLIP:</li></ol><table><thead><tr><th>Method</th><th>Training</th><th>Text encoder params</th><th>Similarity computation</th></tr></thead><tbody><tr><td>CLIP</td><td>Contrastive</td><td>Trainable</td><td>Similarity between image–text pairs</td></tr><tr><td>LSeg</td><td>Supervised</td><td>Frozen</td><td>Similarity between image features and text features</td></tr></tbody></table><ul><li>CLIP input: multiple image–text pairs</li><li>LSeg input: one image + labels (can be viewed as descriptive text for the image)</li></ul><h3 id=22-groupvit>2.2 GroupViT<a hidden class=anchor aria-hidden=true href=#22-groupvit>#</a></h3><p align=center><img src=/cspaulia-blog/posts/clip/groupvit_overview.png alt=groupvit_overview width=50% loading=lazy></p><ol><li>Relationship to CLIP and LSeg:</li></ol><table><thead><tr><th>Method</th><th>Training</th><th>Similarity computation</th><th></th></tr></thead><tbody><tr><td>CLIP</td><td>Contrastive</td><td>Similarity between image–text pairs</td><td></td></tr><tr><td>LSeg</td><td>Supervised</td><td>Similarity between image features and text features</td><td></td></tr><tr><td>GroupViT</td><td>Contrastive</td><td>Similarity between image–text pairs</td><td></td></tr></tbody></table><ul><li>LSeg is trained with supervision on top of CLIP’s aligned feature space.</li><li>GroupViT adjusts the vision encoder architecture in CLIP to better fit segmentation, while keeping CLIP-style contrastive learning.</li></ul><ol start=2><li>Architecture details:</li></ol><p align=center><img src=/cspaulia-blog/posts/clip/groupvit.png alt=groupvit loading=lazy></p><ul><li>Model input: <code>image patches</code> + <code>learnable group tokens</code></li><li>Segmentation: assign <code>patch features</code> to <code>learnable group tokens</code> via <code>Group Block</code></li><li><code>learnable group tokens</code>: similar to cluster centroids</li></ul><h2 id=iii-using-clip-for-object-detection>III. Using CLIP for Object Detection<a hidden class=anchor aria-hidden=true href=#iii-using-clip-for-object-detection>#</a></h2><h3 id=31-vild>3.1 ViLD<a hidden class=anchor aria-hidden=true href=#31-vild>#</a></h3><p align=center><img src=/cspaulia-blog/posts/clip/vild_compare.png alt=vild_compare loading=lazy></p><ul><li><code>Vanilla Detector</code> = <code>Head</code> + <code>Classifier</code> + supervised cross-entropy</li><li><code>ViLD-text</code> = <code>Head</code> + <code>similarity matching</code> + supervised cross-entropy<ul><li>Similarity-matching pipeline (CLIP-style):<ul><li>Feed $n$ labels (via prompt engineering) into the <code>text encoder</code> to get $n$ <code>text embeddings</code>.</li><li>To cover regions that do not match any label, introduce a <code>background embedding</code>.</li><li>Match <code>region embeddings</code> with <code>text embeddings</code> and the <code>background embedding</code> to obtain $n$ class scores plus one background score. This replaces the <code>Classifier</code> and is frozen during training &#x2744;&#xfe0f;.</li></ul></li></ul></li><li><code>ViLD-image</code> = <code>teacher network</code> + <code>student network</code> + L1 distillation<ul><li><code>teacher network</code>: CLIP image encoder</li><li><code>student network</code>: <code>Vanilla Detector</code></li><li>To reduce training cost, pre-extract $m$ <code>region embeddings</code> using a pretrained detector.</li></ul></li><li><code>ViLD</code> = <code>ViLD-text</code> + <code>ViLD-image</code></li></ul><h2 id=iv-using-clip-for-visual-grounding>IV. Using CLIP for Visual Grounding<a hidden class=anchor aria-hidden=true href=#iv-using-clip-for-visual-grounding>#</a></h2><h3 id=41-glip>4.1 GLIP<a hidden class=anchor aria-hidden=true href=#41-glip>#</a></h3><p align=center><img src=/cspaulia-blog/posts/clip/glip.png alt=glip loading=lazy></p><ul><li>Essentially supervised training.</li><li>Compute similarity between <code>Regions</code> and <code>Words</code> to classify/caption <code>Regions</code>.</li><li>Training requires knowing the alignment between <code>Regions</code> and <code>Words</code> in captions. To obtain it:<ul><li>Detection datasets: build captions from bounding-box annotations (e.g., Banana → “There is a banana.”)</li><li>Caption datasets: use a GLIP model trained on detection data to find <code>Regions</code>–<code>Words</code> alignments and construct pseudo labels.</li></ul></li></ul><h2 id=v-using-clip-for-image-generation>V. Using CLIP for Image Generation<a hidden class=anchor aria-hidden=true href=#v-using-clip-for-image-generation>#</a></h2><h3 id=51-clipasso>5.1 CLIPasso<a hidden class=anchor aria-hidden=true href=#51-clipasso>#</a></h3><p align=center><img src=/cspaulia-blog/posts/clip/clipasso.png alt=clipasso loading=lazy></p><h4 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h4><p>Observation: prior sketch generation methods often work only for a specific category.</p><p>Goal: leverage CLIP’s strong generalization to generate sketches for arbitrary categories.</p><h4 id=pipeline>Pipeline<a hidden class=anchor aria-hidden=true href=#pipeline>#</a></h4><ol><li>Generate a sketch:</li></ol><ul><li>Use the <code>image encoder</code> to obtain a heatmap.</li><li>Sample points based on the heatmap.</li><li>Aggregate points with <code>learnable parameters</code> to form Bézier curves, producing a sketch.</li></ul><ol start=2><li>Constrain generation with CLIP:</li></ol><ul><li>Feed the generated sketch and the original image into two different CLIP image encoders.</li><li>$L_g$ constrains geometric consistency (closer is better).</li><li>$L_s$ constrains semantic consistency (closer is better).</li></ul><h2 id=vi-using-clip-for-video-retrieval>VI. Using CLIP for Video Retrieval<a hidden class=anchor aria-hidden=true href=#vi-using-clip-for-video-retrieval>#</a></h2><h3 id=61-clip4clip>6.1 CLIP4Clip<a hidden class=anchor aria-hidden=true href=#61-clip4clip>#</a></h3><h4 id=motivation-1>Motivation<a hidden class=anchor aria-hidden=true href=#motivation-1>#</a></h4><p align=center><img src=/cspaulia-blog/posts/clip/clip4clip.png alt=clip4clip loading=lazy></p><ul><li>CLIP is designed for image–text pairs. For video retrieval, the task is matching one text query against multiple frames and finding the most relevant frames.</li><li>CLIP4Clip explores three matching strategies:<ul><li>Parameter-free type: no extra parameters (e.g., mean pooling; ignores temporal order)</li><li>Sequential type: temporal modules such as LSTM and Transformer</li><li>Tight type: a Transformer Encoder jointly learns text and video features and outputs similarity</li></ul></li></ul><h3 id=62-actionclip>6.2 ActionCLIP<a hidden class=anchor aria-hidden=true href=#62-actionclip>#</a></h3><p align=center><img src=/cspaulia-blog/posts/clip/actionclip_overview.png alt=actionclip_overview width=50% loading=lazy></p><p align=center><img src=/cspaulia-blog/posts/clip/actionclip.png alt=actionclip loading=lazy></p><p>Similar to CLIP4Clip.</p><h2 id=vii-using-clip-for-speech-recognition>VII. Using CLIP for Speech Recognition<a hidden class=anchor aria-hidden=true href=#vii-using-clip-for-speech-recognition>#</a></h2><h3 id=71-audioclip>7.1 AudioCLIP<a hidden class=anchor aria-hidden=true href=#71-audioclip>#</a></h3><p align=center><img src=/cspaulia-blog/posts/clip/audioclip.png alt=audioclip loading=lazy></p><p>Add an audio encoder and follow CLIP-style objectives: <code>audio–image</code> contrastive learning and <code>audio–text</code> contrastive learning.</p><h2 id=viii-using-clip-for-3d-understanding>VIII. Using CLIP for 3D Understanding<a hidden class=anchor aria-hidden=true href=#viii-using-clip-for-3d-understanding>#</a></h2><h3 id=81-pointclip>8.1 PointCLIP<a hidden class=anchor aria-hidden=true href=#81-pointclip>#</a></h3><p align=center><img src=/cspaulia-blog/posts/clip/pointclip.png alt=pointclip loading=lazy></p><ol><li>Project the point cloud into a 2D space.</li><li>Build prompts: <code>Point Cloud Depth Map of a [CLASS]</code></li></ol><h2 id=ix-using-clip-for-depth-estimation>IX. Using CLIP for Depth Estimation<a hidden class=anchor aria-hidden=true href=#ix-using-clip-for-depth-estimation>#</a></h2><h3 id=91-depthclip>9.1 DepthCLIP<a hidden class=anchor aria-hidden=true href=#91-depthclip>#</a></h3><p align=center><img src=/cspaulia-blog/posts/clip/depthclip.png alt=depthclip loading=lazy></p><ul><li>Build 7 text prompts: &ldquo;This object is [distance class]&rdquo;, where <code>[distance class]</code> is:<ul><li>&lsquo;giant&rsquo;</li><li>&rsquo;extremely close'</li><li>&lsquo;close&rsquo;</li><li>&rsquo;not in distance'</li><li>&lsquo;a little remote&rsquo;</li><li>&lsquo;far&rsquo;</li><li>&lsquo;unseen&rsquo;</li></ul></li><li>The network is similar to LSeg and performs classification with <code>Softmax</code>.</li></ul><hr><div class=zhihu-ref><div class=zhihu-ref-title>References</div><ol><li><a href="https://www.bilibili.com/video/BV1FV4y1p7Lm?spm_id_from=333.788.videopod.sections&vd_source=9e4f1724ef60547fa31e3c8270245ff8" target=_blank>CLIP 改进工作串讲（上）【论文精读·42】</a></li><li><a href="https://www.bilibili.com/video/BV1gg411U7n4?spm_id_from=333.788.videopod.sections&vd_source=9e4f1724ef60547fa31e3c8270245ff8" target=_blank>CLIP 改进工作串讲（下）【论文精读·42】</a></li></ol></div></div><br><div><div class=copyright-card><div class=copyright-card-main><div class=copyright-info><h3 class=copyright-title>CLIP and Its Follow-up Works</h3><p class=copyright-link><a href=https://cspaulia.github.io/cspaulia-blog/en/posts/clip/>https://cspaulia.github.io/cspaulia-blog/en/posts/clip/</a></p><div class=copyright-meta><span><strong>Author</strong><br>CSPaulia</span>
<span><strong>Published at</strong><br>June 3, 2025</span>
<span><strong>Copyright</strong><br><a rel=license href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank>CC BY-NC-SA 4.0</a></span></div></div><div class=copyright-icon>©</div></div></div></div><footer class=post-footer><p style=font-size:medium;margin-bottom:5px;font-weight:700>categories</p><ul class=post-tags><li><a href=https://cspaulia.github.io/cspaulia-blog/en/categories/base-model/>Base Model</a></li></ul><p style=font-size:medium;margin-bottom:5px;font-weight:700>tags</p><ul class=post-tags><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/clip/>CLIP</a></li></ul><nav class=paginav><a class=prev href=https://cspaulia.github.io/cspaulia-blog/en/posts/hugo-papermod/><span class=title>« Prev</span><br><span>The Evolution of PaperMod</span>
</a><a class=next href=https://cspaulia.github.io/cspaulia-blog/en/posts/screen/><span class=title>Next »</span><br><span>Using `screen` to Manage Terminal Sessions</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share CLIP and Its Follow-up Works on x" href="https://x.com/intent/tweet/?text=CLIP%20and%20Its%20Follow-up%20Works&amp;url=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fen%2fposts%2fclip%2f&amp;hashtags=CLIP"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share CLIP and Its Follow-up Works on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fen%2fposts%2fclip%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share CLIP and Its Follow-up Works on telegram" href="https://telegram.me/share/url?text=CLIP%20and%20Its%20Follow-up%20Works&amp;url=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fen%2fposts%2fclip%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://cspaulia.github.io/cspaulia-blog/en/>cspaulia-blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>