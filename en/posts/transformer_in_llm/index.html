<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LM Architecture and Training | cspaulia-blog</title><meta name=keywords content="LLM,Architecture,Training"><meta name=description content="Notes on LLM architecture and training hyperparameters"><meta name=author content="CSPaulia"><link rel=canonical href=https://cspaulia.github.io/cspaulia-blog/en/posts/transformer_in_llm/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/cspaulia-blog/assets/css/stylesheet.4a40da687e9ad320449d5d267445e114ee7b6620a3d534bde2b12baa408f07f5.css integrity="sha256-SkDaaH6a0yBEnV0mdEXhFO57ZiCj1TS94rErqkCPB/U=" rel="preload stylesheet" as=style><link rel=icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://cspaulia.github.io/cspaulia-blog/posts/transformer_in_llm/><link rel=alternate hreflang=en href=https://cspaulia.github.io/cspaulia-blog/en/posts/transformer_in_llm/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1,trust:!0,strict:!1})})</script><meta property="og:url" content="https://cspaulia.github.io/cspaulia-blog/en/posts/transformer_in_llm/"><meta property="og:site_name" content="cspaulia-blog"><meta property="og:title" content="LM Architecture and Training"><meta property="og:description" content="Notes on LLM architecture and training hyperparameters"><meta property="og:locale" content="en-US"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-10T10:00:03+08:00"><meta property="article:modified_time" content="2026-01-30T16:43:57+08:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Architecture"><meta property="article:tag" content="Training"><meta property="og:image" content="https://cspaulia.github.io/cspaulia-blog/en/posts/transformer_in_llm/LLMs.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cspaulia.github.io/cspaulia-blog/en/posts/transformer_in_llm/LLMs.jpg"><meta name=twitter:title content="LM Architecture and Training"><meta name=twitter:description content="Notes on LLM architecture and training hyperparameters"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://cspaulia.github.io/cspaulia-blog/en/posts/"},{"@type":"ListItem","position":2,"name":"LM Architecture and Training","item":"https://cspaulia.github.io/cspaulia-blog/en/posts/transformer_in_llm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LM Architecture and Training","name":"LM Architecture and Training","description":"Notes on LLM architecture and training hyperparameters","keywords":["LLM","Architecture","Training"],"articleBody":"This is an excellent blog post summarizing architectural differences among several popular LLMs.\n1. Original Transformer vs Modern Variants The table below summarizes the major architectural and training differences between the original Transformer (Vaswani et al., 2017) and common Transformer variants used in modern LLMs:\nAspect Original Transformer (2017) Common Variants in Modern LLMs Normalization order Post-LN Pre-LN Activation ReLU SwiGLU (GELU/SiLU/Swish variants, etc.) Dropout Widely used Often reduced or removed for large models Normalization type LayerNorm RMSNorm (also LayerNorm/ScaleNorm variants) Linear layers With bias Bias-free Attention heads Multi-head attention (fixed) GQA / MQA, etc. Positional encoding Absolute (sinusoidal) RoPE, etc. Other - FlashAttention, MoE, hierarchical parallelism, etc. 1.1. Pre-norm vs Post-norm Almost all modern language models use pre-norm (except BERT), which tends to make training more stable.\nLeft: pre-norm. Right: post-norm.\nNew! Left: pre-norm. Right: “double norm” (used by e.g. Grok, Gemma 2).\nNew! OlMo 2 applies post-norm only to the non-residual branch.\n1.2. LayerNorm vs RMSNorm Original Transformer: LayerNorm (GPT-1/2/3, OPT, GPT-J, BLOOM)\n$$ y = \\frac{x - \\textbf{E}[x]}{\\sqrt{\\textbf{Var}[x] + \\epsilon}} * \\gamma + \\beta $$\nModern LMs: RMSNorm (LLaMA family, PaLM, T5)\n$$ y = \\frac{x}{\\sqrt{\\lVert x \\rVert^2_2 + \\epsilon}} * \\gamma $$\nAdvantages of RMSNorm: faster in practice without hurting accuracy\nfewer operations (no mean computation) fewer parameters (no bias term) 1.3. FFN: With Bias vs Bias-free Original Transformer: with bias\n$$ \\textbf{FFN}(x) = \\max(0,xW_1+b_1)W_2+b_2 $$\nModern LMs: bias-free\n$$ \\textbf{FFN}(x) = \\sigma(xW_1)W_2 $$\nAdvantages of bias-free FFNs: smaller memory footprint and more stable optimization.\n1.4. Activation Functions Activation Model ReLU Original transformer, T5, Gopher, Chinchilla, OPT GeLU GPT1/2/3, GPTJ, GPT-Neox, BLOOM GeGLU T5 v1.1, mT5, LaMDA, Phi3, Gemma 2, Gemma 3 SwiGLU LLaMa 1/2/3, PaLM, Mistral, OlMo, most models post 2023 For an introduction to activations, see this post.\n1.5. Positional Encoding 1.5.1. Sinusoidal Positional Encoding Key idea: use sine/cosine waves of different frequencies to encode position information across dimensions.\nFor position $pos$ in the sequence and dimension index $i$, the encoding is:\n$$ \\begin{aligned} PE_{(pos,2i)} = sin(\\frac{pos}{10000^{2i/d_{model}}}) \\ PE_{(pos,2i+1)} = cos(\\frac{pos}{10000^{2i/d_{model}}}) \\end{aligned} $$\nWhere:\n$pos$ is the position index (starting from 0) $i$ is the dimension index $d_{model}$ is the model hidden size 10000 is a heuristic constant that controls the frequency range Capturing relative positions Suppose the model attends to two tokens at positions $pos_1$ and $pos_2$. Their encodings are:\n$$ \\begin{aligned} E_1 = [sin(\\frac{pos_1}{10000^{0}}), cos(\\frac{pos_1}{10000^{0}}), sin(\\frac{pos_1}{10000^{2/d_{model}}}), cos(\\frac{pos_1}{10000^{2/d_{model}}}), \\dots ] \\\\ E_2 = [sin(\\frac{pos_2}{10000^{0}}), cos(\\frac{pos_2}{10000^{0}}), sin(\\frac{pos_2}{10000^{2/d_{model}}}), cos(\\frac{pos_2}{10000^{2/d_{model}}}), \\dots ] \\end{aligned} $$\nInside the model we may take an inner product:\n$$ E_1 \\cdot E_2 = sin(\\frac{pos_1}{10000^{0}})sin(\\frac{pos_2}{10000^{0}}) + cos(\\frac{pos_1}{10000^{0}})cos(\\frac{pos_2}{10000^{0}}) + \\dots $$\nUsing the trigonometric identity:\n$$ \\sin a \\sin b + \\cos a \\cos b = \\cos(a - b) $$\nWe obtain:\n$$ E_1 \\cdot E_2 = \\cos(\\frac{pos_1 - pos_2}{10000^{0}}) + \\cos(\\frac{pos_1 - pos_2}{10000^{2/d_{model}}}) + \\dots $$\nThis shows the dot product depends on the relative distance $pos_1 - pos_2$.\n1.5.2. Absolute Positional Encoding / Learnable Positional Embedding My take: “absolute positional encoding” is more of a concept — it encodes a token’s absolute index directly, rather than encoding relative offsets between tokens.\nA learnable positional embedding learns an embedding matrix:\n$$ P = [u_0, u_1, u_2, \\dots, u_{L-1}] \\in \\mathbb{R}^{L \\times d_{model}} $$\nWhere:\n$L$ is the maximum sequence length each row $u_i$ is a trainable embedding vector for position $i$ The final input to the model becomes:\n$$ Embed(x, i) = v_x + u_i $$\nwhere $v_x$ is the token embedding of token $x$.\n1.5.3. Relative Positional Encoding Limitations of absolute positional encoding:\npoor length generalization: if trained with a fixed length (e.g., 512), it may not extrapolate beyond that weak relative awareness: the model knows “the 5th word” and “the 10th word” but not explicitly that they are “5 positions apart” However, word order relations in natural language are often relative:\nIn the dependency between “the cat” and “the big cat”, “cat” is only a few steps away from “the”.\nSo the goal of relative positional encoding is to let the model learn how the distance $(i-j)$ between token $i$ and token $j$ affects attention.\nOne approach is to inject relative position information directly into the attention logits:\n$$ e_{ij} = \\frac{(x_i W_Q)(x_j W_K + a^K_{ij})^T}{\\sqrt{d_k}} $$\nwhere $a_{ij}^K$ is a vector representing the relative position between token $i$ and token $j$.\n1.5.4. RoPE (Rotary Position Embedding) How can we transform the position-encoded embeddings $x$ and $y$ such that their dot product depends only on the relative position? Concretely, we want:\n$$ \\langle f(x, i), f(y, j) \\rangle = g(x, y, i-j) \\tag{1} $$ where $\\langle \\cdot, \\cdot \\rangle$ denotes an inner product.\nSinusoidal encoding does not satisfy Eq. (1): With sinusoidal encoding, the embedding can be written as $Embed(x, i) = v_x + E_i$, where $v_x$ is the token embedding and $E_i$ is the positional encoding. Then $\\langle Embed(x, i), Embed(y, j) \\rangle = \\langle v_x + E_i, v_y + E_j \\rangle = \\langle v_x, v_y \\rangle + \\langle v_x, E_j \\rangle + \\langle E_i, v_y \\rangle + \\langle E_i, E_j \\rangle$, which contains terms depending on absolute positions $i$ and $j$, not only their difference $(i-j)$. Absolute (learnable) positional embeddings also do not satisfy Eq. (1). Relative positional encoding does not preserve the pure inner-product form in Eq. (1): the geometric interpretation is weakened: the dot product is no longer simply cosine similarity between vectors once the bias term is injected symmetry is broken: $e_{ij}$ and $e_{ji}$ can differ, enabling directionality the probabilistic interpretation weakens: logits before softmax are no longer determined only by vector similarity; extra bias can affect stability The core idea of RoPE is to embed position into each pair of dimensions via complex rotation (equivalently, a 2D plane rotation).\n$$ \\begin{aligned} x_p \u0026= [x_{p,0}, x_{p,1}, \\dots, x_{p,d-1}]\\\\ f_{{q,k}}(x_p,p) \u0026= \\mathbf{R}^d_{\\Theta,p},W_{{q,k}},x_p\\\\ \\mathbf{R}^d_{\\Theta,p} \u0026= \\begin{bmatrix} \\cos(p\\theta_0) \u0026 -\\sin(p\\theta_0) \u0026 \u0026 \u0026 \\\\ \\sin(p\\theta_0) \u0026 \\cos(p\\theta_0) \u0026 \u0026 \u0026 \\\\ \u0026 \u0026 \\cos(p\\theta_1) \u0026 -\\sin(p\\theta_1) \u0026 \\\\ \u0026 \u0026 \\sin(p\\theta_1) \u0026 \\cos(p\\theta_1) \u0026 \\\\ \u0026 \u0026 \u0026 \u0026 \\ddots \\\\ \u0026 \u0026 \u0026 \u0026 \u0026 \\cos(p\\theta_{d/2-1}) \u0026 -\\sin(p\\theta_{d/2-1}) \\\\ \u0026 \u0026 \u0026 \u0026 \u0026 \\sin(p\\theta_{d/2-1}) \u0026 \\cos(p\\theta_{d/2-1}) \\end{bmatrix} \\end{aligned} $$\nwhere $\\theta_k = 10000^{-2k/d}$.\nNext, we show why RoPE satisfies Eq. (1).\nFor two adjacent embedding dimensions $2k$ and $2k+1$ of token $x$, we have:\n$$ \\tilde{x}_{p}^{(k)} = \\begin{bmatrix} cos(p\\theta_k) \u0026 -sin(p\\theta_k) \\\\ sin(p\\theta_k) \u0026 cos(p\\theta_k) \\end{bmatrix} \\begin{bmatrix} x_{p,2k} \\\\ x_{p,2k+1} \\end{bmatrix} =(x_{p,2k}+ix_{p,2k+1})e^{ip\\theta_k} $$\nTherefore:\n$$ \\langle \\tilde{x}_{p}^{(k)}, \\tilde{y}_{q}^{(k)} \\rangle = \\tilde{x}_{p}^{(k)} \\cdot \\overline{\\tilde{y}}_{q}^{(k)} = (x_{p,2k}+ix_{p,2k+1})(y_{q,2k}-iy_{q,2k+1})e^{i(p-q)\\theta_k} $$\nThis depends on $(p-q)$, meeting the requirement in Eq. (1).\n2. Hyperparameters 2.1. FFN hidden size Let $d_{ff}$ be the FFN hidden size and $d_{model}$ be the model hidden size.\n$$ d_{ff} = 4d_{model} $$\nThen the parameter count of a standard FFN is:\nfirst layer: $d_{model} \\times d_{ff} = 4d_{model}^2$ second layer: $d_{ff} \\times d_{model} = 4d_{model}^2$ total: $8d_{model}^2$ For an FFN with a GLU-type activation, the parameter count is:\nGLU content projection: $d_{model} \\times d’_{ff}$ GLU gate projection: $d_{model} \\times d’_{ff}$ second layer: $d’_{ff} \\times d_{model}$ total: $3d_{model} \\times d’_{ff}$ To match the parameter count of a standard FFN, we need:\n$$ 3d_{model} \\times d’_{ff} = 8d_{model}^2 $$\n即\n$$ d’_{ff} = \\frac{8}{3}d_{model} $$\nThe table below lists $d_{ff}/d_{model}$ ratios used by some popular models:\nModel $( d_{ff} / d_{model} )$ PaLM 4.00 Mistral 7B 3.50 LLaMA-2 70B 3.50 LLaMA 70B 2.68 Qwen 14B 2.67 DeepSeek 67B 2.68 Yi 34B 2.85 T5 v1.1 2.50 2.2. Number of attention heads and head dimension In practice, many models set $d_{head} = d_{model} / num_{heads}$ (and we generally want $d_{head} \\gtrsim d_{model}/num_{heads}$).\nModel Num heads Head dim Model dim Ratio GPT3 96 128 12288 1 T5 128 128 1024 16 T5 v1.1 64 64 4096 1 LaMDA 128 128 8192 2 PaLM 48 258 18432 1.48 LLaMA2 64 128 8192 1 2.3. Model aspect ratio Here the “aspect ratio” refers to:\n$$ d_{model} / num_{layers} $$\nModel ( d_{model} / n_{layer} ) BLOOM 205 T5 v1.1 171 PaLM (540B) 156 GPT3 / OPT / Mistral / Qwen 128 LLaMA / LLaMA2 / Chinchila 102 T5 (11B) 43 GPT2 33 Very deep models are harder to parallelize and tend to have higher latency.\n2.4. Vocabulary size Monolingual: 30k–50k tokens Multilingual: 100k–250k tokens 2.5. Dropout and weight decay Older models tend to use more dropout. Newer models more often rely on weight decay; empirically it interacts with the loss dynamics (e.g., faster loss decrease later in training) rather than merely preventing overfitting. Model Dropout* Weight decay Original transformer 0.1 0 GPT2 0.1 0.1 T5 0.1 0 GPT3 0.1 0.1 T5 v1.1 0 0 PaLM 0 (variable) OPT 0.1 0.1 LLaMA 0 0.1 Qwen 14B 0.1 0.1 3. Training stability tips During training, we want to avoid “spikes” (the blue curve below):\nz-loss Consider the softmax at the final layer of an LLM:\n$$ P(y=i|x) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} = \\frac{e^{z_i}}{Z} $$\nSo for cross-entropy loss we have:\n$$ Loss_{CE} = -\\log P(y=i|x) = -\\log \\frac{e^{z_i}}{\\sum_j e^{z_j}} = -z_i + \\log Z $$\nIf $Z$ becomes too small, $Loss_{CE}$ can become too large, leading to instability.\nSo we try to keep $Z$ close to 1 (equivalently, keep $\\log Z$ close to 0) by adding a z-loss term:\n$$ Loss_{z} = ((\\log Z)^2 - 0)^2 = (\\log Z)^2 $$\nFinally:\n$$ Loss = Loss_{CE} + \\lambda Loss_{z} $$\nwhere $\\lambda$ is a small coefficient, typically $1e-3$ or $1e-4$.\n4. Architecture optimizations 4.1. KV Cache Image source: link\nStandard attention computation:\n$$ Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V $$\nAssume $X \\in \\mathbb{R}^{b \\times T \\times D}$ and $W_{\\{Q, K, V\\}} \\in \\mathbb{R}^{D \\times (hd)}$, where $T$ is the sequence length, $h$ is the number of attention heads, and $d$ is the per-head dimension. Let $D = hd$. The compute cost is:\ncompute K/Q/V: $3 \\times 2bTD^2 = 6bT(hd)^2$ compute $Q \\times K$: $2bhT^2d$ compute softmax: $n \\times bhT^2$ (softmax involves $n$ operations) compute $Output_{softmax} \\times V$: $2bhT^2d$ output projection: $2bTD^2$ total $\\approx 8bTD^2 + 4bhT^2d$ (ignoring softmax) The memory cost is:\nweight parameters: $W_{\\{Q, K, V\\}}$: $3 \\times D(hd) = 3(hd)^2$ output projection: $(hd)D = (hd)^2$ intermediate activations: input: $bTD$ K/Q/V: $3 \\times bhTd$ attention weights (after softmax): $bhT^2$ output: $bTD$ (input to the next layer; often not counted for this layer) total $\\approx 4(hd)^2 + bTD + 3bhTd + bhT^2$ Attention with KV cache:\nDuring training, KV caching does not reduce compute, so it is often not used.\nIn inference, if the current sequence length is $t$, predicting the next token costs $\\approx 8btD^2 + 4bht^2d$. Without KV cache, predicting the next token after that would cost $\\approx 8b(t+1)D^2 + 4bh(t+1)^2d$, which grows significantly with sequence length.\nWith KV cache, predicting the next token costs:\ncompute K/Q/V: since we reuse $K_{1:(t-1)}, V_{1:(t-1)}$, we only compute $Q_t, K_t, V_t$, costing $3 \\times 2bD^2$ compute $Q_t \\times K_{1:t}$: $2bhtd$ softmax: $n \\times bht$ compute $Output_{softmax} \\times V_{1:t}$: $2bhtd$ output projection: $2bD^2$ total $\\approx 8bD^2 + 4bhtd$ (ignoring softmax) The compute now grows only mildly with sequence length.\nKV cache slightly increases memory in inference. Without KV cache, memory is dominated by weights $4(hd)^2$. With KV cache, we add cache storage $2bhtd$, so total memory is $\\approx 4(hd)^2 + 2bhtd$.\n4.2. MQA and GQA To reduce KV-cache memory, a simple idea is to let attention heads share $K$ and $V$:\nif all $h$ heads share a single $K$ and $V$, it is MQA (Multi-query Attention) if we split $h$ heads into $g$ groups and each group shares $K$ and $V$, it is GQA (Grouped-query Attention) Illustration:\nModel Training-time Inference-time Notes GPT-3 / GPT-4 MHA MHA / GQA (partially optimized) GPT-4 reportedly uses GQA PaLM 2 GQA GQA native training structure Claude 3 GQA GQA publicly described by Anthropic LLaMA 2 MHA GQA (converted) converted later by Meta Mistral GQA GQA end-to-end GQA Falcon MQA MQA optimized for long-context inference Gemini 1.5 GQA GQA used by Google for multimodal LLMs Compared to MHA, MQA/GQA keep compute complexity roughly unchanged but reduce memory. Suppose $h$ query heads share $k$ key/value heads:\nweight parameters: $W_{\\{Q\\}}$: $D(hd) = (hd)^2$ $W_{\\{K, V\\}}$: $2 \\times D(kd) = 2(hd)(kd)$ output projection: $(hd)D = (hd)^2$ intermediate activations: input: $bTD$ $Q$: $bhTd$ $KV$: $2bkdT$ attention weights (after softmax): $bhT^2$ output: $bTD$ (input to the next layer; often not counted for this layer) total memory $\\approx 2(hd)^2 + 2hkd^2 + 2bTD + 2bkdT + bhT^2$ 4.3. Sparse Attention Sparse attention: see this blog post.\nReferences stanford-cs336 lecture 3 ","wordCount":"2174","inLanguage":"en","image":"https://cspaulia.github.io/cspaulia-blog/en/posts/transformer_in_llm/LLMs.jpg","datePublished":"2025-10-10T10:00:03+08:00","dateModified":"2026-01-30T16:43:57+08:00","author":{"@type":"Person","name":"CSPaulia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://cspaulia.github.io/cspaulia-blog/en/posts/transformer_in_llm/"},"publisher":{"@type":"Organization","name":"cspaulia-blog","logo":{"@type":"ImageObject","url":"https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://cspaulia.github.io/cspaulia-blog/en/ accesskey=h title="Home (Alt + H)"><img src=https://cspaulia.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://cspaulia.github.io/cspaulia-blog/ title=中文 aria-label=中文>中文</a></li></ul></div></div><ul id=menu><li><a href=https://cspaulia.github.io/cspaulia-blog/en/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/series/ title=Series><span>Series</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://cspaulia.github.io/cspaulia-blog/en/>Home</a>&nbsp;»&nbsp;<a href=https://cspaulia.github.io/cspaulia-blog/en/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">LM Architecture and Training</h1><div class=post-description>Notes on LLM architecture and training hyperparameters</div><div class=post-meta><span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg> October 10, 2025</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 19l7-7 3 3-7 7-3-3z"/><path d="M18 13l-1.5-7.5L2 2l3.5 14.5L13 18l5-5z"/><path d="M2 2l7.586 7.586"/><circle cx="11" cy="11" r="2"/></svg> January 30, 2026</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><path d="M14 2v6h6"/><line x1="8" y1="13" x2="16" y2="13"/><line x1="8" y1="17" x2="16" y2="17"/><line x1="8" y1="9" x2="12" y2="9"/></svg> 2174 words</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg> 5 min</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg> CSPaulia</span>
&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://cspaulia.github.io/cspaulia-blog/posts/transformer_in_llm/>中文</a></li></ul><span id=busuanzi_container_page_pv>｜ <span id=busuanzi_value_page_pv></span> views</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-original-transformer-vs-modern-variants aria-label="1. Original Transformer vs Modern Variants">1. Original Transformer vs Modern Variants</a><ul><li><a href=#11-pre-norm-vs-post-norm aria-label="1.1. Pre-norm vs Post-norm">1.1. Pre-norm vs Post-norm</a></li><li><a href=#12-layernorm-vs-rmsnorm aria-label="1.2. LayerNorm vs RMSNorm">1.2. LayerNorm vs RMSNorm</a></li><li><a href=#13-ffn-with-bias-vs-bias-free aria-label="1.3. FFN: With Bias vs Bias-free">1.3. FFN: With Bias vs Bias-free</a></li><li><a href=#14-activation-functions aria-label="1.4. Activation Functions">1.4. Activation Functions</a></li><li><a href=#15-positional-encoding aria-label="1.5. Positional Encoding">1.5. Positional Encoding</a></li></ul></li><li><a href=#2-hyperparameters aria-label="2. Hyperparameters">2. Hyperparameters</a><ul><li><a href=#21-ffn-hidden-size aria-label="2.1. FFN hidden size">2.1. FFN hidden size</a></li><li><a href=#22-number-of-attention-heads-and-head-dimension aria-label="2.2. Number of attention heads and head dimension">2.2. Number of attention heads and head dimension</a></li><li><a href=#23-model-aspect-ratio aria-label="2.3. Model aspect ratio">2.3. Model aspect ratio</a></li><li><a href=#24-vocabulary-size aria-label="2.4. Vocabulary size">2.4. Vocabulary size</a></li><li><a href=#25-dropout-and-weight-decay aria-label="2.5. Dropout and weight decay">2.5. Dropout and weight decay</a></li></ul></li><li><a href=#3-training-stability-tips aria-label="3. Training stability tips">3. Training stability tips</a><ul><li><a href=#z-loss aria-label=z-loss>z-loss</a></li></ul></li><li><a href=#4-architecture-optimizations aria-label="4. Architecture optimizations">4. Architecture optimizations</a><ul><li><a href=#41-kv-cache aria-label="4.1. KV Cache">4.1. KV Cache</a></li><li><a href=#42-mqa-and-gqa aria-label="4.2. MQA and GQA">4.2. MQA and GQA</a></li><li><a href=#43-sparse-attention aria-label="4.3. Sparse Attention">4.3. Sparse Attention</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h2[id],h3[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>This is an excellent <a href=https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison>blog post</a> summarizing architectural differences among several popular LLMs.</p><h2 id=1-original-transformer-vs-modern-variants>1. Original Transformer vs Modern Variants<a hidden class=anchor aria-hidden=true href=#1-original-transformer-vs-modern-variants>#</a></h2><p>The table below summarizes the major architectural and training differences between the original Transformer (Vaswani et al., 2017) and common Transformer variants used in modern LLMs:</p><table><thead><tr><th>Aspect</th><th>Original Transformer (2017)</th><th>Common Variants in Modern LLMs</th></tr></thead><tbody><tr><td>Normalization order</td><td>Post-LN</td><td>Pre-LN</td></tr><tr><td>Activation</td><td>ReLU</td><td>SwiGLU (GELU/SiLU/Swish variants, etc.)</td></tr><tr><td>Dropout</td><td>Widely used</td><td>Often reduced or removed for large models</td></tr><tr><td>Normalization type</td><td>LayerNorm</td><td>RMSNorm (also LayerNorm/ScaleNorm variants)</td></tr><tr><td>Linear layers</td><td>With bias</td><td>Bias-free</td></tr><tr><td>Attention heads</td><td>Multi-head attention (fixed)</td><td>GQA / MQA, etc.</td></tr><tr><td>Positional encoding</td><td>Absolute (sinusoidal)</td><td>RoPE, etc.</td></tr><tr><td>Other</td><td>-</td><td>FlashAttention, MoE, hierarchical parallelism, etc.</td></tr></tbody></table><h3 id=11-pre-norm-vs-post-norm>1.1. Pre-norm vs Post-norm<a hidden class=anchor aria-hidden=true href=#11-pre-norm-vs-post-norm>#</a></h3><p>Almost all modern language models use pre-norm (except BERT), which tends to make training more stable.</p><p>Left: pre-norm. Right: post-norm.</p><img src=pre-post-norm.png alt=pre-vs-post width=300><p><strong>New!</strong> Left: pre-norm. Right: “double norm” (used by e.g. Grok, Gemma 2).</p><img src=pre-double-norm.png alt=pre-vs-double width=300><p><strong>New!</strong> OlMo 2 applies post-norm only to the non-residual branch.</p><hr><h3 id=12-layernorm-vs-rmsnorm>1.2. LayerNorm vs RMSNorm<a hidden class=anchor aria-hidden=true href=#12-layernorm-vs-rmsnorm>#</a></h3><p>Original Transformer: <strong>LayerNorm</strong> (GPT-1/2/3, OPT, GPT-J, BLOOM)</p><p>$$
y = \frac{x - \textbf{E}[x]}{\sqrt{\textbf{Var}[x] + \epsilon}} * \gamma + \beta
$$</p><p>Modern LMs: <strong>RMSNorm</strong> (LLaMA family, PaLM, T5)</p><p>$$
y = \frac{x}{\sqrt{\lVert x \rVert^2_2 + \epsilon}} * \gamma
$$</p><p>Advantages of RMSNorm: faster in practice without hurting accuracy</p><ul><li>fewer operations (no mean computation)</li><li>fewer parameters (no bias term)</li></ul><hr><h3 id=13-ffn-with-bias-vs-bias-free>1.3. FFN: With Bias vs Bias-free<a hidden class=anchor aria-hidden=true href=#13-ffn-with-bias-vs-bias-free>#</a></h3><p>Original Transformer: with bias</p><p>$$
\textbf{FFN}(x) = \max(0,xW_1+b_1)W_2+b_2
$$</p><p>Modern LMs: bias-free</p><p>$$
\textbf{FFN}(x) = \sigma(xW_1)W_2
$$</p><p>Advantages of bias-free FFNs: smaller memory footprint and more stable optimization.</p><hr><h3 id=14-activation-functions>1.4. Activation Functions<a hidden class=anchor aria-hidden=true href=#14-activation-functions>#</a></h3><table><thead><tr><th style=text-align:center>Activation</th><th style=text-align:center>Model</th></tr></thead><tbody><tr><td style=text-align:center>ReLU</td><td style=text-align:center>Original transformer, T5, Gopher, Chinchilla, OPT</td></tr><tr><td style=text-align:center>GeLU</td><td style=text-align:center>GPT1/2/3, GPTJ, GPT-Neox, BLOOM</td></tr><tr><td style=text-align:center>GeGLU</td><td style=text-align:center>T5 v1.1, mT5, LaMDA, Phi3, Gemma 2, Gemma 3</td></tr><tr><td style=text-align:center>SwiGLU</td><td style=text-align:center>LLaMa 1/2/3, PaLM, Mistral, OlMo, most models post 2023</td></tr></tbody></table><p>For an introduction to activations, see this <a href=../activation/>post</a>.</p><hr><h3 id=15-positional-encoding>1.5. Positional Encoding<a hidden class=anchor aria-hidden=true href=#15-positional-encoding>#</a></h3><h4 id=151-sinusoidal-positional-encoding>1.5.1. Sinusoidal Positional Encoding<a hidden class=anchor aria-hidden=true href=#151-sinusoidal-positional-encoding>#</a></h4><p><strong>Key idea</strong>: use sine/cosine waves of different frequencies to encode position information across dimensions.</p><p>For position $pos$ in the sequence and dimension index $i$, the encoding is:</p><p>$$
\begin{aligned}
PE_{(pos,2i)} = sin(\frac{pos}{10000^{2i/d_{model}}}) \
PE_{(pos,2i+1)} = cos(\frac{pos}{10000^{2i/d_{model}}})
\end{aligned}
$$</p><p>Where:</p><ul><li>$pos$ is the position index (starting from 0)</li><li>$i$ is the dimension index</li><li>$d_{model}$ is the model hidden size</li><li>10000 is a heuristic constant that controls the frequency range</li></ul><h5 id=capturing-relative-positions>Capturing relative positions<a hidden class=anchor aria-hidden=true href=#capturing-relative-positions>#</a></h5><p>Suppose the model attends to two tokens at positions $pos_1$ and $pos_2$. Their encodings are:</p><p>$$
\begin{aligned}
E_1 = [sin(\frac{pos_1}{10000^{0}}), cos(\frac{pos_1}{10000^{0}}), sin(\frac{pos_1}{10000^{2/d_{model}}}), cos(\frac{pos_1}{10000^{2/d_{model}}}), \dots ] \\
E_2 = [sin(\frac{pos_2}{10000^{0}}), cos(\frac{pos_2}{10000^{0}}), sin(\frac{pos_2}{10000^{2/d_{model}}}), cos(\frac{pos_2}{10000^{2/d_{model}}}), \dots ]
\end{aligned}
$$</p><p>Inside the model we may take an inner product:</p><p>$$
E_1 \cdot E_2 = sin(\frac{pos_1}{10000^{0}})sin(\frac{pos_2}{10000^{0}}) + cos(\frac{pos_1}{10000^{0}})cos(\frac{pos_2}{10000^{0}}) + \dots
$$</p><p>Using the trigonometric identity:</p><p>$$
\sin a \sin b + \cos a \cos b = \cos(a - b)
$$</p><p>We obtain:</p><p>$$
E_1 \cdot E_2 = \cos(\frac{pos_1 - pos_2}{10000^{0}}) + \cos(\frac{pos_1 - pos_2}{10000^{2/d_{model}}}) + \dots
$$</p><p>This shows the dot product depends on the relative distance $pos_1 - pos_2$.</p><hr><h4 id=152-absolute-positional-encoding--learnable-positional-embedding>1.5.2. Absolute Positional Encoding / Learnable Positional Embedding<a hidden class=anchor aria-hidden=true href=#152-absolute-positional-encoding--learnable-positional-embedding>#</a></h4><blockquote><p>My take: “absolute positional encoding” is more of a concept — it encodes a token’s absolute index directly, rather than encoding relative offsets between tokens.</p></blockquote><p>A learnable positional embedding <strong>learns an embedding matrix</strong>:</p><p>$$
P = [u_0, u_1, u_2, \dots, u_{L-1}] \in \mathbb{R}^{L \times d_{model}}
$$</p><p>Where:</p><ul><li>$L$ is the maximum sequence length</li><li>each row $u_i$ is a trainable embedding vector for position $i$</li></ul><p>The final input to the model becomes:</p><p>$$
Embed(x, i) = v_x + u_i
$$</p><p>where $v_x$ is the token embedding of token $x$.</p><hr><h4 id=153-relative-positional-encoding>1.5.3. Relative Positional Encoding<a hidden class=anchor aria-hidden=true href=#153-relative-positional-encoding>#</a></h4><p>Limitations of absolute positional encoding:</p><ul><li>poor length generalization: if trained with a fixed length (e.g., 512), it may not extrapolate beyond that</li><li>weak relative awareness: the model knows “the 5th word” and “the 10th word” but not explicitly that they are “5 positions apart”</li></ul><p>However, word order relations in natural language are often relative:</p><blockquote><p>In the dependency between “the cat” and “the big cat”, “cat” is only a few steps away from “the”.</p></blockquote><p>So the goal of relative positional encoding is to let the model learn how the distance $(i-j)$ between token $i$ and token $j$ affects attention.</p><p>One approach is to inject relative position information directly into the attention logits:</p><p>$$
e_{ij} = \frac{(x_i W_Q)(x_j W_K + a^K_{ij})^T}{\sqrt{d_k}}
$$</p><p>where $a_{ij}^K$ is a vector representing the relative position between token $i$ and token $j$.</p><hr><h4 id=154-rope-rotary-position-embedding>1.5.4. RoPE (Rotary Position Embedding)<a hidden class=anchor aria-hidden=true href=#154-rope-rotary-position-embedding>#</a></h4><p>How can we transform the <strong>position-encoded embeddings</strong> $x$ and $y$ such that their dot product depends only on the relative position? Concretely, we want:</p><div id=eq:goal>$$
\langle f(x, i), f(y, j) \rangle = g(x, y, i-j) \tag{1}
$$</div><p>where $\langle \cdot, \cdot \rangle$ denotes an inner product.</p><ul><li>Sinusoidal encoding does <strong>not</strong> satisfy Eq. <a href=#eq:goal>(1)</a>:<ul><li>With sinusoidal encoding, the embedding can be written as $Embed(x, i) = v_x + E_i$, where $v_x$ is the token embedding and $E_i$ is the positional encoding.</li><li>Then $\langle Embed(x, i), Embed(y, j) \rangle = \langle v_x + E_i, v_y + E_j \rangle = \langle v_x, v_y \rangle + \langle v_x, E_j \rangle + \langle E_i, v_y \rangle + \langle E_i, E_j \rangle$, which contains terms depending on absolute positions $i$ and $j$, not only their difference $(i-j)$.</li></ul></li><li>Absolute (learnable) positional embeddings also do <strong>not</strong> satisfy Eq. <a href=#eq:goal>(1)</a>.</li><li>Relative positional encoding does not preserve the pure inner-product form in Eq. <a href=#eq:goal>(1)</a>:<ul><li>the geometric interpretation is weakened: the dot product is no longer simply cosine similarity between vectors once the bias term is injected</li><li>symmetry is broken: $e_{ij}$ and $e_{ji}$ can differ, enabling directionality</li><li>the probabilistic interpretation weakens: logits before softmax are no longer determined only by vector similarity; extra bias can affect stability</li></ul></li></ul><p>The core idea of RoPE is to embed position into each pair of dimensions via complex rotation (equivalently, a 2D plane rotation).</p><img src=rope_example.png alt=rope-example width=400><p>$$
\begin{aligned}
x_p &= [x_{p,0}, x_{p,1}, \dots, x_{p,d-1}]\\
f_{{q,k}}(x_p,p) &= \mathbf{R}^d_{\Theta,p},W_{{q,k}},x_p\\
\mathbf{R}^d_{\Theta,p}
&=
\begin{bmatrix}
\cos(p\theta_0) & -\sin(p\theta_0) & & & \\
\sin(p\theta_0) & \cos(p\theta_0) & & & \\
& & \cos(p\theta_1) & -\sin(p\theta_1) & \\
& & \sin(p\theta_1) & \cos(p\theta_1) & \\
& & & & \ddots \\
& & & & & \cos(p\theta_{d/2-1}) & -\sin(p\theta_{d/2-1}) \\
& & & & & \sin(p\theta_{d/2-1}) & \cos(p\theta_{d/2-1})
\end{bmatrix}
\end{aligned}
$$</p><p>where $\theta_k = 10000^{-2k/d}$.</p><p>Next, we show why RoPE satisfies Eq. <a href=#eq:goal>(1)</a>.</p><p>For two adjacent embedding dimensions $2k$ and $2k+1$ of token $x$, we have:</p><p>$$
\tilde{x}_{p}^{(k)} = \begin{bmatrix}
cos(p\theta_k) & -sin(p\theta_k) \\
sin(p\theta_k) & cos(p\theta_k)
\end{bmatrix}
\begin{bmatrix}
x_{p,2k} \\
x_{p,2k+1}
\end{bmatrix}
=(x_{p,2k}+ix_{p,2k+1})e^{ip\theta_k}
$$</p><p>Therefore:</p><p>$$
\langle \tilde{x}_{p}^{(k)}, \tilde{y}_{q}^{(k)} \rangle = \tilde{x}_{p}^{(k)} \cdot \overline{\tilde{y}}_{q}^{(k)} = (x_{p,2k}+ix_{p,2k+1})(y_{q,2k}-iy_{q,2k+1})e^{i(p-q)\theta_k}
$$</p><p>This depends on $(p-q)$, meeting the requirement in Eq. <a href=#eq:goal>(1)</a>.</p><hr><h2 id=2-hyperparameters>2. Hyperparameters<a hidden class=anchor aria-hidden=true href=#2-hyperparameters>#</a></h2><h3 id=21-ffn-hidden-size>2.1. FFN hidden size<a hidden class=anchor aria-hidden=true href=#21-ffn-hidden-size>#</a></h3><p>Let $d_{ff}$ be the FFN hidden size and $d_{model}$ be the model hidden size.</p><p>$$
d_{ff} = 4d_{model}
$$</p><p>Then the parameter count of a standard FFN is:</p><ul><li>first layer: $d_{model} \times d_{ff} = 4d_{model}^2$</li><li>second layer: $d_{ff} \times d_{model} = 4d_{model}^2$</li><li>total: $8d_{model}^2$</li></ul><p>For an FFN <strong>with a GLU-type activation</strong>, the parameter count is:</p><ul><li>GLU content projection: $d_{model} \times d&rsquo;_{ff}$</li><li>GLU gate projection: $d_{model} \times d&rsquo;_{ff}$</li><li>second layer: $d&rsquo;_{ff} \times d_{model}$</li><li>total: $3d_{model} \times d&rsquo;_{ff}$</li></ul><p>To match the parameter count of a standard FFN, we need:</p><p>$$
3d_{model} \times d&rsquo;_{ff} = 8d_{model}^2
$$</p><p>即</p><p>$$
d&rsquo;_{ff} = \frac{8}{3}d_{model}
$$</p><p>The table below lists $d_{ff}/d_{model}$ ratios used by some popular models:</p><table><thead><tr><th>Model</th><th>$( d_{ff} / d_{model} )$</th></tr></thead><tbody><tr><td>PaLM</td><td>4.00</td></tr><tr><td>Mistral 7B</td><td>3.50</td></tr><tr><td>LLaMA-2 70B</td><td>3.50</td></tr><tr><td>LLaMA 70B</td><td>2.68</td></tr><tr><td>Qwen 14B</td><td>2.67</td></tr><tr><td>DeepSeek 67B</td><td>2.68</td></tr><tr><td>Yi 34B</td><td>2.85</td></tr><tr><td>T5 v1.1</td><td>2.50</td></tr></tbody></table><hr><h3 id=22-number-of-attention-heads-and-head-dimension>2.2. Number of attention heads and head dimension<a hidden class=anchor aria-hidden=true href=#22-number-of-attention-heads-and-head-dimension>#</a></h3><p>In practice, many models set $d_{head} = d_{model} / num_{heads}$ (and we generally want $d_{head} \gtrsim d_{model}/num_{heads}$).</p><table><thead><tr><th>Model</th><th>Num heads</th><th>Head dim</th><th>Model dim</th><th>Ratio</th></tr></thead><tbody><tr><td>GPT3</td><td>96</td><td>128</td><td>12288</td><td>1</td></tr><tr><td>T5</td><td>128</td><td>128</td><td>1024</td><td>16</td></tr><tr><td>T5 v1.1</td><td>64</td><td>64</td><td>4096</td><td>1</td></tr><tr><td>LaMDA</td><td>128</td><td>128</td><td>8192</td><td>2</td></tr><tr><td>PaLM</td><td>48</td><td>258</td><td>18432</td><td>1.48</td></tr><tr><td>LLaMA2</td><td>64</td><td>128</td><td>8192</td><td>1</td></tr></tbody></table><hr><h3 id=23-model-aspect-ratio>2.3. Model aspect ratio<a hidden class=anchor aria-hidden=true href=#23-model-aspect-ratio>#</a></h3><p>Here the “aspect ratio” refers to:</p><p>$$
d_{model} / num_{layers}
$$</p><table><thead><tr><th>Model</th><th>( d_{model} / n_{layer} )</th></tr></thead><tbody><tr><td>BLOOM</td><td>205</td></tr><tr><td>T5 v1.1</td><td>171</td></tr><tr><td>PaLM (540B)</td><td>156</td></tr><tr><td>GPT3 / OPT / Mistral / Qwen</td><td>128</td></tr><tr><td>LLaMA / LLaMA2 / Chinchila</td><td>102</td></tr><tr><td>T5 (11B)</td><td>43</td></tr><tr><td>GPT2</td><td>33</td></tr></tbody></table><p>Very deep models are harder to parallelize and tend to have higher latency.</p><img src=parallel.png alt=model-parallelism width=400><hr><h3 id=24-vocabulary-size>2.4. Vocabulary size<a hidden class=anchor aria-hidden=true href=#24-vocabulary-size>#</a></h3><ul><li>Monolingual: 30k–50k tokens</li><li>Multilingual: 100k–250k tokens</li></ul><hr><h3 id=25-dropout-and-weight-decay>2.5. Dropout and weight decay<a hidden class=anchor aria-hidden=true href=#25-dropout-and-weight-decay>#</a></h3><ul><li>Older models tend to use more dropout.</li><li>Newer models more often rely on weight decay; empirically it interacts with the loss dynamics (e.g., faster loss decrease later in training) rather than merely preventing overfitting.</li></ul><table><thead><tr><th>Model</th><th>Dropout*</th><th>Weight decay</th></tr></thead><tbody><tr><td>Original transformer</td><td>0.1</td><td>0</td></tr><tr><td>GPT2</td><td>0.1</td><td>0.1</td></tr><tr><td>T5</td><td>0.1</td><td>0</td></tr><tr><td>GPT3</td><td>0.1</td><td>0.1</td></tr><tr><td>T5 v1.1</td><td>0</td><td>0</td></tr><tr><td>PaLM</td><td>0</td><td>(variable)</td></tr><tr><td>OPT</td><td>0.1</td><td>0.1</td></tr><tr><td>LLaMA</td><td>0</td><td>0.1</td></tr><tr><td>Qwen 14B</td><td>0.1</td><td>0.1</td></tr></tbody></table><img src=weight_decay_effect.png alt=weight-decay-effect width=400><hr><h2 id=3-training-stability-tips>3. Training stability tips<a hidden class=anchor aria-hidden=true href=#3-training-stability-tips>#</a></h2><p>During training, we want to avoid “spikes” (the blue curve below):</p><img src=stability.png alt=training-stability-techniques width=600><h3 id=z-loss>z-loss<a hidden class=anchor aria-hidden=true href=#z-loss>#</a></h3><img src=softmax_in_llm.png alt=softmax_in_llm width=200><p>Consider the softmax at the final layer of an LLM:</p><p>$$
P(y=i|x) = \frac{e^{z_i}}{\sum_j e^{z_j}} = \frac{e^{z_i}}{Z}
$$</p><p>So for cross-entropy loss we have:</p><p>$$
Loss_{CE} = -\log P(y=i|x) = -\log \frac{e^{z_i}}{\sum_j e^{z_j}} = -z_i + \log Z
$$</p><p>If $Z$ becomes too small, $Loss_{CE}$ can become too large, leading to instability.</p><p>So we try to keep $Z$ close to 1 (equivalently, keep $\log Z$ close to 0) by adding a z-loss term:</p><p>$$
Loss_{z} = ((\log Z)^2 - 0)^2 = (\log Z)^2
$$</p><p>Finally:</p><p>$$
Loss = Loss_{CE} + \lambda Loss_{z}
$$</p><p>where $\lambda$ is a small coefficient, typically $1e-3$ or $1e-4$.</p><hr><h2 id=4-architecture-optimizations>4. Architecture optimizations<a hidden class=anchor aria-hidden=true href=#4-architecture-optimizations>#</a></h2><h3 id=41-kv-cache>4.1. KV Cache<a hidden class=anchor aria-hidden=true href=#41-kv-cache>#</a></h3><img src=kv_cache.gif alt=kv-cache width=600><p>Image source: <a href=https://medium.com/@joaolages/kv-caching-explained-276520203249>link</a></p><p><strong>Standard attention computation</strong>:</p><p>$$
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
$$</p><p>Assume $X \in \mathbb{R}^{b \times T \times D}$ and $W_{\{Q, K, V\}} \in \mathbb{R}^{D \times (hd)}$, where $T$ is the sequence length, $h$ is the number of attention heads, and $d$ is the per-head dimension. Let $D = hd$. The compute cost is:</p><ul><li>compute K/Q/V: $3 \times 2bTD^2 = 6bT(hd)^2$</li><li>compute $Q \times K$: $2bhT^2d$</li><li>compute softmax: $n \times bhT^2$ (softmax involves $n$ operations)</li><li>compute $Output_{softmax} \times V$: $2bhT^2d$</li><li>output projection: $2bTD^2$</li><li>total $\approx 8bTD^2 + 4bhT^2d$ (ignoring softmax)</li></ul><p>The memory cost is:</p><ul><li>weight parameters:<ul><li>$W_{\{Q, K, V\}}$: $3 \times D(hd) = 3(hd)^2$</li><li>output projection: $(hd)D = (hd)^2$</li></ul></li><li>intermediate activations:<ul><li>input: $bTD$</li><li>K/Q/V: $3 \times bhTd$</li><li>attention weights (after softmax): $bhT^2$</li><li>output: $bTD$ (input to the next layer; often not counted for this layer)</li></ul></li><li>total $\approx 4(hd)^2 + bTD + 3bhTd + bhT^2$</li></ul><p><strong>Attention with KV cache</strong>:</p><p>During training, KV caching does not reduce compute, so it is often not used.</p><p>In inference, if the current sequence length is $t$, predicting the next token costs $\approx 8btD^2 + 4bht^2d$. Without KV cache, predicting the <em>next</em> token after that would cost $\approx 8b(t+1)D^2 + 4bh(t+1)^2d$, which grows significantly with sequence length.</p><p>With KV cache, predicting the next token costs:</p><ul><li>compute K/Q/V: since we reuse $K_{1:(t-1)}, V_{1:(t-1)}$, we only compute $Q_t, K_t, V_t$, costing $3 \times 2bD^2$</li><li>compute $Q_t \times K_{1:t}$: $2bhtd$</li><li>softmax: $n \times bht$</li><li>compute $Output_{softmax} \times V_{1:t}$: $2bhtd$</li><li>output projection: $2bD^2$</li><li>total $\approx 8bD^2 + 4bhtd$ (ignoring softmax)</li></ul><p>The compute now grows only mildly with sequence length.</p><p>KV cache slightly increases memory in inference. Without KV cache, memory is dominated by weights $4(hd)^2$. With KV cache, we add cache storage $2bhtd$, so total memory is $\approx 4(hd)^2 + 2bhtd$.</p><hr><h3 id=42-mqa-and-gqa>4.2. MQA and GQA<a hidden class=anchor aria-hidden=true href=#42-mqa-and-gqa>#</a></h3><p>To reduce KV-cache memory, a simple idea is to let attention heads <strong>share</strong> $K$ and $V$:</p><ul><li>if all $h$ heads share a single $K$ and $V$, it is MQA (Multi-query Attention)</li><li>if we split $h$ heads into $g$ groups and each group shares $K$ and $V$, it is GQA (Grouped-query Attention)</li></ul><p>Illustration:</p><img src=attention_variant.png alt=attention-variants width=600><table><thead><tr><th>Model</th><th>Training-time</th><th>Inference-time</th><th>Notes</th></tr></thead><tbody><tr><td>GPT-3 / GPT-4</td><td>MHA</td><td>MHA / GQA (partially optimized)</td><td>GPT-4 reportedly uses GQA</td></tr><tr><td>PaLM 2</td><td>GQA</td><td>GQA</td><td>native training structure</td></tr><tr><td>Claude 3</td><td>GQA</td><td>GQA</td><td>publicly described by Anthropic</td></tr><tr><td>LLaMA 2</td><td>MHA</td><td>GQA (converted)</td><td>converted later by Meta</td></tr><tr><td>Mistral</td><td>GQA</td><td>GQA</td><td>end-to-end GQA</td></tr><tr><td>Falcon</td><td>MQA</td><td>MQA</td><td>optimized for long-context inference</td></tr><tr><td>Gemini 1.5</td><td>GQA</td><td>GQA</td><td>used by Google for multimodal LLMs</td></tr></tbody></table><p>Compared to MHA, MQA/GQA keep compute complexity roughly unchanged but reduce memory. Suppose $h$ query heads share $k$ key/value heads:</p><ul><li>weight parameters:<ul><li>$W_{\{Q\}}$: $D(hd) = (hd)^2$</li><li>$W_{\{K, V\}}$: $2 \times D(kd) = 2(hd)(kd)$</li><li>output projection: $(hd)D = (hd)^2$</li></ul></li><li>intermediate activations:<ul><li>input: $bTD$</li><li>$Q$: $bhTd$</li><li>$KV$: $2bkdT$</li><li>attention weights (after softmax): $bhT^2$</li><li>output: $bTD$ (input to the next layer; often not counted for this layer)</li></ul></li><li>total memory $\approx 2(hd)^2 + 2hkd^2 + 2bTD + 2bkdT + bhT^2$</li></ul><hr><h3 id=43-sparse-attention>4.3. Sparse Attention<a hidden class=anchor aria-hidden=true href=#43-sparse-attention>#</a></h3><p>Sparse attention: see this <a href=https://newsletter.theaiedge.io/p/understanding-the-sparse-transformers>blog post</a>.</p><img src=sparse_attention.png alt=sparse-attention><hr><div class=zhihu-ref><div class=zhihu-ref-title>References</div><ol><li><a href=https://github.com/stanford-cs336/spring2025-lectures/blob/e9cb2488fdb53ea37f0e38924ec3a1701925cef3/nonexecutable/2025%20Lecture%203%20-%20architecture.pdf target=_blank>stanford-cs336 lecture 3</a></li></ol></div></div><br><div><div class=copyright-card><div class=copyright-card-main><div class=copyright-info><h3 class=copyright-title>LM Architecture and Training</h3><p class=copyright-link><a href=https://cspaulia.github.io/cspaulia-blog/en/posts/transformer_in_llm/>https://cspaulia.github.io/cspaulia-blog/en/posts/transformer_in_llm/</a></p><div class=copyright-meta><span><strong>Author</strong><br>CSPaulia</span>
<span><strong>Published at</strong><br>October 10, 2025</span>
<span><strong>Copyright</strong><br><a rel=license href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank>CC BY-NC-SA 4.0</a></span></div></div><div class=copyright-icon>©</div></div></div></div><footer class=post-footer><p style=font-size:medium;margin-bottom:5px;font-weight:700>categories</p><ul class=post-tags><li><a href=https://cspaulia.github.io/cspaulia-blog/en/categories/large-language-model/>Large Language Model</a></li></ul><p style=font-size:medium;margin-bottom:5px;font-weight:700>tags</p><ul class=post-tags><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/llm/>LLM</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/architecture/>Architecture</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/training/>Training</a></li></ul><nav class=paginav><a class=next href=https://cspaulia.github.io/cspaulia-blog/en/posts/generation_with_sdes/><span class=title>Next »</span><br><span>Generation Models with SDEs</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share LM Architecture and Training on x" href="https://x.com/intent/tweet/?text=LM%20Architecture%20and%20Training&amp;url=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fen%2fposts%2ftransformer_in_llm%2f&amp;hashtags=LLM%2cArchitecture%2cTraining"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LM Architecture and Training on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fen%2fposts%2ftransformer_in_llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LM Architecture and Training on telegram" href="https://telegram.me/share/url?text=LM%20Architecture%20and%20Training&amp;url=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fen%2fposts%2ftransformer_in_llm%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://cspaulia.github.io/cspaulia-blog/en/>cspaulia-blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>