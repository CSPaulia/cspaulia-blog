<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>SFT and RLHF | cspaulia-blog</title><meta name=keywords content="SFT,Reinforcement Learning,RLHF"><meta name=description content="Introduction of SFT and RLHF."><meta name=author content="CSPaulia"><link rel=canonical href=https://cspaulia.github.io/cspaulia-blog/en/posts/sft_rlhf/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/cspaulia-blog/assets/css/stylesheet.4a40da687e9ad320449d5d267445e114ee7b6620a3d534bde2b12baa408f07f5.css integrity="sha256-SkDaaH6a0yBEnV0mdEXhFO57ZiCj1TS94rErqkCPB/U=" rel="preload stylesheet" as=style><link rel=icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://cspaulia.github.io/cspaulia-blog/posts/sft_rlhf/><link rel=alternate hreflang=en href=https://cspaulia.github.io/cspaulia-blog/en/posts/sft_rlhf/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1,trust:!0,strict:!1})})</script><meta property="og:url" content="https://cspaulia.github.io/cspaulia-blog/en/posts/sft_rlhf/"><meta property="og:site_name" content="cspaulia-blog"><meta property="og:title" content="SFT and RLHF"><meta property="og:description" content="Introduction of SFT and RLHF."><meta property="og:locale" content="en-US"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-29T11:30:03+08:00"><meta property="article:modified_time" content="2026-01-30T16:43:57+08:00"><meta property="article:tag" content="SFT"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="RLHF"><meta property="og:image" content="https://cspaulia.github.io/cspaulia-blog/en/posts/sft_rlhf/cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cspaulia.github.io/cspaulia-blog/en/posts/sft_rlhf/cover.jpg"><meta name=twitter:title content="SFT and RLHF"><meta name=twitter:description content="Introduction of SFT and RLHF."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://cspaulia.github.io/cspaulia-blog/en/posts/"},{"@type":"ListItem","position":2,"name":"SFT and RLHF","item":"https://cspaulia.github.io/cspaulia-blog/en/posts/sft_rlhf/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"SFT and RLHF","name":"SFT and RLHF","description":"Introduction of SFT and RLHF.","keywords":["SFT","Reinforcement Learning","RLHF"],"articleBody":"1. Post-Training: Three Stages From InstructGPT[1]\nCollect data and train a supervised policy. Sample a prompt from the prompt dataset. Annotators label the desired output. Use the labeled data to perform supervised fine-tuning of the LLM. Collect comparison data and train a reward model. Sample a prompt and multiple model outputs. Annotators rank these outputs from “best” to “worst”. Use the ranking data to train the reward model. With the trained reward model, optimize the policy using reinforcement learning. Sample a new prompt from the dataset. Generate an output with the current policy. The reward model scores the output (Reward). Update the policy using PPO (or other RL methods) based on the reward. 2. Building an SFT Dataset 2.1. Issues in the dataset An example from the FLAN dataset:\nWhat is this text about? OPTIONS: - World - Sport - Business - Science/Tech\nNatural conversations usually do not contain such explicit options. An example from OpenAssistant:\nQuestion: Can you write a short introduction about the relevance of the term\"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\nAnswer: “Monopsony” refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. […]. Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue. References: Bivens, J., \u0026 Mishel, L.(2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.\nHuman annotators often cannot write answers that are this long and detailed. How should you choose notation and even writing style? Should the answer include references? Getting references right is hard. We often want academic-style outputs, so we add references into answers in the dataset. But the model may only learn the shallow pattern “answers should include references”, which can increase hallucinations. Should the answers contain complex knowledge? Dataset size. Dataset safety. How to decide the output length: as the figure below shows, instruction datasets can have very different input/output lengths. 2.2. Practical tips for constructing SFT data SFT works best when the base model already has certain capabilities, and the data helps “extract” them. If you try to use SFT to “add” behaviors the model fundamentally lacks, results are often poor. Not all factually correct data improves performance. Even high-quality factual data can disrupt the model’s existing distribution/alignment and degrade performance. Some data types (e.g., safety, instruction-following, style) can yield large gains even in small amounts. However, improving long-tail behaviors (broad coverage, sparse scenarios) typically requires much more data. 2.3. Instruction tuning during pretraining Pretrain on web data or a pretraining corpus. Mix instruction-tuning data into pretraining. Do an additional short instruction-tuning stage. 2.4. Midtraining / Two-phase training This recipe[2] seems to be adopted by many LLM companies (though detailed docs are rare):\nIn the Stable stage, train on a pure pretraining dataset (left in the figure). In the Decay stage, train on a mixture of pretraining + instruction-tuning data (right in the figure). 3. RLHF (Reinforcement Learning with Human Feedback) 3.1. From imitation to optimization Imitation (SFT): adjust the model’s output distribution to match a reference distribution $p^*(y|x)$, so that $\\hat{p}(y|x) \\approx p^*(y|x)$.\nFrom a pure generative modeling perspective, SFT teaches the model to imitate the reference distribution. Training requires data from the reference policy (e.g., human-labeled datasets); otherwise, there is nothing to imitate. Optimization (RLHF): continuously adjust $\\hat{p}(y|x)$ to maximize $\\max\\limits_{p} E_p[R(y,x)]$, where $R(y,x)$ is the reward.\nWe are not optimizing an abstract “true distribution”, but a reward function we can define and measure (in RLHF, this reward comes from human feedback: rankings, pairwise preferences, or a trained reward model). At this stage, we no longer view the LM as an approximation to $p^*(y|x)$ (as in SFT). Instead, we treat it as a policy to maximize the reward signal. 3.2. Why RLHF is needed Cost: SFT is expensive, especially the annotation cost. G–V Gap (Generation–Value Gap): what people write (generation distribution $G$) is not always aligned with what people actually prefer (from the viewpoint of a value model $V$). A prior study[3] found that when some annotators compared their own summaries with model-written summaries, they sometimes preferred the model’s summaries—suggesting human-written references are not always optimal.\n3.3. How to collect RLHF data Option 1: (On the web) ask the model to produce $N$ outputs and let annotators (users) rank them.\nPotential issues: The labels may be low-quality or incorrect, or even generated by other LLMs—depending on the annotators. The annotator population distribution can significantly shape model behavior. For example, if many annotators are from Asia, the model may drift toward “Asian-style” outputs.\nDifferent annotators care about different things. Some focus on formatting; others focus on content.\nOption 2: use a large language model (e.g., GPT-4) to rank $N$ outputs (possibly from multiple models). This is often called AI Feedback.\n3.4. Methods to implement RLHF 3.4.1. PPO with human feedback Vanilla PPO = policy gradient + off-policy correction. Its objective is:\n$$ \\max L^{CLIP}(\\theta) - \\beta KL[\\pi_\\theta||\\pi_{old}] $$\n其中$L^{CLIP}(\\theta) = \\mathbb{E}_t \\Big[ \\min \\big( r_t(\\theta) \\hat{A}_t,; \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t \\big) \\Big]$，\n其中$r_t(\\theta) = \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}$，\n人类反馈下的PPO的优化目标为：\n$$ L^{RLHF}(\\theta) = \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) ] - \\beta D_{KL} \\left[ \\pi(y|x) || \\pi_{\\text{ref}}(y|x) \\right] $$\nWhere:\n$r(x,y)$: the reward model’s score for output $y$ given input $x$. $\\pi(y\\mid x)$: the current policy (the RLHF-trained model), i.e., the distribution over outputs $y$ given input $x$. $\\pi_{\\text{ref}}(y\\mid x)$: the reference policy, usually a frozen SFT model, used to prevent the policy from drifting too far. $D_{KL} \\left[ \\pi(y|x) || \\pi_{\\text{ref}}(y|x) \\right]$: the KL divergence between the current and reference policies. Q1: Why does vanilla PPO optimize $\\mathbb{E}_t[ r_t(\\theta)\\hat{A}t ]$, while RLHF optimizes $\\mathbb{E}{x,y}[r(x,y)]$?\nIn standard RL tasks, advantages $\\hat{A}_t$ are often estimated from an offline behavior policy, so the objective takes the form $\\max \\mathbb{E}_t[ r_t(\\theta)\\hat{A}_t ]$. In RLHF for LLMs, we use a reward model to directly score the on-policy outputs from the current LLM, so we do not need importance sampling correction via $r_t(\\theta)$. Q2: What is the difference between $\\pi_{old}$ in vanilla PPO and $\\pi_{\\text{ref}}$ in RLHF PPO?\n$\\pi_{old}$: the policy from the previous iteration; used for data collection and importance sampling; updated continuously. $\\pi_{\\text{ref}}$: a fixed reference policy, typically the frozen SFT model before RLHF; it is not updated. 3.4.2. DPO DPO uses the same high-level objective as PPO:\n$$ \\max L^{RLHF}(\\theta) = \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) ] - \\beta D_{KL} \\left[ \\pi(y|x) || \\pi_{\\text{ref}}(y|x) \\right] $$\nFrom this objective, the optimal policy is:\n$$ \\pi(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y)) $$\n其中，$Z(x) = \\sum\\limits_y \\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))$\nClick to expand the derivation $$ \\begin{align} \u0026\\max\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) ] - \\beta D_{KL} \\left[ \\pi(y|x) || \\pi_{\\text{ref}}(y|x) \\right] \\\\ = \u0026\\max\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) ] - \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\beta \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} ] \\\\ = \u0026\\max\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) - \\beta \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} ] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\beta \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - r(x,y) ] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\log \\exp(\\frac{1}{\\beta}r(x,y)) ] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))}] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))\\frac{1}{Z(x)} Z(x)}] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))} - \\log Z(x)] \\\\ \\end{align} $$\nwhere $Z(x) = \\sum\\limits_y \\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))$.\nLet $\\pi^\\star(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y)) = \\frac{\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))}{\\sum\\limits_y \\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))}$. Then:\n$$ \\begin{align} \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))} - \\log Z(x)] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi^\\star(y|x)} - \\log Z(x)] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi^\\star(y|x)}] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [D_{KL} (\\pi(y|x) || \\pi^\\star(y|x))] \\\\ \\end{align} $$\nThe optimum is $\\pi(y|x) = \\pi^\\star(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))$.\nPlugging the optimal policy into the maximum likelihood form of the Bradley–Terry model yields the DPO loss:\n$$ Loss_{DPO} = - \\ln \\sigma \\left( \\beta \\ln \\frac{\\pi(y^+ \\mid x)}{\\pi_{ref}(y^+ \\mid x)} - \\beta \\ln \\frac{\\pi(y^- \\mid x)}{\\pi_{ref}(y^- \\mid x)} \\right) $$\nHere, $y^+$ is the preferred (higher-quality) sample and $y^-$ is the dispreferred (lower-quality) sample.\nClick to expand: Bradley–Terry model Bradley–Terry model:\n$$ P(i\u003ej) = \\frac{\\alpha_i}{\\alpha_i + \\alpha_j} $$\n$P(i\u003ej)$ is the probability that item $i$ beats item $j$. A common loss is\n$$ Loss = -\\mathbb{E}_{(\\alpha_x, \\alpha_y) \\sim D} [ \\ln \\frac{\\alpha_i}{\\alpha_i + \\alpha_j}] $$\nBradley–Terry model for LLM preferences:\n$$ P(y_1\u003ey_2) = \\frac{r(x,y_1)}{r(x,y_1) + r(x,y_2)} $$\nwhere $x$ is the input prompt, $y$ is an output, and $r(x,y)$ is the reward score. To avoid negative values, we exponentiate:\n$$ P(y_1\u003ey_2) = \\frac{\\exp(r(x,y_1))}{\\exp(r(x,y_1)) + \\exp(r(x,y_2))} $$\nThe loss is\n$$ \\begin{align} \\text{Loss} \u0026= - \\mathbb{E}_{(x, y^+, y^-) \\sim D} [ \\ln ( \\frac{\\exp(r(x, y^+))}{\\exp(r(x, y^+)) + \\exp(r(x, y^-))} ) ] \\\\ \u0026= - \\mathbb{E}_{(x, y^+, y^-) \\sim D} [ \\ln ( \\frac{1}{1 + \\exp(r(x, y^-)) - r(x, y^+)} ) ] \\\\ \u0026= - \\mathbb{E}_{(x, y^+, y^-) \\sim D} [ \\ln \\sigma ( r(x, y^+) - r(x, y^-) ) ] \\end{align} $$\nwhere $\\sigma(x) = \\frac{1}{1+\\exp(-x)}$ is the sigmoid.\nClick to expand the derivation $$ \\begin{align} \u0026\\pi(y \\mid x) = \\frac{1}{Z(x)} \\pi_{ref}(y \\mid x) \\exp( \\frac{1}{\\beta} r(x, y)) \\\\ \\Rightarrow \u0026\\exp( \\frac{1}{\\beta} r(x, y) ) = \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} Z(x) \\\\ \\Rightarrow \u0026r(x, y) = \\beta \\ln ( \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} Z(x) ) \\\\ \\Rightarrow \u0026r(x, y) = \\beta \\ln ( \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} ) + \\beta \\ln Z(x) \\\\ \\end{align} $$\nSubstitute into the Bradley–Terry formulation:\n$$ \\begin{align} Loss \u0026 = - \\ln \\sigma ( r(x, y^+) - r(x, y^-) ) \\\\ \u0026 = - \\ln \\sigma (\\beta \\ln ( \\frac{\\pi(y^+ \\mid x)}{\\pi_{ref}(y^+ \\mid x)} ) + \\beta \\ln Z(x) - \\beta \\ln ( \\frac{\\pi(y^- \\mid x)}{\\pi_{ref}(y^- \\mid x)} ) - \\beta \\ln Z(x)) \\\\ \u0026 = - \\ln \\sigma (\\beta \\ln ( \\frac{\\pi(y^+ \\mid x)}{\\pi_{ref}(y^+ \\mid x)} ) - \\beta \\ln ( \\frac{\\pi(y^- \\mid x)}{\\pi_{ref}(y^- \\mid x)} )) \\end{align} $$\nReferences Ouyang L, Wu J, Jiang X, et al. Training language models to follow instructions with human feedback[J]. Advances in neural information processing systems, 2022, 35: 27730-27744. Hu S, Tu Y, Han X, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies[J]. arXiv preprint arXiv:2404.06395, 2024. Zhang T, Ladhak F, Durmus E, et al. Benchmarking large language models for news summarization[J]. Transactions of the Association for Computational Linguistics, 2024, 12: 39-57. stanford-cs336 lecture 15 ","wordCount":"2089","inLanguage":"en","image":"https://cspaulia.github.io/cspaulia-blog/en/posts/sft_rlhf/cover.jpg","datePublished":"2025-09-29T11:30:03+08:00","dateModified":"2026-01-30T16:43:57+08:00","author":{"@type":"Person","name":"CSPaulia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://cspaulia.github.io/cspaulia-blog/en/posts/sft_rlhf/"},"publisher":{"@type":"Organization","name":"cspaulia-blog","logo":{"@type":"ImageObject","url":"https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://cspaulia.github.io/cspaulia-blog/en/ accesskey=h title="Home (Alt + H)"><img src=https://cspaulia.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://cspaulia.github.io/cspaulia-blog/ title=中文 aria-label=中文>中文</a></li></ul></div></div><ul id=menu><li><a href=https://cspaulia.github.io/cspaulia-blog/en/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/series/ title=Series><span>Series</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://cspaulia.github.io/cspaulia-blog/en/>Home</a>&nbsp;»&nbsp;<a href=https://cspaulia.github.io/cspaulia-blog/en/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">SFT and RLHF</h1><div class=post-description>Introduction of SFT and RLHF.</div><div class=post-meta><span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg> September 29, 2025</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 19l7-7 3 3-7 7-3-3z"/><path d="M18 13l-1.5-7.5L2 2l3.5 14.5L13 18l5-5z"/><path d="M2 2l7.586 7.586"/><circle cx="11" cy="11" r="2"/></svg> January 30, 2026</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><path d="M14 2v6h6"/><line x1="8" y1="13" x2="16" y2="13"/><line x1="8" y1="17" x2="16" y2="17"/><line x1="8" y1="9" x2="12" y2="9"/></svg> 2089 words</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg> 5 min</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg> CSPaulia</span>
&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://cspaulia.github.io/cspaulia-blog/posts/sft_rlhf/>中文</a></li></ul><span id=busuanzi_container_page_pv>｜ <span id=busuanzi_value_page_pv></span> views</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-post-training-three-stages aria-label="1. Post-Training: Three Stages">1. Post-Training: Three Stages</a></li><li><a href=#2-building-an-sft-dataset aria-label="2. Building an SFT Dataset">2. Building an SFT Dataset</a><ul><li><a href=#21-issues-in-the-dataset aria-label="2.1. Issues in the dataset">2.1. Issues in the dataset</a></li><li><a href=#22-practical-tips-for-constructing-sft-data aria-label="2.2. Practical tips for constructing SFT data">2.2. Practical tips for constructing SFT data</a></li><li><a href=#23-instruction-tuning-during-pretraining aria-label="2.3. Instruction tuning during pretraining">2.3. Instruction tuning during pretraining</a></li><li><a href=#24-midtraining--two-phase-training aria-label="2.4. Midtraining / Two-phase training">2.4. Midtraining / Two-phase training</a></li></ul></li><li><a href=#3-rlhf-reinforcement-learning-with-human-feedback aria-label="3. RLHF (Reinforcement Learning with Human Feedback)">3. RLHF (Reinforcement Learning with Human Feedback)</a><ul><li><a href=#31-from-imitation-to-optimization aria-label="3.1. From imitation to optimization">3.1. From imitation to optimization</a></li><li><a href=#32-why-rlhf-is-needed aria-label="3.2. Why RLHF is needed">3.2. Why RLHF is needed</a></li><li><a href=#33-how-to-collect-rlhf-data aria-label="3.3. How to collect RLHF data">3.3. How to collect RLHF data</a></li><li><a href=#34-methods-to-implement-rlhf aria-label="3.4. Methods to implement RLHF">3.4. Methods to implement RLHF</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h2[id],h3[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=1-post-training-three-stages>1. Post-Training: Three Stages<a hidden class=anchor aria-hidden=true href=#1-post-training-three-stages>#</a></h2><blockquote><p>From InstructGPT<a href=/cspaulia-blog/en/posts/sft_rlhf/#ref1>[1]</a></p></blockquote><p><img alt=stages loading=lazy src=/cspaulia-blog/posts/sft_rlhf/stage.png></p><ol><li>Collect data and train a <strong>supervised</strong> policy.</li></ol><ul><li>Sample a prompt from the prompt dataset.</li><li>Annotators label the desired output.</li><li>Use the labeled data to perform supervised fine-tuning of the LLM.</li></ul><ol start=2><li>Collect <strong>comparison data</strong> and train a <strong>reward model</strong>.</li></ol><ul><li>Sample a prompt and multiple model outputs.</li><li>Annotators rank these outputs from “best” to “worst”.</li><li>Use the ranking data to train the reward model.</li></ul><ol start=3><li>With the <strong>trained reward model</strong>, optimize the policy using <strong>reinforcement learning</strong>.</li></ol><ul><li>Sample a new prompt from the dataset.</li><li>Generate an output with the current policy.</li><li>The reward model scores the output (Reward).</li><li>Update the policy using PPO (or other RL methods) based on the reward.</li></ul><hr><h2 id=2-building-an-sft-dataset>2. Building an SFT Dataset<a hidden class=anchor aria-hidden=true href=#2-building-an-sft-dataset>#</a></h2><h3 id=21-issues-in-the-dataset>2.1. Issues in the dataset<a hidden class=anchor aria-hidden=true href=#21-issues-in-the-dataset>#</a></h3><blockquote><p>An example from the FLAN dataset:</p><p>What is this text about? OPTIONS: - World - Sport - Business - Science/Tech</p></blockquote><ul><li>Natural conversations usually do not contain such explicit options.</li></ul><blockquote><p>An example from OpenAssistant:</p><p>Question: Can you write a short introduction about the relevance of the term"monopsony" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.</p><p>Answer: &ldquo;Monopsony&rdquo; refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. [&mldr;]. Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue. References: Bivens, J., & Mishel, L.(2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.</p></blockquote><ul><li>Human annotators often cannot write answers that are this <strong>long and detailed</strong>.</li><li>How should you choose notation and even writing style?</li><li>Should the answer include <strong>references</strong>?<ul><li>Getting references right is hard. We often want academic-style outputs, so we add references into answers in the dataset. But the model may only learn the shallow pattern “answers should include references”, which can increase hallucinations.</li></ul></li><li>Should the answers contain <strong>complex knowledge</strong>?</li><li>Dataset size.</li><li>Dataset safety.</li><li>How to decide the output length: as the figure below shows, instruction datasets can have very different input/output lengths.</li></ul><p><img alt=instruction_dataset loading=lazy src=/cspaulia-blog/posts/sft_rlhf/instrction_dataset.png></p><hr><h3 id=22-practical-tips-for-constructing-sft-data>2.2. Practical tips for constructing SFT data<a hidden class=anchor aria-hidden=true href=#22-practical-tips-for-constructing-sft-data>#</a></h3><ol><li>SFT works best when the base model already has certain capabilities, and the data helps “extract” them. If you try to use SFT to “add” behaviors the model fundamentally lacks, results are often poor.</li><li>Not all factually correct data improves performance. Even high-quality factual data can disrupt the model’s existing distribution/alignment and degrade performance.</li><li>Some data types (e.g., safety, instruction-following, style) can yield large gains even in small amounts. However, improving long-tail behaviors (broad coverage, sparse scenarios) typically requires much more data.</li></ol><hr><h3 id=23-instruction-tuning-during-pretraining>2.3. Instruction tuning during pretraining<a hidden class=anchor aria-hidden=true href=#23-instruction-tuning-during-pretraining>#</a></h3><ol><li>Pretrain on web data or a pretraining corpus.</li><li>Mix instruction-tuning data into pretraining.</li><li>Do an additional short instruction-tuning stage.</li></ol><hr><h3 id=24-midtraining--two-phase-training>2.4. Midtraining / Two-phase training<a hidden class=anchor aria-hidden=true href=#24-midtraining--two-phase-training>#</a></h3><p><img alt=minicpm loading=lazy src=/cspaulia-blog/posts/sft_rlhf/minicpm.png></p><p>This recipe<a href=/cspaulia-blog/en/posts/sft_rlhf/#ref2>[2]</a> seems to be adopted by many LLM companies (though detailed docs are rare):</p><ul><li>In the Stable stage, train on a pure pretraining dataset (left in the figure).</li><li>In the Decay stage, train on a mixture of pretraining + instruction-tuning data (right in the figure).</li></ul><hr><h2 id=3-rlhf-reinforcement-learning-with-human-feedback>3. RLHF (Reinforcement Learning with Human Feedback)<a hidden class=anchor aria-hidden=true href=#3-rlhf-reinforcement-learning-with-human-feedback>#</a></h2><h3 id=31-from-imitation-to-optimization>3.1. From imitation to optimization<a hidden class=anchor aria-hidden=true href=#31-from-imitation-to-optimization>#</a></h3><p><strong>Imitation (SFT)</strong>: adjust the model’s output distribution to match a reference distribution $p^*(y|x)$, so that $\hat{p}(y|x) \approx p^*(y|x)$.</p><ul><li>From a pure generative modeling perspective, SFT teaches the model to imitate the reference distribution.</li><li>Training requires data from the reference policy (e.g., human-labeled datasets); otherwise, there is nothing to imitate.</li></ul><p><strong>Optimization (RLHF)</strong>: continuously adjust $\hat{p}(y|x)$ to maximize $\max\limits_{p} E_p[R(y,x)]$, where $R(y,x)$ is the reward.</p><ul><li>We are not optimizing an abstract “true distribution”, but a reward function we can define and measure (in RLHF, this reward comes from human feedback: rankings, pairwise preferences, or a trained reward model).</li><li>At this stage, we no longer view the LM as an approximation to $p^*(y|x)$ (as in SFT). Instead, we treat it as a <strong>policy</strong> to maximize the reward signal.</li></ul><hr><h3 id=32-why-rlhf-is-needed>3.2. Why RLHF is needed<a hidden class=anchor aria-hidden=true href=#32-why-rlhf-is-needed>#</a></h3><ol><li><strong>Cost</strong>: SFT is expensive, especially the annotation cost.</li><li><strong>G–V Gap (Generation–Value Gap)</strong>: what people write (generation distribution $G$) is not always aligned with what people actually prefer (from the viewpoint of a value model $V$).</li></ol><blockquote><p>A prior study<a href=/cspaulia-blog/en/posts/sft_rlhf/#ref3>[3]</a> found that when some annotators compared their own summaries with model-written summaries, they sometimes preferred the model’s summaries—suggesting human-written references are not always optimal.</p></blockquote><hr><h3 id=33-how-to-collect-rlhf-data>3.3. How to collect RLHF data<a hidden class=anchor aria-hidden=true href=#33-how-to-collect-rlhf-data>#</a></h3><p><strong>Option 1</strong>: (On the web) ask the model to produce $N$ outputs and let annotators (users) rank them.</p><ul><li>Potential issues:<ul><li>The labels may be <strong>low-quality</strong> or <strong>incorrect</strong>, or even generated by other LLMs—depending on the annotators.</li><li>The annotator population distribution can significantly shape model behavior.</li></ul><blockquote><p>For example, if many annotators are from Asia, the model may drift toward “Asian-style” outputs.</p></blockquote><ul><li>Different annotators care about different things.</li></ul><blockquote><p>Some focus on formatting; others focus on content.</p></blockquote></li></ul><p><strong>Option 2</strong>: use a large language model (e.g., GPT-4) to rank $N$ outputs (possibly from multiple models). This is often called AI Feedback.</p><hr><h3 id=34-methods-to-implement-rlhf>3.4. Methods to implement RLHF<a hidden class=anchor aria-hidden=true href=#34-methods-to-implement-rlhf>#</a></h3><h4 id=341-ppo-with-human-feedback>3.4.1. PPO with human feedback<a hidden class=anchor aria-hidden=true href=#341-ppo-with-human-feedback>#</a></h4><p>Vanilla PPO = policy gradient + off-policy correction. Its objective is:</p><p>$$
\max L^{CLIP}(\theta) - \beta KL[\pi_\theta||\pi_{old}]
$$</p><p>其中$L^{CLIP}(\theta) = \mathbb{E}_t \Big[ \min \big( r_t(\theta) \hat{A}_t,; \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t \big) \Big]$，</p><p>其中$r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}$，</p><p><img alt=rlhf loading=lazy src=/cspaulia-blog/posts/sft_rlhf/rlhf.png></p><p>人类反馈下的PPO的优化目标为：</p><p>$$
L^{RLHF}(\theta) = \mathbb{E}_{x \sim D, y \sim \pi} [ r(x,y) ] - \beta D_{KL} \left[ \pi(y|x) || \pi_{\text{ref}}(y|x) \right]
$$</p><p>Where:</p><ul><li>$r(x,y)$: the reward model’s score for output $y$ given input $x$.</li><li>$\pi(y\mid x)$: the current policy (the RLHF-trained model), i.e., the distribution over outputs $y$ given input $x$.</li><li>$\pi_{\text{ref}}(y\mid x)$: the reference policy, usually a frozen SFT model, used to prevent the policy from drifting too far.</li><li>$D_{KL} \left[ \pi(y|x) || \pi_{\text{ref}}(y|x) \right]$: the KL divergence between the current and reference policies.</li></ul><p><strong>Q1</strong>: Why does vanilla PPO optimize $\mathbb{E}_t[ r_t(\theta)\hat{A}<em>t ]$, while RLHF optimizes $\mathbb{E}</em>{x,y}[r(x,y)]$?</p><ul><li>In standard RL tasks, advantages $\hat{A}_t$ are often estimated from an <strong>offline</strong> behavior policy, so the objective takes the form $\max \mathbb{E}_t[ r_t(\theta)\hat{A}_t ]$.</li><li>In RLHF for LLMs, we use a reward model to directly score the <strong>on-policy</strong> outputs from the current LLM, so we do not need importance sampling correction via $r_t(\theta)$.</li></ul><p><strong>Q2</strong>: What is the difference between $\pi_{old}$ in vanilla PPO and $\pi_{\text{ref}}$ in RLHF PPO?</p><ul><li>$\pi_{old}$: the policy from the previous iteration; used for data collection and importance sampling; updated continuously.</li><li>$\pi_{\text{ref}}$: a fixed reference policy, typically the frozen SFT model before RLHF; it is not updated.</li></ul><hr><h4 id=342-dpo>3.4.2. DPO<a hidden class=anchor aria-hidden=true href=#342-dpo>#</a></h4><p><img alt=dpo loading=lazy src=/cspaulia-blog/posts/sft_rlhf/dpo.png></p><p>DPO uses the same high-level objective as PPO:</p><p>$$
\max L^{RLHF}(\theta) = \mathbb{E}_{x \sim D, y \sim \pi} [ r(x,y) ] - \beta D_{KL} \left[ \pi(y|x) || \pi_{\text{ref}}(y|x) \right]
$$</p><p>From this objective, the optimal policy is:</p><p>$$
\pi(y|x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))
$$</p><p>其中，$Z(x) = \sum\limits_y \pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))$</p><details><summary>Click to expand the derivation</summary><p>$$
\begin{align}
&\max\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ r(x,y) ] - \beta D_{KL} \left[ \pi(y|x) || \pi_{\text{ref}}(y|x) \right] \\
= &\max\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ r(x,y) ] - \mathbb{E}_{x \sim D, y \sim \pi} [ \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} ] \\
= &\max\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ r(x,y) - \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} ] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - r(x,y) ] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \log \exp(\frac{1}{\beta}r(x,y)) ] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))}] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))\frac{1}{Z(x)} Z(x)}] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \log \frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))} - \log Z(x)] \\
\end{align}
$$</p><p>where $Z(x) = \sum\limits_y \pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))$.</p><p>Let $\pi^\star(y|x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y)) = \frac{\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))}{\sum\limits_y \pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))}$. Then:</p><p>$$
\begin{align}
&\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \log \frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))} - \log Z(x)] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \log \frac{\pi(y|x)}{\pi^\star(y|x)} - \log Z(x)] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \log \frac{\pi(y|x)}{\pi^\star(y|x)}] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [D_{KL} (\pi(y|x) || \pi^\star(y|x))] \\
\end{align}
$$</p><p>The optimum is $\pi(y|x) = \pi^\star(y|x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))$.</p></details><p>Plugging the optimal policy into the maximum likelihood form of the Bradley–Terry model yields the DPO loss:</p><p>$$
Loss_{DPO} = - \ln \sigma \left( \beta \ln \frac{\pi(y^+ \mid x)}{\pi_{ref}(y^+ \mid x)} - \beta \ln \frac{\pi(y^- \mid x)}{\pi_{ref}(y^- \mid x)} \right)
$$</p><p>Here, $y^+$ is the preferred (higher-quality) sample and $y^-$ is the dispreferred (lower-quality) sample.</p><details><summary>Click to expand: Bradley–Terry model</summary><p><strong>Bradley–Terry model</strong>:</p><p>$$
P(i>j) = \frac{\alpha_i}{\alpha_i + \alpha_j}
$$</p><p>$P(i>j)$ is the probability that item $i$ beats item $j$. A common loss is</p><p>$$
Loss = -\mathbb{E}_{(\alpha_x, \alpha_y) \sim D} [ \ln \frac{\alpha_i}{\alpha_i + \alpha_j}]
$$</p><p><strong>Bradley–Terry model for LLM preferences</strong>:</p><p>$$
P(y_1>y_2) = \frac{r(x,y_1)}{r(x,y_1) + r(x,y_2)}
$$</p><p>where $x$ is the input prompt, $y$ is an output, and $r(x,y)$ is the reward score. To avoid negative values, we exponentiate:</p><p>$$
P(y_1>y_2) = \frac{\exp(r(x,y_1))}{\exp(r(x,y_1)) + \exp(r(x,y_2))}
$$</p><p>The loss is</p><p>$$
\begin{align}
\text{Loss} &= - \mathbb{E}_{(x, y^+, y^-) \sim D} [ \ln ( \frac{\exp(r(x, y^+))}{\exp(r(x, y^+)) + \exp(r(x, y^-))} ) ] \\
&= - \mathbb{E}_{(x, y^+, y^-) \sim D} [ \ln ( \frac{1}{1 + \exp(r(x, y^-)) - r(x, y^+)} ) ] \\
&= - \mathbb{E}_{(x, y^+, y^-) \sim D} [ \ln \sigma ( r(x, y^+) - r(x, y^-) ) ]
\end{align}
$$</p><p>where $\sigma(x) = \frac{1}{1+\exp(-x)}$ is the sigmoid.</p></details><details><summary>Click to expand the derivation</summary><p>$$
\begin{align}
&\pi(y \mid x) = \frac{1}{Z(x)} \pi_{ref}(y \mid x) \exp( \frac{1}{\beta} r(x, y)) \\
\Rightarrow &\exp( \frac{1}{\beta} r(x, y) ) = \frac{\pi(y \mid x)}{\pi_{ref}(y \mid x)} Z(x) \\
\Rightarrow &amp;r(x, y) = \beta \ln ( \frac{\pi(y \mid x)}{\pi_{ref}(y \mid x)} Z(x) ) \\
\Rightarrow &amp;r(x, y) = \beta \ln ( \frac{\pi(y \mid x)}{\pi_{ref}(y \mid x)} ) + \beta \ln Z(x) \\
\end{align}
$$</p><p>Substitute into the Bradley–Terry formulation:</p><p>$$
\begin{align}
Loss & = - \ln \sigma ( r(x, y^+) - r(x, y^-) ) \\
& = - \ln \sigma (\beta \ln ( \frac{\pi(y^+ \mid x)}{\pi_{ref}(y^+ \mid x)} ) + \beta \ln Z(x) - \beta \ln ( \frac{\pi(y^- \mid x)}{\pi_{ref}(y^- \mid x)} ) - \beta \ln Z(x)) \\
& = - \ln \sigma (\beta \ln ( \frac{\pi(y^+ \mid x)}{\pi_{ref}(y^+ \mid x)} ) - \beta \ln ( \frac{\pi(y^- \mid x)}{\pi_{ref}(y^- \mid x)} ))
\end{align}
$$</p></details><hr><div class=zhihu-ref><div class=zhihu-ref-title>References</div><ol><li id=ref1><a href=https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf target=_blank>Ouyang L, Wu J, Jiang X, et al. Training language models to follow instructions with human feedback[J]. Advances in neural information processing systems, 2022, 35: 27730-27744.</a></li><li id=ref2><a href=https://arxiv.org/pdf/2404.06395 target=_blank>Hu S, Tu Y, Han X, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies[J]. arXiv preprint arXiv:2404.06395, 2024.</a></li><li id=ref3><a href="https://watermark02.silverchair.com/tacl_a_00632.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA0kwggNFBgkqhkiG9w0BBwagggM2MIIDMgIBADCCAysGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM56gOdOIymzmdZk7SAgEQgIIC_MoTKqHYldlrb721Q5h2KffVo_kOA5sAdAk6Iko4Wu_cYFNYwzooRlN4Vmg64QkEP1rbgfQi1FVMWnNzCPbYCyhxpMlFcYX6bAPaLSTZiI3kUweSL5mPGPsGGycoEX1MAF5i4yKnT4pM7UKL5izeIaflqaWa-Rzo0cqhI66vUbfbp5WQILP2RQOA5qNoLUFbXtLx0TUgxuo-HaFfh3L3IEi1f3Loyj2-UJw4V7Dr7DYCf39XXfICpALlzjoTlJ_3s5YqMcbKn-9my4-DVh4EjU4QenszgsYc2GuHAXE23nUeFGfX93vMXESnP8mwRkVuKuaGodxiW-SZnO4jEQfAv8qH692lrJmkq9iT_WIvHE__hJQtIVr6cZgY9fXGrBzGmK6Hkz79PyMaAK5VglXB-dqB9JrQJQzXdjoIV4qmCKzVGOwYtT5qOOtoNPzHOTaBseERNSMMJw4Jq8t8S4-8GbFfmUcBdQrgs2HM5uYn2igRj_F-xMyRVe5y-jl8s_0Q8dFfliXCESTTI8p0NIqqz0uCY7iB64TemqFVEtR8M1FNbbJ82bhBAKg1zoJtG8hC3pIiQiXawsZzd9oFbx0GFwDdbvbYETEp5R3sJQ8wT1Ra2WhCPicvfSWgdC2EZRCWM4NvUZhsnpBfvZ4DlaE6xWma0u1x_rzrkfabjJcPnbC05EaF9dfx5pJ-X5uPnb1bDyIZQ5zyoueKtaPZzQVtP0mSH7qozQLRLR6UsdHH1RexlpnBOUyYtQtU50Xs0rySHvuWDYaeMICgOQwYGl8heOYlIUzexCgVvyuJz3c2HgwbUxQI3eAH05D74ApGpnaqKy1QNqQJKKXxIZ8QmpQYC4vnKBes20kXjILZYtSzqB6QZOHtfQNee_NTb3E3Q8cJANsOVBZeiIub1HggCbXtfvFmu1R9sTZDQCOrT_U-ABpM1ANqxk32Ty81QZjlj0A8vKh_wFqmoCff8IWI2kb0srnEyK0wt7gzw5fX1cbFfM7d53Y5T9eg_8t857NX" target=_blank>Zhang T, Ladhak F, Durmus E, et al. Benchmarking large language models for news summarization[J]. Transactions of the Association for Computational Linguistics, 2024, 12: 39-57.</a></li><li id=ref4><a href=https://github.com/stanford-cs336/spring2025-lectures/blob/61eddac004df975466cff0329b615f2d24230069/nonexecutable/2025%20Lecture%2015%20-%20RLHF%20Alignment.pdf target=_blank>stanford-cs336 lecture 15</a></li></ol></div></div><br><div><div class=copyright-card><div class=copyright-card-main><div class=copyright-info><h3 class=copyright-title>SFT and RLHF</h3><p class=copyright-link><a href=https://cspaulia.github.io/cspaulia-blog/en/posts/sft_rlhf/>https://cspaulia.github.io/cspaulia-blog/en/posts/sft_rlhf/</a></p><div class=copyright-meta><span><strong>Author</strong><br>CSPaulia</span>
<span><strong>Published at</strong><br>September 29, 2025</span>
<span><strong>Copyright</strong><br><a rel=license href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank>CC BY-NC-SA 4.0</a></span></div></div><div class=copyright-icon>©</div></div></div></div><footer class=post-footer><p style=font-size:medium;margin-bottom:5px;font-weight:700>categories</p><ul class=post-tags><li><a href=https://cspaulia.github.io/cspaulia-blog/en/categories/deep-learning-skills/>Deep Learning Skills</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/categories/large-language-model/>Large Language Model</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/categories/reinforcement-learning/>Reinforcement Learning</a></li></ul><p style=font-size:medium;margin-bottom:5px;font-weight:700>tags</p><ul class=post-tags><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/sft/>SFT</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/reinforcement-learning/>Reinforcement Learning</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/en/tags/rlhf/>RLHF</a></li></ul><nav class=paginav><a class=prev href=https://cspaulia.github.io/cspaulia-blog/en/posts/generation_with_sdes/><span class=title>« Prev</span><br><span>Generation Models with SDEs</span>
</a><a class=next href=https://cspaulia.github.io/cspaulia-blog/en/posts/primitives/><span class=title>Next »</span><br><span>Training Primitives and Resource Accounting</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share SFT and RLHF on x" href="https://x.com/intent/tweet/?text=SFT%20and%20RLHF&amp;url=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fen%2fposts%2fsft_rlhf%2f&amp;hashtags=SFT%2cReinforcementLearning%2cRLHF"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SFT and RLHF on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fen%2fposts%2fsft_rlhf%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SFT and RLHF on telegram" href="https://telegram.me/share/url?text=SFT%20and%20RLHF&amp;url=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fen%2fposts%2fsft_rlhf%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://cspaulia.github.io/cspaulia-blog/en/>cspaulia-blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>