---
title: "Tokenization"
date: 2025-07-17T10:20:03+08:00
# weight: 1
# aliases: ["/first"]
series:
    main: "Large Language Model"
    subseries: "Tokenization"
categories: ["Large Language Model", "NLP"]
tags: ["Tokenization", "LLM"]
author: "CSPaulia"
# author: ["Me", "You"] # multiple authors
showToc: true
TocOpen: true # show table of contents
draft: false
hidemeta: false
comments: false
description: "Tokenization in LLM"
# canonicalURL: "https://canonical.url/to/page"
disableShare: false
disableHLJS: false
hideSummary: true
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "tokenization_cover.jpg" # image path/url
    alt: "tokenization" # alt text
    caption: "tokenization" # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: true # only hide on current single page
    hiddenInList: false # hide on list pages and home
editPost:
    URL: "https://cspaulia.github.io/cspaulia-blog/content/"
    Text: "Suggest Changes" # edit text
    appendFilePath: true # to append file path to Edit link
---

ğŸ‘‰ åœ¨çº¿ä½“éªŒåœ°å€ï¼š[Tokenization å¯è§†åŒ–å·¥å…·](https://tiktokenizer.vercel.app)

---

åŸå§‹çš„æ–‡æœ¬ç»Ÿä¸€è¡¨å¾ä¸º Unicode å­—ç¬¦ä¸²

```python
string = "Hello, ğŸŒ! ä½ å¥½!"
```

è¯­è¨€æ¨¡å‹ä¼šå¯¹ä¸€ç³»åˆ—tokenï¼ˆé€šå¸¸ç”¨æ•´æ•°ç´¢å¼•è¡¨ç¤ºï¼‰ä¸Šçš„å¯èƒ½æ€§è¿›è¡Œå»ºæ¨¡ï¼Œæ„æˆä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ

```python
indices = [15496, 11, 995, 0]
```

æˆ‘ä»¬éœ€è¦ï¼š

- âœ… ä¸€ä¸ªæ–¹æ³•ï¼š**å°†å­—ç¬¦ä¸²ç¼–ç ä¸º token**
- âœ… ä¸€ä¸ªæ–¹æ³•ï¼š**å°† token è§£ç å›å­—ç¬¦ä¸²**

```python
class Tokenizer:
    def encode(self, string: str) -> list[int]:
        ...
    def decode(self, indices: list[int]) -> str:
        ...
```

- `vocab_size`: è¯è¡¨å¤§å°ï¼Œå³å¯èƒ½å‡ºç°çš„ tokenï¼ˆæ•´æ•° IDï¼‰æ€»æ•°ã€‚

---

## 1. Character-based tokenization

### 1.1. Unicode æ¦‚è¿°

- ç»Ÿä¸€å…¨çƒå­—ç¬¦ç¼–ç çš„æ ‡å‡†ï¼ˆçº¦ 150,000 ä¸ªå­—ç¬¦ï¼‰
- `ord(char)`ï¼šè·å–å­—ç¬¦çš„åè¿›åˆ¶ç¼–ç 

```python
ord("h")     # 104
ord("ğŸ˜Š")    # 128522
```

### 1.2. ç¼–è§£ç 

```python
class CharacterTokenizer(Tokenizer):
    """Represent a string as a sequence of Unicode code points."""
    def encode(self, string: str) -> list[int]:
        return list(map(ord, string))
    def decode(self, indices: list[int]) -> str:
        return "".join(map(chr, indices))
```

**ç¤ºä¾‹**ï¼š

```python
tokenizer = CharacterTokenizer()
string = "Hello, ğŸŒ! ä½ å¥½!"  # @inspect string
indices = tokenizer.encode(string)  # @inspect indices
reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string
```

**è¾“å‡º**ï¼š

```text
string = "Hello, ğŸŒ! ä½ å¥½!"
indices = [72, 101, 108, 108, 111, 44, 32, 127757, 33, 32, 20320, 22909, 33]
reconstructed_string = "Hello, ğŸŒ! ä½ å¥½!"
```

### 1.3. å­˜åœ¨çš„é—®é¢˜

- é—®é¢˜ä¸€ï¼šè¿™ä¼šæ˜¯ä¸€ä¸ªç›¸å½“å¤§çš„è¯æ±‡è¡¨ï¼ˆvocabularyï¼‰
- é—®é¢˜äºŒï¼šå¾ˆå¤šå­—ç¬¦å‡ºç°å‡ ç‡å¾ˆä½ï¼ˆä¾‹å¦‚ğŸŒï¼‰ï¼Œå¯¹è¯æ±‡è¡¨çš„ä½¿ç”¨å¹¶ä¸é«˜æ•ˆ

    ```python
    def get_compression_ratio(string: str, indices: list[int]) -> float:
        """Given `string` that has been tokenized into `indices`, ."""
        num_bytes = len(bytes(string, encoding="utf-8"))  # @inspect num_bytes
        num_tokens = len(indices)                       # @inspect num_tokens
        return num_bytes / num_tokens

    vocabulary_size = max(indices) + 1  # This is a lower bound @inspect vocabulary_size
    compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio
    ```

    **è¾“å‡º**ï¼š

    ```text
    vocabulary_size = 127758
    num_bytes = 20
    num_tokens = 13
    compression_ratio = 1.5384615384615385
    ```

---

## 2. Byte-based tokenization

- Unicode å­—ç¬¦ä¸²ï¼ˆStringï¼‰å¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸²å­—èŠ‚ï¼ˆByteï¼‰ï¼Œå…¶ä¸­å­—èŠ‚ï¼ˆå³å…«ä½äºŒè¿›åˆ¶ï¼‰å¯ä»¥è¡¨ç¤ºä¸º0åˆ°255çš„æ•°å­—
- æœ€å¸¸è§çš„ Unicode ç¼–ç æ˜¯ UTF-8

    **è¾“å…¥**ï¼š

    ```python
    bytes("a", encoding="utf-8")
    bytes("ğŸŒ", encoding="utf-8")
    ```

    **è¾“å‡º**ï¼š

    ```text
    b"a" # one byte
    b"\xf0\x9f\x8c\x8d"s # multiple bytes
    ```

### 2.1. ç¼–è§£ç 

```python
class ByteTokenizer(Tokenizer):
    """Represent a string as a sequence of bytes."""
    def encode(self, string: str) -> list[int]:
        string_bytes = string.encode("utf-8")  # @inspect string_bytes
        indices = list(map(int, string_bytes))  # @inspect indices
        return indices
    def decode(self, indices: list[int]) -> str:
        string_bytes = bytes(indices)  # @inspect string_bytes
        string = string_bytes.decode("utf-8")  # @inspect string
        return string
```

**ç¤ºä¾‹**ï¼š

```python
tokenizer = ByteTokenizer()
string = "Hello, ğŸŒ! ä½ å¥½!"  # @inspect string
indices = tokenizer.encode(string)  # @inspect indices
reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string
```

**è¾“å‡º**ï¼š

```text
string = "Hello, ğŸŒ! ä½ å¥½!"
string_bytes = "b'Hello, \\xf0\\x9f\\x8c\\x8d!\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd'"
indices = [72, 101, 108, 108, 111, 44, 32, 240, 159, 140, 141, 33, 32, 228, 189, 160, 229, 165, 189, 33]
reconstructed_string = "Hello, ğŸŒ! ä½ å¥½!"
```

### 2.2. å­˜åœ¨çš„é—®é¢˜

- é—®é¢˜ä¸€ï¼šè™½ç„¶è¯æ±‡è¡¨å¾ˆå°ï¼ˆä»…ä¸º256ï¼‰ï¼Œä½†è¿™ä¹Ÿå¯¼è‡´åºåˆ—å¾ˆé•¿ã€‚è€Œåœ¨ Transformer ä¸­ï¼Œè®¡ç®—å¤æ‚åº¦æ˜¯éšç€åºåˆ—é•¿åº¦**äºŒæ¬¡å¢é•¿**çš„

    ```python
    vocabulary_size = 256  # This is a lower bound @inspect vocabulary_size
    compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio
    ```

    **è¾“å‡º**ï¼š

    ```text
    num_bytes = 20
    num_tokens = 20
    compression_ratio = 1.0
    ```

---

## 3. Word-based tokenization

ä½¿ç”¨ç±»ä¼¼äºä¼ ç»ŸNLPåˆ†è¯æ–¹æ³•åˆ†ç¦»å­—ç¬¦ä¸²ï¼Œ**è¾“å…¥**ï¼š

```python
string = "I'll say supercalifragilisticexpialidocious!"
segments = regex.findall(r"\w+|.", string)  # @inspect segments
```

**è¾“å‡º**ï¼š

```text
segments = ["I", "ll", "say", "supercalifragilisticexpialidocious", "!"]
```

### 3.1. ç¼–è§£ç 

- è¦å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ª`tokenizer`ï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™äº›ç‰‡æ®µæ˜ å°„ä¸ºæ•´æ•°
- æ„å»ºä¸€ä¸ªä»æ¯ä¸ªç‰‡æ®µåˆ°æ•´æ•°çš„æ˜ å°„

### 3.2. å­˜åœ¨çš„é—®é¢˜

- è¯çš„æ•°é‡æ˜¯éå¸¸åºå¤§çš„
- å¾ˆå¤šè¯å¾ˆå°‘å‡ºç°ï¼Œæ¨¡å‹ä¸ä¼šä»è¿™äº›è¯ä¸­å­¦ä¹ åˆ°å¾ˆå¤šå†…å®¹
- å®ƒæ— æ³•æä¾›ä¸€ä¸ªå›ºå®šé•¿åº¦çš„è¯å…¸

---

## 4. Byte Pair Encodingï¼ˆBPEï¼‰

**ä¸»è¦æ€æƒ³**ï¼šåœ¨åŸå§‹æ–‡æœ¬ä¸Šè®­ç»ƒ`tokenizer`ï¼Œè‡ªå‘çš„ç”Ÿæˆè¯æ±‡è¡¨

**æ„å›¾**ï¼šå¯¹äºå¸¸è§çš„å­—ç¬¦åºåˆ—ï¼Œå¯ä»¥ä»…ç”¨ä¸€ä¸ªtokenè¡¨ç¤ºï¼›å¯¹äºä¸å¸¸è§çš„å­—ç¬¦åºåˆ—ï¼Œåˆ™ç”¨å¤šä¸ªtokenè¡¨ç¤º

**ç®—æ³•ç®€è¿°**ï¼šé¦–å…ˆå°†æ¯ä¸€ä¸ª**byte**çœ‹ä½œæ˜¯ä¸€ä¸ªtokenï¼Œéšåé€æ¸åˆå¹¶å¸¸å‡ºç°çš„ç›¸é‚»tokenä¸ºä¸€ä¸ªæ–°token

### 4.1. ç®—æ³•æµç¨‹

```python
def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:  # @inspect indices, @inspect pair, @inspect new_index
    """Return `indices`, but with all instances of `pair` replaced with `new_index`."""
    new_indices = []  # @inspect new_indices
    i = 0  # @inspect i
    while i < len(indices):
        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:
            new_indices.append(new_index)
            i += 2
        else:
            new_indices.append(indices[i])
            i += 1
    return new_indices
```

### 4.2. BPE ç¼–ç å™¨

```python
class BPETokenizer(Tokenizer):
    """BPE tokenizer given a set of merges and a vocabulary."""
    def __init__(self, params: BPETokenizerParams):
        self.params = params
    def encode(self, string: str) -> list[int]:
        indices = list(map(int, string.encode("utf-8")))  # @inspect indices
        # Note: this is a very slow implementation
        for pair, new_index in self.params.merges.items():  # @inspect pair, @inspect new_index
            indices = merge(indices, pair, new_index)
        return indices
    def decode(self, indices: list[int]) -> str:
        bytes_list = list(map(self.params.vocab.get, indices))  # @inspect bytes_list
        string = b"".join(bytes_list).decode("utf-8")  # @inspect string
        return string
```

### 4.3. è®­ç»ƒ BPE

```python
def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:  # @inspect string, @inspect num_merges
    # Start with the list of bytes of string.
    indices = list(map(int, string.encode("utf-8")))  # @inspect indices
    merges: dict[tuple[int, int], int] = {}  # index1, index2 => merged index
    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -> bytes

    for i in range(num_merges):
        # Count the number of occurrences of each pair of tokens
        counts = defaultdict(int)
        for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair
            counts[(index1, index2)] += 1  # @inspect counts

        # Find the most common pair.
        pair = max(counts, key=counts.get)  # @inspect pair
        index1, index2 = pair

        # Merge that pair.
        new_index = 256 + i  # @inspect new_index
        merges[pair] = new_index  # @inspect merges
        vocab[new_index] = vocab[index1] + vocab[index2]  # @inspect vocab
        indices = merge(indices, pair, new_index)  # @inspect indices
    
    return BPETokenizerParams(vocab=vocab, merges=merges)
```

**ç¤ºä¾‹**:

```python
text = "the cat in the hat"  # @inspect string
params = train_bpe(text, num_merges=3)

string = "the quick brown fox"  # @inspect string
tokenizer = BPETokenizer(params)
indices = tokenizer.encode(string)  # @inspect indices
reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string
```

ä»£ç é€»è¾‘ä¸ºï¼š

1. åˆå§‹åŒ–è¯æ±‡è¡¨ï¼Œç”¨0-255è¡¨å¾byte
2. ä¾æ¬¡å¯»æ‰¾æœ€å¤šæ¬¡å‡ºç°çš„ç›¸é‚»byte
   - (116, 104) --> 256 å³ ('t', 'h') --> 'th'
   - (256, 101) --> 257 å³ ('th', 'e') --> 'the'
   - (257, 32) --> 258 å³ ï¼ˆ'the', ' 'ï¼‰--> 'the '
3. è¯æ±‡è¡¨é•¿åº¦æ›´æ–°è‡³259
4. ç”¨æ–°è¯æ±‡è¡¨å¯¹å­—ç¬¦ä¸²è¿›è¡Œç¼–ç 

---

<div class="zhihu-ref">
  <div class="zhihu-ref-title">å‚è€ƒæ–‡çŒ®</div>
  <ol>
    <li><a href="https://stanford-cs336.github.io/spring2025-lectures/?trace=var/traces/lecture_01.json" target="_blank">stanford-cs336 lecture 1</a></li>
  </ol>
</div>