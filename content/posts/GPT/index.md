---
title: "GPTç³»åˆ—"
date: 2025-06-18T20:00:00+08:00
series:
    main: "Large Language Model"
    subseries: "Mainstream Series"
categories: ["Large Language Model"]
tags: ["GPT", "Pre-training", "LLM"]
author: "CSPaulia"
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: "è¯¦è§£ GPTç³»åˆ— é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹"
UseHugoToc: true
cover:
    image: "gpt-cover.png" 
    alt: "GPT Architecture" 
    caption: "GPT Architecture" 
    relative: false
    hidden: true
---

## GPT-1

### å‡ºå‘ç‚¹

ç›®å‰å°šä¸æ¸…æ¥šå“ªç§ç±»å‹çš„ä¼˜åŒ–ç›®æ ‡æœ€æœ‰æ•ˆåœ°å­¦ä¹ å¯¹è¿ç§»æœ‰æ•ˆçš„æ–‡æœ¬è¡¨ç¤ºï¼ˆä¸ªäººç†è§£å¯¹äºä¸åŒçš„NLPä»»åŠ¡ï¼Œä¸çŸ¥é“å“ªç§ä¼˜åŒ–ç›®æ ‡æ˜¯æœ€å¥½çš„ï¼‰

### æ–¹æ³•

- åŠç›‘ç£çš„æ–¹æ³•
- Transformerï¼ˆç”¨äºå¤„ç†é•¿æœŸä¾èµ–æ€§çš„æ›´å¤šç»“æ„åŒ–è®°å¿†ï¼‰--> å¼ºå¤§çš„è¿ç§»æ€§èƒ½ğŸ”¤

#### 1. éç›‘ç£é¢„è®­ç»ƒï¼ˆUnsupervised pre-trainingï¼‰

- **ä¼˜åŒ–ç›®æ ‡ï¼š**

    ç»™å®šä¸€ç»„ unsupervised corpus of tokens $U = \\{u_1, \cdots , u_n\\}$

    $$
    L_1(U) = \sum_i \log{P(u_i | u_{i-k}, \cdots, u_{i-1}; \Theta)}, i \in \{1,\cdots, n\}
    $$

    - kæ˜¯ä¸Šä¸‹æ–‡çª—å£çš„å¤§å°ï¼Œä½¿ç”¨å…·æœ‰å‚æ•°$Î¸$çš„ç¥ç»ç½‘ç»œå¯¹æ¡ä»¶æ¦‚ç‡$P$è¿›è¡Œå»ºæ¨¡ï¼›
    - è¿™é‡Œçš„ $P(u_i | u_{i-k}, \cdots, u_{i-1}; \Theta)$ æŒ‡çš„æ˜¯å·²çŸ¥æ¨¡å‹å‚æ•°$Î¸$ä¸å‰ n ä¸ªtokençš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹å‡ºç¬¬ i ä¸ªtokençš„æ¦‚ç‡

    ç”±äºGPTä½¿ç”¨çš„æ˜¯éç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ï¼Œåœ¨ç»™å®šä¸€æ®µæ–‡æœ¬ä¸­çš„ k ä¸ªtokenæ—¶ï¼Œå°±æ˜¯è¦è®©æ¨¡å‹é¡ºåˆ©çš„é¢„æµ‹å‡ºç¬¬ i ä¸ªtokenã€‚å› æ­¤å°†æ¯ä¸ªtokençš„é¢„æµ‹æ¦‚ç‡ $P(u_i | u_{i-k}, \cdots, u_{i-1}; \Theta)$ æ±‚å’Œå¹¶æœ€å¤§åŒ–ï¼Œå°±æ˜¯è¯¥æ¨¡å‹çš„ä¼˜åŒ–ç›®æ ‡ï¼Œè¯¥ç›®æ ‡é€‚ç”¨äºä»»ä½•ä»»åŠ¡ã€‚ï¼ˆè§£å†³å‡ºå‘ç‚¹é—®é¢˜ï¼‰

- **æ¨¡å‹æ¶æ„ï¼š**

    multi-layer Transformer decoder

    $$
    \begin{aligned}
    h_0 &= UW_e + W_p \\\\
    h_i &= \text{transformer block}(h_{i-1}) \\\\
    P(u) &= \text{softmax}(h_n W_e^T)
    \end{aligned}
    $$

    - We is the token embedding matrix

    - Wp is the position embedding matrix

####  2. åŸºäºç›‘ç£çš„å¾®è°ƒï¼ˆSupervised fine-tuningï¼‰

å‡è®¾æœ‰ä¸€æ ‡æ³¨è¿‡çš„æ•°æ®é›†ï¼Œå…¶åŒ…å«ï¼š

- a sequence of input tokens, $x1, \cdots , xm$
- label $y$

è·å¾—æœ€åä¸€å±‚Transformerå—çš„æ¿€æ´»å±‚è¾“å‡º$h^m_l$

$$
P(y|x^1, \dots, x^m) = \text{softmax}(h_l^m W_y)
$$

$$
L_2(\hat{C}) = \sum_{(x, y)} \log P(y|x^1, \dots, x^m)
$$

å’Œç›®æ ‡å‡½æ•° L1 æ„é€ ç±»ä¼¼ï¼Œä¸è¿‡æ˜¯ä»¤é¢„æµ‹æ ‡ç­¾æ¦‚ç‡æœ€å¤§

$$
L_3(\hat{C}) = L_2(\hat{C}) + \lambda * L_1(\hat{C})
$$

å¾®è°ƒä»»åŠ¡ä¸­çš„ä¼˜åŒ–ç›®æ ‡å‡½æ•°ç”±L1å’ŒL2ç»„æˆã€‚

#### 3. ä¸åŒä»»åŠ¡çš„è¾“å…¥æ„é€ ï¼ˆTask-specific input transformationsï¼‰

<p align="center">
  <img src="gpt-tasks.png" alt="gpt-tasks" />
</p>

ç®€å•è®²è®²ç›¸ä¼¼åº¦ä»»åŠ¡ã€‚ç”±äºGPTæ˜¯å•å‘çš„æ¨¡å‹ï¼ˆTransformeræ˜¯ä¸€ä¸ªè¯ä¸€ä¸ªè¯çš„ç”Ÿæˆçš„ï¼‰ï¼Œæ‰€ä»¥åœ¨å¤„ç†ç›¸ä¼¼åº¦ä»»åŠ¡æ—¶ï¼ŒText 1 å’Œ Text 2 çš„å…ˆåé¡ºåºå¾ˆé‡è¦ï¼Œå¯ä»¥æŒ‰ç…§ä¸åŒçš„æ’åˆ—é¡ºåºæ’æ”¾ï¼Œåˆ©ç”¨GPTè®¡ç®—ç›¸ä¼¼åº¦å–å¹³å‡ç›¸ä¼¼åº¦ã€‚

## GPT-2

### å‡ºå‘ç‚¹

åˆ›å»ºMachine Learningç³»ç»Ÿçš„ä¸»è¦æ–¹æ³•æ˜¯æ”¶é›†ä¸€ä¸ªç”¨äºè®­ç»ƒçš„æ•°æ®é›†ï¼Œåœ¨æŸä¸€ç‰¹å®šé¢†åŸŸä½¿ç”¨æŸä¸€ç‰¹å®šæ•°æ®é›†æ˜¯å¯¼è‡´æ¨¡å‹ç¼ºä¹æ³›åŒ–æ€§èƒ½çš„ä¸»è¦åŸå› ã€‚

### æ–¹æ³•

- Multitask learning å¤šä»»åŠ¡å­¦ä¹  --> è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨zero-shotè®¾ç½®ä¸­æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡ï¼Œåœ¨æ²¡æœ‰ä»»ä½•å‚æ•°æˆ–æ¶æ„ä¿®æ”¹çš„æƒ…å†µä¸‹
- pre-training + supervised finetuning
- æ¨¡å‹çš„ä¼˜åŒ–ç›®æ ‡ä¸º p(output|input, task)ï¼Œå…·ä½“å¯ä»¥æè¿°ä¸º {task(è§†ä½œprompt), input, output}ï¼š
    
    - ä¾‹1ï¼ša translation training example can be written as the sequence (translate to french, english text, french text)
    - ä¾‹2ï¼šreading comprehension training example can be written as (answer the question, document, question, answer)

#### è®­ç»ƒæ•°æ®

- Redditç½‘ç«™ä¸Šè‡³å°‘åŒ…å« 3 karmaçš„æ–‡ç« ï¼Œçˆ¬å–äº†4500ä¸‡ä¸ªé“¾æ¥ï¼Œæœ€ç»ˆè·å¾—800ä¸‡ä¸ªæ–‡ä»¶ï¼ŒåŒ…å«40GBçš„æ–‡æœ¬å†…å®¹

#### æ¨¡å‹

ä¸GPTå¤§è‡´ä¸€è‡´

## GPT-3

### å‡ºå‘ç‚¹

å¤§å¤šæ•°è¯­è¨€æ¨¡å‹åœ¨ä»»åŠ¡ä¸å¯çŸ¥çš„æƒ…å†µä¸‹ï¼Œä»ç„¶éœ€è¦ç‰¹å®šäºä»»åŠ¡çš„æ•°æ®é›†å’Œç‰¹å®šäºä»»åŠ¡çš„å¾®è°ƒ

- éœ€è¦é’ˆå¯¹ä»»åŠ¡çš„ã€åŒ…å«æ ‡æ³¨å®ä¾‹çš„å¤§æ•°æ®é›†
- åœ¨å¾®è°ƒæ•°æ®é›†ä¸Šçš„æ•ˆæœå¥½å¹¶ä¸ä»£è¡¨æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½è‰¯å¥½

### æ–¹æ³•

meta-learningï¼šè®­ç»ƒä¸€ä¸ªæ³›åŒ–æ€§ä¸é”™çš„æ¨¡å‹

in-context learningï¼šåœ¨åç»­è¿‡ç¨‹ä¸­ï¼Œå³ä½¿å·²çŸ¥ä¸€äº›è®­ç»ƒæ ·æœ¬ï¼Œä¹Ÿä¸æ›´æ–°æ¨¡å‹æƒé‡ï¼ˆä¸ªäººç†è§£å°±æ˜¯åœ¨æé—®è¿‡ç¨‹ä¸­åŒ…å«ä¸€äº›è®­ç»ƒæ ·æœ¬ï¼‰ï¼š

- zero-shot
- one-shot
- few-shot

<p align="center">
  <img src="gpt-3-tasks.png" alt="gpt-3-tasks" />
</p>

#### æ¨¡å‹åŠå…¶æ¶æ„

- ä½¿ç”¨ä¸GPT-2ç›¸åŒçš„æ¨¡å‹å’Œæ¶â€‹â€‹æ„
- Sparse Transformer
- 8ç§ä¸åŒå°ºå¯¸

<p align="center">
  <img src="gpt-3-models.png" alt="gpt-3-models" />
</p>

---

<div class="zhihu-ref">
  <div class="zhihu-ref-title">å‚è€ƒæ–‡çŒ®</div>
  <ol>
    <li><a href="https://www.bilibili.com/video/BV1AF411b7xQ?spm_id_from=333.788.videopod.sections&vd_source=9e4f1724ef60547fa31e3c8270245ff8" target="_blank">GPTï¼ŒGPT-2ï¼ŒGPT-3 è®ºæ–‡ç²¾è¯»ã€è®ºæ–‡ç²¾è¯»ã€‘</a></li>
    <li><a href="https://www.mikecaptain.com/resources/pdf/GPT-1.pdf" target="_blank">Improving language understanding by generative pre-training</a></li>
    <li><a href="https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf" target="_blank">Language models are unsupervised multitask learners</a></li>
    <li><a href="https://www.mikecaptain.com/resources/pdf/GPT-3.pdf" target="_blank">Language Models are Few-Shot Learners</a></li>
  </ol>
</div>