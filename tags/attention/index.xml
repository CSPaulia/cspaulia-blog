<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Attention on cspaulia-blog</title>
    <link>https://cspaulia.github.io/cspaulia-blog/tags/attention/</link>
    <description>Recent content in Attention on cspaulia-blog</description>
    <image>
      <title>cspaulia-blog</title>
      <url>https://cspaulia.github.io/cspaulia-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://cspaulia.github.io/cspaulia-blog/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.148.1</generator>
    <language>en</language>
    <lastBuildDate>Wed, 18 Jun 2025 20:47:05 +0800</lastBuildDate>
    <atom:link href="https://cspaulia.github.io/cspaulia-blog/tags/attention/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>交叉注意力机制</title>
      <link>https://cspaulia.github.io/cspaulia-blog/posts/cross_attention/</link>
      <pubDate>Wed, 21 May 2025 22:04:00 +0800</pubDate>
      <guid>https://cspaulia.github.io/cspaulia-blog/posts/cross_attention/</guid>
      <description>&lt;h2 id=&#34;cross-attention&#34;&gt;Cross Attention&lt;/h2&gt;
&lt;p&gt;来自&lt;a href=&#34;https://vaclavkosar.com/ml/cross-attention-in-transformer-architecture&#34;&gt;博客&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;简介&#34;&gt;简介&lt;/h3&gt;
&lt;p&gt;Cross Attention是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;融合两种不同的嵌入序列的注意力机制&lt;/li&gt;
&lt;li&gt;两个序列必须包含&lt;strong&gt;相同的维度&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;两个序列可以来自不同的模态（例如文本、图像、声音）&lt;/li&gt;
&lt;li&gt;其中一个序列作为&lt;strong&gt;Query的输入&lt;/strong&gt;，决定了输出的长度&lt;/li&gt;
&lt;li&gt;另一个序列作为&lt;strong&gt;Key和Value的输入&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cross-attention-vs-self-attention&#34;&gt;Cross Attention vs Self-attention&lt;/h3&gt;
&lt;p&gt;Cross Attention与Self-attention只有输入不同。Cross Attention输入为两个维度相同的嵌入序列；Self-attention输入为一个嵌入序列，其KQV均由该序列生成。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;cross attention&#34; loading=&#34;lazy&#34; src=&#34;https://cspaulia.github.io/cspaulia-blog/posts/cross_attention/cross_attention.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;cross-attention算法&#34;&gt;Cross Attention算法&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;拥有两个序列S1、S2&lt;/li&gt;
&lt;li&gt;计算S1的K、V&lt;/li&gt;
&lt;li&gt;计算S2的Q&lt;/li&gt;
&lt;li&gt;根据K和Q计算注意力矩阵&lt;/li&gt;
&lt;li&gt;将V应用于注意力矩阵&lt;/li&gt;
&lt;li&gt;输出的序列长度与S2一致&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\pmb{\text{softmax}}((W_Q S_2)(W_K S_1)^\mathrm{T})W_v S_1
$$&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
