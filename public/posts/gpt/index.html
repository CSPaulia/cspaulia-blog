<!doctype html><html lang=en dir=auto><head><script src="/cspaulia-blog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=cspaulia-blog/livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GPT系列 | cspaulia-blog</title>
<meta name=keywords content="GPT,Pre-training,LLM"><meta name=description content="详解 GPT系列 预训练语言模型"><meta name=author content="CSPaulia"><link rel=canonical href=http://localhost:1313/cspaulia-blog/posts/gpt/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/cspaulia-blog/assets/css/stylesheet.f663e09d6ac46b213461c85d30de0047e58a639fa07ab9cf1761e4fb281dd19a.css integrity="sha256-9mPgnWrEayE0YchdMN4AR+WKY5+gernPF2Hk+ygd0Zo=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/cspaulia-blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/cspaulia-blog/posts/gpt/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1,trust:!0,strict:!1})})</script><meta property="og:url" content="http://localhost:1313/cspaulia-blog/posts/gpt/"><meta property="og:site_name" content="cspaulia-blog"><meta property="og:title" content="GPT系列"><meta property="og:description" content="详解 GPT系列 预训练语言模型"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-18T20:00:00+08:00"><meta property="article:modified_time" content="2025-06-18T20:47:05+08:00"><meta property="article:tag" content="GPT"><meta property="article:tag" content="Pre-Training"><meta property="article:tag" content="LLM"><meta property="og:image" content="http://localhost:1313/cspaulia-blog/gpt-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/cspaulia-blog/gpt-cover.png"><meta name=twitter:title content="GPT系列"><meta name=twitter:description content="详解 GPT系列 预训练语言模型"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/cspaulia-blog/posts/"},{"@type":"ListItem","position":2,"name":"GPT系列","item":"http://localhost:1313/cspaulia-blog/posts/gpt/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GPT系列","name":"GPT系列","description":"详解 GPT系列 预训练语言模型","keywords":["GPT","Pre-training","LLM"],"articleBody":"GPT-1 出发点 目前尚不清楚哪种类型的优化目标最有效地学习对迁移有效的文本表示（个人理解对于不同的NLP任务，不知道哪种优化目标是最好的）\n方法 半监督的方法 Transformer（用于处理长期依赖性的更多结构化记忆）–\u003e 强大的迁移性能🔤 1. 非监督预训练（Unsupervised pre-training） 优化目标：\n给定一组 unsupervised corpus of tokens $U = \\{u_1, \\cdots , u_n\\}$\n$$ L_1(U) = \\sum_i \\log{P(u_i | u_{i-k}, \\cdots, u_{i-1}; \\Theta)}, i \\in {1,\\cdots, n} $$\nk是上下文窗口的大小，使用具有参数$θ$的神经网络对条件概率$P$进行建模； 这里的 $P(u_i | u_{i-k}, \\cdots, u_{i-1}; \\Theta)$ 指的是已知模型参数$θ$与前 n 个token的情况下，预测出第 i 个token的概率 由于GPT使用的是非监督预训练方法，在给定一段文本中的 k 个token时，就是要让模型顺利的预测出第 i 个token。因此将每个token的预测概率 $P(u_i | u_{i-k}, \\cdots, u_{i-1}; \\Theta)$ 求和并最大化，就是该模型的优化目标，该目标适用于任何任务。（解决出发点问题）\n模型架构：\nmulti-layer Transformer decoder\n$$ \\begin{aligned} h_0 \u0026= UW_e + W_p \\\\ h_i \u0026= \\text{transformer block}(h_{i-1}) \\\\ P(u) \u0026= \\text{softmax}(h_n W_e^T) \\end{aligned} $$\nWe is the token embedding matrix\nWp is the position embedding matrix\n2. 基于监督的微调（Supervised fine-tuning） 假设有一标注过的数据集，其包含：\na sequence of input tokens, $x1, \\cdots , xm$ label $y$ 获得最后一层Transformer块的激活层输出$h^m_l$\n$$ P(y|x^1, \\dots, x^m) = \\text{softmax}(h_l^m W_y) $$\n$$ L_2(\\hat{C}) = \\sum_{(x, y)} \\log P(y|x^1, \\dots, x^m) $$\n和目标函数 L1 构造类似，不过是令预测标签概率最大\n$$ L_3(\\hat{C}) = L_2(\\hat{C}) + \\lambda * L_1(\\hat{C}) $$\n微调任务中的优化目标函数由L1和L2组成。\n3. 不同任务的输入构造（Task-specific input transformations） 简单讲讲相似度任务。由于GPT是单向的模型（Transformer是一个词一个词的生成的），所以在处理相似度任务时，Text 1 和 Text 2 的先后顺序很重要，可以按照不同的排列顺序排放，利用GPT计算相似度取平均相似度。\nGPT-2 出发点 创建Machine Learning系统的主要方法是收集一个用于训练的数据集，在某一特定领域使用某一特定数据集是导致模型缺乏泛化性能的主要原因。\n方法 Multitask learning 多任务学习 –\u003e 语言模型可以在zero-shot设置中执行下游任务，在没有任何参数或架构修改的情况下\npre-training + supervised finetuning\n模型的优化目标为 p(output|input, task)，具体可以描述为 {task(视作prompt), input, output}：\n例1：a translation training example can be written as the sequence (translate to french, english text, french text) 例2：reading comprehension training example can be written as (answer the question, document, question, answer) 训练数据 Reddit网站上至少包含 3 karma的文章，爬取了4500万个链接，最终获得800万个文件，包含40GB的文本内容 模型 与GPT大致一致\nGPT-3 出发点 大多数语言模型在任务不可知的情况下，仍然需要特定于任务的数据集和特定于任务的微调\n需要针对任务的、包含标注实例的大数据集 在微调数据集上的效果好并不代表模型的泛化性能良好 方法 meta-learning：训练一个泛化性不错的模型\nin-context learning：在后续过程中，即使已知一些训练样本，也不更新模型权重（个人理解就是在提问过程中包含一些训练样本）：\nzero-shot one-shot few-shot 模型及其架构 使用与GPT-2相同的模型和架​​构 Sparse Transformer 8种不同尺寸 参考文献 GPT，GPT-2，GPT-3 论文精读【论文精读】 Improving language understanding by generative pre-training Language models are unsupervised multitask learners Language Models are Few-Shot Learners ","wordCount":"248","inLanguage":"en","image":"http://localhost:1313/cspaulia-blog/gpt-cover.png","datePublished":"2025-06-18T20:00:00+08:00","dateModified":"2025-06-18T20:47:05+08:00","author":{"@type":"Person","name":"CSPaulia"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/cspaulia-blog/posts/gpt/"},"publisher":{"@type":"Organization","name":"cspaulia-blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/cspaulia-blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/cspaulia-blog/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/cspaulia-blog/posts/ title=posts><span>posts</span></a></li><li><a href=http://localhost:1313/cspaulia-blog/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/cspaulia-blog/series/ title=series><span>series</span></a></li><li><a href=http://localhost:1313/cspaulia-blog/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/cspaulia-blog/archives/ title=archives><span>archives</span></a></li><li><a href=http://localhost:1313/cspaulia-blog/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/cspaulia-blog/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/cspaulia-blog/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">GPT系列</h1><div class=post-description>详解 GPT系列 预训练语言模型</div><div class=post-meta><span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg> June 18, 2025</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 19l7-7 3 3-7 7-3-3z"/><path d="M18 13l-1.5-7.5L2 2l3.5 14.5L13 18l5-5z"/><path d="M2 2l7.586 7.586"/><circle cx="11" cy="11" r="2"/></svg> June 18, 2025</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><path d="M14 2v6h6"/><line x1="8" y1="13" x2="16" y2="13"/><line x1="8" y1="17" x2="16" y2="17"/><line x1="8" y1="9" x2="12" y2="9"/></svg> 248 words</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg> 2 min</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg> CSPaulia</span>
&nbsp;|&nbsp;<a href=https://cspaulia.github.io/cspaulia-blog/content//posts/GPT/index.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#gpt-1 aria-label=GPT-1>GPT-1</a><ul><li><a href=#%e5%87%ba%e5%8f%91%e7%82%b9 aria-label=出发点>出发点</a></li><li><a href=#%e6%96%b9%e6%b3%95 aria-label=方法>方法</a></li></ul></li><li><a href=#gpt-2 aria-label=GPT-2>GPT-2</a><ul><li><a href=#%e5%87%ba%e5%8f%91%e7%82%b9-1 aria-label=出发点>出发点</a></li><li><a href=#%e6%96%b9%e6%b3%95-1 aria-label=方法>方法</a></li></ul></li><li><a href=#gpt-3 aria-label=GPT-3>GPT-3</a><ul><li><a href=#%e5%87%ba%e5%8f%91%e7%82%b9-2 aria-label=出发点>出发点</a></li><li><a href=#%e6%96%b9%e6%b3%95-2 aria-label=方法>方法</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h2[id],h3[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=gpt-1>GPT-1<a hidden class=anchor aria-hidden=true href=#gpt-1>#</a></h2><h3 id=出发点>出发点<a hidden class=anchor aria-hidden=true href=#出发点>#</a></h3><p>目前尚不清楚哪种类型的优化目标最有效地学习对迁移有效的文本表示（个人理解对于不同的NLP任务，不知道哪种优化目标是最好的）</p><h3 id=方法>方法<a hidden class=anchor aria-hidden=true href=#方法>#</a></h3><ul><li>半监督的方法</li><li>Transformer（用于处理长期依赖性的更多结构化记忆）&ndash;> 强大的迁移性能🔤</li></ul><h4 id=1-非监督预训练unsupervised-pre-training>1. 非监督预训练（Unsupervised pre-training）<a hidden class=anchor aria-hidden=true href=#1-非监督预训练unsupervised-pre-training>#</a></h4><ul><li><p><strong>优化目标：</strong></p><p>给定一组 unsupervised corpus of tokens $U = \{u_1, \cdots , u_n\}$</p><p>$$
L_1(U) = \sum_i \log{P(u_i | u_{i-k}, \cdots, u_{i-1}; \Theta)}, i \in {1,\cdots, n}
$$</p><ul><li>k是上下文窗口的大小，使用具有参数$θ$的神经网络对条件概率$P$进行建模；</li><li>这里的 $P(u_i | u_{i-k}, \cdots, u_{i-1}; \Theta)$ 指的是已知模型参数$θ$与前 n 个token的情况下，预测出第 i 个token的概率</li></ul><p>由于GPT使用的是非监督预训练方法，在给定一段文本中的 k 个token时，就是要让模型顺利的预测出第 i 个token。因此将每个token的预测概率 $P(u_i | u_{i-k}, \cdots, u_{i-1}; \Theta)$ 求和并最大化，就是该模型的优化目标，该目标适用于任何任务。（解决出发点问题）</p></li><li><p><strong>模型架构：</strong></p><p>multi-layer Transformer decoder</p><p>$$
\begin{aligned}
h_0 &= UW_e + W_p \\
h_i &= \text{transformer block}(h_{i-1}) \\
P(u) &= \text{softmax}(h_n W_e^T)
\end{aligned}
$$</p><ul><li><p>We is the token embedding matrix</p></li><li><p>Wp is the position embedding matrix</p></li></ul></li></ul><h4 id=2-基于监督的微调supervised-fine-tuning>2. 基于监督的微调（Supervised fine-tuning）<a hidden class=anchor aria-hidden=true href=#2-基于监督的微调supervised-fine-tuning>#</a></h4><p>假设有一标注过的数据集，其包含：</p><ul><li>a sequence of input tokens, $x1, \cdots , xm$</li><li>label $y$</li></ul><p>获得最后一层Transformer块的激活层输出$h^m_l$</p><p>$$
P(y|x^1, \dots, x^m) = \text{softmax}(h_l^m W_y)
$$</p><p>$$
L_2(\hat{C}) = \sum_{(x, y)} \log P(y|x^1, \dots, x^m)
$$</p><p>和目标函数 L1 构造类似，不过是令预测标签概率最大</p><p>$$
L_3(\hat{C}) = L_2(\hat{C}) + \lambda * L_1(\hat{C})
$$</p><p>微调任务中的优化目标函数由L1和L2组成。</p><h4 id=3-不同任务的输入构造task-specific-input-transformations>3. 不同任务的输入构造（Task-specific input transformations）<a hidden class=anchor aria-hidden=true href=#3-不同任务的输入构造task-specific-input-transformations>#</a></h4><p align=center><img src=gpt-tasks.png alt=gpt-tasks></p><p>简单讲讲相似度任务。由于GPT是单向的模型（Transformer是一个词一个词的生成的），所以在处理相似度任务时，Text 1 和 Text 2 的先后顺序很重要，可以按照不同的排列顺序排放，利用GPT计算相似度取平均相似度。</p><h2 id=gpt-2>GPT-2<a hidden class=anchor aria-hidden=true href=#gpt-2>#</a></h2><h3 id=出发点-1>出发点<a hidden class=anchor aria-hidden=true href=#出发点-1>#</a></h3><p>创建Machine Learning系统的主要方法是收集一个用于训练的数据集，在某一特定领域使用某一特定数据集是导致模型缺乏泛化性能的主要原因。</p><h3 id=方法-1>方法<a hidden class=anchor aria-hidden=true href=#方法-1>#</a></h3><ul><li><p>Multitask learning 多任务学习 &ndash;> 语言模型可以在zero-shot设置中执行下游任务，在没有任何参数或架构修改的情况下</p></li><li><p>pre-training + supervised finetuning</p></li><li><p>模型的优化目标为 p(output|input, task)，具体可以描述为 {task(视作prompt), input, output}：</p><ul><li>例1：a translation training example can be written as the sequence (translate to french, english text, french text)</li><li>例2：reading comprehension training example can be written as (answer the question, document, question, answer)</li></ul></li></ul><h4 id=训练数据>训练数据<a hidden class=anchor aria-hidden=true href=#训练数据>#</a></h4><ul><li>Reddit网站上至少包含 3 karma的文章，爬取了4500万个链接，最终获得800万个文件，包含40GB的文本内容</li></ul><h4 id=模型>模型<a hidden class=anchor aria-hidden=true href=#模型>#</a></h4><p>与GPT大致一致</p><h2 id=gpt-3>GPT-3<a hidden class=anchor aria-hidden=true href=#gpt-3>#</a></h2><h3 id=出发点-2>出发点<a hidden class=anchor aria-hidden=true href=#出发点-2>#</a></h3><p>大多数语言模型在任务不可知的情况下，仍然需要特定于任务的数据集和特定于任务的微调</p><ul><li>需要针对任务的、包含标注实例的大数据集</li><li>在微调数据集上的效果好并不代表模型的泛化性能良好</li></ul><h3 id=方法-2>方法<a hidden class=anchor aria-hidden=true href=#方法-2>#</a></h3><p>meta-learning：训练一个泛化性不错的模型</p><p>in-context learning：在后续过程中，即使已知一些训练样本，也不更新模型权重（个人理解就是在提问过程中包含一些训练样本）：</p><ul><li>zero-shot</li><li>one-shot</li><li>few-shot</li></ul><p align=center><img src=gpt-3-tasks.png alt=gpt-3-tasks></p><h4 id=模型及其架构>模型及其架构<a hidden class=anchor aria-hidden=true href=#模型及其架构>#</a></h4><ul><li>使用与GPT-2相同的模型和架​​构</li><li>Sparse Transformer</li><li>8种不同尺寸</li></ul><p align=center><img src=gpt-3-models.png alt=gpt-3-models></p><hr><div class=zhihu-ref><div class=zhihu-ref-title>参考文献</div><ol><li><a href="https://www.bilibili.com/video/BV1AF411b7xQ?spm_id_from=333.788.videopod.sections&vd_source=9e4f1724ef60547fa31e3c8270245ff8" target=_blank>GPT，GPT-2，GPT-3 论文精读【论文精读】</a></li><li><a href=https://www.mikecaptain.com/resources/pdf/GPT-1.pdf target=_blank>Improving language understanding by generative pre-training</a></li><li><a href=https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf target=_blank>Language models are unsupervised multitask learners</a></li><li><a href=https://www.mikecaptain.com/resources/pdf/GPT-3.pdf target=_blank>Language Models are Few-Shot Learners</a></li></ol></div></div><footer class=post-footer><p style=font-size:medium;margin-bottom:5px;font-weight:700>categories</p><ul class=post-tags><li><a href=http://localhost:1313/cspaulia-blog/categories/large-language-model/>Large Language Model</a></li></ul><p style=font-size:medium;margin-bottom:5px;font-weight:700>tags</p><ul class=post-tags><li><a href=http://localhost:1313/cspaulia-blog/tags/gpt/>GPT</a></li><li><a href=http://localhost:1313/cspaulia-blog/tags/pre-training/>Pre-Training</a></li><li><a href=http://localhost:1313/cspaulia-blog/tags/llm/>LLM</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/cspaulia-blog/posts/metric/><span class=title>« Prev</span><br><span>记录100种评价指标</span>
</a><a class=next href=http://localhost:1313/cspaulia-blog/posts/lora/><span class=title>Next »</span><br><span>LoRA: 大模型低秩适配方法详解</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share GPT系列 on x" href="https://x.com/intent/tweet/?text=GPT%e7%b3%bb%e5%88%97&amp;url=http%3a%2f%2flocalhost%3a1313%2fcspaulia-blog%2fposts%2fgpt%2f&amp;hashtags=GPT%2cPre-training%2cLLM"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share GPT系列 on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fcspaulia-blog%2fposts%2fgpt%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share GPT系列 on telegram" href="https://telegram.me/share/url?text=GPT%e7%b3%bb%e5%88%97&amp;url=http%3a%2f%2flocalhost%3a1313%2fcspaulia-blog%2fposts%2fgpt%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/cspaulia-blog/>cspaulia-blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>