[{"content":"CLIP 对比预训练：\n从网络上收集N个图片-文本对（OpenAI收集了4亿个图片文本对）作为正样本；\nN张图片与剩下的N-1张图片对应的文本组成数据对，作为负样本（即文本描述和图片内容不符）；\nN张图片送入图像编码器，对应文本送入文本编码器，将图像特征与文本特征做点积，得到相似度矩阵；\n将矩阵的每一行当作是一个N类预测的结果，以第 i 行为例，为了使第 i 行、第 i 列的值最大（在第 i 行中相似度最大），我们的 label 应该也是 i，将第 i 行与 label 作 cross entropy，即可完成矩阵的优化；\n将矩阵的每一列当作是一个N类预测的结果，以第 i 列为例，为了使第 i 列、第 i 行的值最大（在第 i 列中相似度最大），我们的 label 应该也是 i，将第 i 列与 label 作 cross entropy，即可完成矩阵的优化。\n从 label 中构建数据分类器：\n用文本标签构建句子，送入文本编码器得到文本特征。 用于zero-shot预测：\n将标签构建的文本特征与图像特征进行相似度匹配，从而完成预测。 LSeg 与CLIP的关系：\n利用已经对齐好的 CLIP 特征空间，将语义标签和像素特征映射到同一空间，通过相似性进行分割预测；\n文本编码器：与CLIP保持一致，训练时不更新参数\u0026#x2744;\u0026#xfe0f;；\n与CLIP不同的点：\n列1 列2 列3 内容1 内容2 内容3 内容4 内容5 内容6 - 对比学习:x:，有监督学习:white_check_mark:； - ","permalink":"http://localhost:1313/cspaulia-blog/posts/clip/","summary":"\u003ch2 id=\"clip\"\u003eCLIP\u003c/h2\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"clip.png\" alt=\"clip\" /\u003e\n\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e对比预训练：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e从网络上收集N个图片-文本对（OpenAI收集了4亿个图片文本对）作为正样本；\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eN张图片与剩下的N-1张图片对应的文本组成数据对，作为负样本（即文本描述和图片内容不符）；\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eN张图片送入图像编码器，对应文本送入文本编码器，将图像特征与文本特征做点积，得到相似度矩阵；\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e将矩阵的每一行当作是一个N类预测的结果，以第 i 行为例，为了使第 i 行、第 i 列的值最大（在第 i 行中相似度最大），我们的 label 应该也是 i，将第 i 行与 label 作 cross entropy，即可完成矩阵的优化；\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e将矩阵的每一列当作是一个N类预测的结果，以第 i 列为例，为了使第 i 列、第 i 行的值最大（在第 i 列中相似度最大），我们的 label 应该也是 i，将第 i 列与 label 作 cross entropy，即可完成矩阵的优化。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e从 label 中构建数据分类器：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e用文本标签构建句子，送入文本编码器得到文本特征。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e用于zero-shot预测：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e将标签构建的文本特征与图像特征进行相似度匹配，从而完成预测。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"lseg\"\u003eLSeg\u003c/h2\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"lseg.png\" alt=\"lseg\" /\u003e\n\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e与CLIP的关系：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e利用已经对齐好的 CLIP 特征空间，将语义标签和像素特征映射到同一空间，通过相似性进行分割预测；\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e文本编码器：与CLIP保持一致，训练时不更新参数\u0026#x2744;\u0026#xfe0f;；\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e与CLIP不同的点：\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ctable style=\"width:100%; text-align:center;\"\u003e\n  \u003ctr\u003e\n    \u003cth\u003e列1\u003c/th\u003e\n    \u003cth\u003e列2\u003c/th\u003e\n    \u003cth\u003e列3\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e内容1\u003c/td\u003e\n    \u003ctd\u003e内容2\u003c/td\u003e\n    \u003ctd\u003e内容3\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e内容4\u003c/td\u003e\n    \u003ctd\u003e内容5\u003c/td\u003e\n    \u003ctd\u003e内容6\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\u003cpre\u003e\u003ccode\u003e- 对比学习:x:，有监督学习:white_check_mark:；\n\n- \n\u003c/code\u003e\u003c/pre\u003e","title":"CLIP及其改进工作"},{"content":"screen基本命令 新建一个screen会话 screen -S \u0026lt;名字\u0026gt; 查看所有screen会话 screen -ls 恢复之前分离的会话 screen -r \u0026lt;会话ID\u0026gt; 退出当前screen会话 键盘点击ctrl+a , 然后按d 查看当前所在会话(id.name) echo $STY 关闭会话 如果在会话之中，输入exit或者Ctrl+d来终止这个会话。成功终止后，如果有其他处于Attached状态的screen界面，他就会跳到那个界面中，如果没有，他就会跳到默认界面上。\n删除会话\nscreen -X -S session_name quit 清理会话 screen -wipe #清理那些dead的会话 ","permalink":"http://localhost:1313/cspaulia-blog/posts/screen/","summary":"\u003ch2 id=\"screen基本命令\"\u003escreen基本命令\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e新建一个screen会话\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003escreen -S \u0026lt;名字\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e查看所有screen会话\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003escreen -ls\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e恢复之前分离的会话\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003escreen -r \u0026lt;会话ID\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e退出当前screen会话\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e键盘点击ctrl+a , 然后按d\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e查看当前所在会话(id.name)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eecho $STY\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e关闭会话\n如果在会话之中，输入exit或者Ctrl+d来终止这个会话。成功终止后，如果有其他处于Attached状态的screen界面，他就会跳到那个界面中，如果没有，他就会跳到默认界面上。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e删除会话\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003escreen -X -S session_name quit\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e清理会话\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003escreen -wipe #清理那些dead的会话\n\u003c/code\u003e\u003c/pre\u003e","title":"终端多路复用工具Screen的用法"},{"content":"Cross Attention 来自博客\n简介 Cross Attention是：\n融合两种不同的嵌入序列的注意力机制 两个序列必须包含相同的维度 两个序列可以来自不同的模态（例如文本、图像、声音） 其中一个序列作为Query的输入，决定了输出的长度 另一个序列作为Key和Value的输入 Cross Attention vs Self-attention Cross Attention与Self-attention只有输入不同。Cross Attention输入为两个维度相同的嵌入序列；Self-attention输入为一个嵌入序列，其KQV均由该序列生成。\nCross Attention算法 拥有两个序列S1、S2 计算S1的K、V 计算S2的Q 根据K和Q计算注意力矩阵 将V应用于注意力矩阵 输出的序列长度与S2一致 $$ \\pmb{\\text{softmax}}((W_Q S_2)(W_K S_1)^\\mathrm{T})W_v S_1 $$\n","permalink":"http://localhost:1313/cspaulia-blog/posts/cross_attention/","summary":"\u003ch2 id=\"cross-attention\"\u003eCross Attention\u003c/h2\u003e\n\u003cp\u003e来自\u003ca href=\"https://vaclavkosar.com/ml/cross-attention-in-transformer-architecture\"\u003e博客\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"简介\"\u003e简介\u003c/h3\u003e\n\u003cp\u003eCross Attention是：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e融合两种不同的嵌入序列的注意力机制\u003c/li\u003e\n\u003cli\u003e两个序列必须包含\u003cstrong\u003e相同的维度\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e两个序列可以来自不同的模态（例如文本、图像、声音）\u003c/li\u003e\n\u003cli\u003e其中一个序列作为\u003cstrong\u003eQuery的输入\u003c/strong\u003e，决定了输出的长度\u003c/li\u003e\n\u003cli\u003e另一个序列作为\u003cstrong\u003eKey和Value的输入\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"cross-attention-vs-self-attention\"\u003eCross Attention vs Self-attention\u003c/h3\u003e\n\u003cp\u003eCross Attention与Self-attention只有输入不同。Cross Attention输入为两个维度相同的嵌入序列；Self-attention输入为一个嵌入序列，其KQV均由该序列生成。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"cross attention\" loading=\"lazy\" src=\"/cspaulia-blog/posts/cross_attention/cross_attention.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"cross-attention算法\"\u003eCross Attention算法\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e拥有两个序列S1、S2\u003c/li\u003e\n\u003cli\u003e计算S1的K、V\u003c/li\u003e\n\u003cli\u003e计算S2的Q\u003c/li\u003e\n\u003cli\u003e根据K和Q计算注意力矩阵\u003c/li\u003e\n\u003cli\u003e将V应用于注意力矩阵\u003c/li\u003e\n\u003cli\u003e输出的序列长度与S2一致\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\n\\pmb{\\text{softmax}}((W_Q S_2)(W_K S_1)^\\mathrm{T})W_v S_1\n$$\u003c/p\u003e","title":"交叉注意力机制"},{"content":"GELU $$ \\text{GELU}(x) = 0.5x(1+\\tanh(\\sqrt{\\frac{2}{\\pi}}(x+0.044715x^3))) $$\n优点 具有更光滑的导数：GELU函数的导数是连续的，这使得在训练深度神经网络时可以更容易地传播梯度，避免了ReLU函数在$x=0$处的导数不连续的问题，从而减少了训练过程中出现的梯度消失问题。 可以提高模型的性能：在实际任务中，使用GELU函数的模型通常比使用ReLU函数的模型表现更好，尤其是在自然语言处理和计算机视觉任务中。 可以加速收敛：GELU函数在激活函数的非线性变换中引入了类似于sigmoid函数的变换，这使得GELU函数的输出可以落在一个更广的范围内，有助于加速模型的收敛速度。 ","permalink":"http://localhost:1313/cspaulia-blog/posts/activation/","summary":"\u003ch3 id=\"gelu\"\u003eGELU\u003c/h3\u003e\n\u003cp\u003e$$\n\\text{GELU}(x) = 0.5x(1+\\tanh(\\sqrt{\\frac{2}{\\pi}}(x+0.044715x^3)))\n$$\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"GELU_Derivative\" loading=\"lazy\" src=\"/cspaulia-blog/posts/activation/GELU_Derivative.jpg\"\u003e\u003c/p\u003e\n\u003ch4 id=\"优点\"\u003e优点\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e具有更光滑的导数\u003c/strong\u003e：GELU函数的导数是连续的，这使得在训练深度神经网络时可以更容易地传播梯度，避免了ReLU函数在$x=0$处的导数不连续的问题，从而减少了训练过程中出现的梯度消失问题。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e可以提高模型的性能\u003c/strong\u003e：在实际任务中，使用GELU函数的模型通常比使用ReLU函数的模型表现更好，尤其是在自然语言处理和计算机视觉任务中。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e可以加速收敛\u003c/strong\u003e：GELU函数在激活函数的非线性变换中引入了类似于sigmoid函数的变换，这使得GELU函数的输出可以落在一个更广的范围内，有助于加速模型的收敛速度。\u003c/li\u003e\n\u003c/ol\u003e","title":"收集N个激活函数"},{"content":"Layer Normalization 在上图中，$N$表示样本轴，$C$表示通道轴，$F$是每个通道的特征数量。BN如右侧所示，它是取不同样本的同一个通道的特征做归一化；LN则是如左侧所示，它取的是同一个样本的不同通道做归一化\n1. BN的问题 1.1. BN与Batch Size BN是按照样本数计算归一化统计量的，当样本数很少时，比如说只有4个，这四个样本的均值和方差便不能反映全局的统计分布息，所以基于少量样本的BN的效果会变得很差。\n1.2. BN与RNN 在一个batch中，通常各个样本的长度都是不同的，当统计到比较靠后的时间片时，例如上图中$t\u0026gt;4$时，这时只有一个样本还有数据，基于这个样本的统计信息不能反映全局分布，所以这时BN的效果并不好。\n另外如果在测试时我们遇到了长度大于任何一个训练样本的测试样本，我们无法找到保存的归一化统计量，所以BN无法运行。\n2. LN详解 2.1. MLP中的LN 先看MLP中的LN。设$H$是一层中隐层节点的数量，$l$是MLP的层数，我们可以计算LN的归一化统计量$\\mu$和$\\sigma$：\n$$ \\mu^{l} = \\frac{1}{H} \\sum_{i=1}^{H} a^l_i ~~~~~~~ \\sigma^{l} = \\sqrt{\\frac{1}{H} \\sum_{i=1}^{H}(a^l_i-\\mu^l)^2} $$\n注意上面统计量的计算是和样本数量没有关系的，它的数量只取决于隐层节点的数量，所以只要隐层节点的数量足够多，我们就能保证LN的归一化统计量足够具有代表性。通过$\\mu^{l}$和$\\sigma^{l}$ 可以得到归一化后的值：\n$$ \\hat{a}^l = \\frac{a^l-\\mu^l}{\\sqrt{(\\sigma^l)^2+\\epsilon}} \\tag{1} $$\n其中$\\epsilon$是一个很小的小数，防止除0。\n在LN中我们也需要一组参数来保证归一化操作不会破坏之前的信息，在LN中这组参数叫做增（gain）$g$和偏置（bias）$b$。假设激活函数为$f$，最终LN的输出为：\n$$ h^l = f(g^l \\odot \\hat{a}^l + b^l) \\tag{2} $$\n合并公式(1)和(2)并忽略参数$l$，有：\n$$ h=f(\\frac{g}{\\sqrt{\\sigma^2+\\epsilon}} \\odot (a-\\mu) + b) $$\n2.2. RNN中的LN 对于RNN时刻$t$时的节点，其输入是$t-1$时刻的隐层状态$h^t$和$t$时刻的输入数据$\\text{x}_t$，可以表示为：\n$$ \\text{a}^t = W_{hh}h^{t-1}+W_{xh}\\text{x}^{t} $$\n接着我们便可以在$\\text{a}^t$上采取和1.1节中完全相同的归一化过程：\n$$ h^t=f(\\frac{g}{\\sqrt{\\sigma^2+\\epsilon}} \\odot (a^t-\\mu^t) + b) ~~~~~~ \\mu^{t} = \\frac{1}{H} \\sum_{i=1}^{H} a^t_i ~~~~~~~ \\sigma^{l} = \\sqrt{\\frac{1}{H} \\sum_{i=1}^{H}(a^t_i-\\mu^t)^2} $$\n","permalink":"http://localhost:1313/cspaulia-blog/posts/norm/","summary":"\u003ch3 id=\"layer-normalization\"\u003e\u003cspan style=\"color: #DFC08A;\"\u003eLayer Normalization\u003c/span\u003e\u003c/h3\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"LNvsBN.jpg\" alt=\"LNvsBN\" /\u003e\n\u003c/p\u003e\n\u003cp\u003e在上图中，$N$表示样本轴，$C$表示通道轴，$F$是每个通道的特征数量。BN如右侧所示，它是取\u003cstrong\u003e不同样本的同一个通道\u003c/strong\u003e的特征做归一化；LN则是如左侧所示，它取的是\u003cstrong\u003e同一个样本的不同通道\u003c/strong\u003e做归一化\u003c/p\u003e\n\u003ch4 id=\"1-bn的问题\"\u003e1. BN的问题\u003c/h4\u003e\n\u003ch5 id=\"11-bn与batch-size\"\u003e1.1. BN与Batch Size\u003c/h5\u003e\n\u003cp\u003eBN是按照\u003cstrong\u003e样本数\u003c/strong\u003e计算归一化统计量的，当样本数很少时，比如说只有4个，这四个样本的均值和方差便不能反映全局的统计分布息，所以基于少量样本的BN的效果会变得很差。\u003c/p\u003e\n\u003ch5 id=\"12-bn与rnn\"\u003e1.2. BN与RNN\u003c/h5\u003e\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"RNN.jpg\" alt=\"RNN\" /\u003e\n\u003c/p\u003e\n\u003cp\u003e在一个batch中，通常各个样本的长度都是不同的，当统计到比较靠后的时间片时，例如上图中$t\u0026gt;4$时，这时只有一个样本还有数据，基于这个样本的统计信息不能反映全局分布，所以这时BN的效果并不好。\u003c/p\u003e\n\u003cp\u003e另外如果在测试时我们遇到了长度大于任何一个训练样本的测试样本，我们无法找到保存的归一化统计量，所以BN无法运行。\u003c/p\u003e\n\u003ch4 id=\"2-ln详解\"\u003e2. LN详解\u003c/h4\u003e\n\u003ch5 id=\"21-mlp中的ln\"\u003e2.1. MLP中的LN\u003c/h5\u003e\n\u003cp\u003e先看MLP中的LN。设$H$是一层中隐层节点的数量，$l$是MLP的层数，我们可以计算LN的归一化统计量$\\mu$和$\\sigma$：\u003c/p\u003e\n\u003cp\u003e$$\n\\mu^{l} = \\frac{1}{H} \\sum_{i=1}^{H} a^l_i ~~~~~~~\n\\sigma^{l} = \\sqrt{\\frac{1}{H} \\sum_{i=1}^{H}(a^l_i-\\mu^l)^2}\n$$\u003c/p\u003e\n\u003cp\u003e注意上面统计量的计算是和样本数量没有关系的，它的数量只取决于隐层节点的数量，所以只要隐层节点的数量足够多，我们就能保证LN的归一化统计量足够具有代表性。通过$\\mu^{l}$和$\\sigma^{l}$\n可以得到归一化后的值：\u003c/p\u003e\n\u003cp\u003e$$\n\\hat{a}^l = \\frac{a^l-\\mu^l}{\\sqrt{(\\sigma^l)^2+\\epsilon}} \\tag{1}\n$$\u003c/p\u003e\n\u003cp\u003e其中$\\epsilon$是一个很小的小数，防止除0。\u003c/p\u003e\n\u003cp\u003e在LN中我们也需要一组参数来保证归一化操作不会破坏之前的信息，在LN中这组参数叫做增（gain）$g$和偏置（bias）$b$。假设激活函数为$f$，最终LN的输出为：\u003c/p\u003e\n\u003cp\u003e$$\nh^l = f(g^l \\odot \\hat{a}^l + b^l) \\tag{2}\n$$\u003c/p\u003e\n\u003cp\u003e合并公式(1)和(2)并忽略参数$l$，有：\u003c/p\u003e\n\u003cp\u003e$$\nh=f(\\frac{g}{\\sqrt{\\sigma^2+\\epsilon}} \\odot (a-\\mu) + b)\n$$\u003c/p\u003e\n\u003ch5 id=\"22-rnn中的ln\"\u003e2.2. RNN中的LN\u003c/h5\u003e\n\u003cp\u003e对于RNN时刻$t$时的节点，其输入是$t-1$时刻的隐层状态$h^t$和$t$时刻的输入数据$\\text{x}_t$，可以表示为：\u003c/p\u003e\n\u003cp\u003e$$\n\\text{a}^t = W_{hh}h^{t-1}+W_{xh}\\text{x}^{t}\n$$\u003c/p\u003e\n\u003cp\u003e接着我们便可以在$\\text{a}^t$上采取和1.1节中完全相同的归一化过程：\u003c/p\u003e","title":"收集N个Norm方法"},{"content":"上传本地文件到Github库 1. 在GitHub上创建仓库（远程仓库） 在 GitHub 上创建一个新的代码仓库\n2. 安装/配置Git 安装略，配置 Git：\ngit config --global user.name \u0026quot;Your Name\u0026quot; git config --global user.email \u0026quot;your_email@example.com\u0026quot; 这里的 \u0026ldquo;Your Name\u0026rdquo; 和 \u0026ldquo;your_email@example.com\u0026rdquo; 分别为你的用户名和邮箱地址\n检查 Git 是否已经配置用户名和邮箱：\ngit config --global user.name git config --global user.email 3. 上传文件到Github（本地仓库–\u0026gt;远程仓库） 初始化 Git 仓库, 执行以下命令：\ngit init 将 csj_project 项目文件夹添加到本地仓库中，执行以下命令：\ngit add csj_project/ #或者输入 git add . 将当前工作目录中的更改保存到本地代码仓库中，执行以下命令：\ngit commit -m \u0026quot;Initial commit\u0026quot; 在 GitHub 上创建一个新的远程仓库，获取复制该仓库的 SSH 或 HTTPS 链接\n将本地仓库与远程仓库进行关联，执行以下命令：\ngit remote add origin \u0026lt;远程仓库链接\u0026gt; \u0026lt;远程仓库链接\u0026gt;就是你刚才复制的仓库的 SSH 或 HTTPS 链接，例如我的就是： https://github.com/CSPaulia/(库的名称).git 完整命令 git remote add origin https://github.com/CSPaulia/(库的名称).git 将本地仓库中的代码推送到远程仓库中，执行以下命令：\n#git push origin 分支名 #完整命令 git push -u origin master ","permalink":"http://localhost:1313/cspaulia-blog/posts/github-tips/","summary":"\u003ch2 id=\"上传本地文件到github库\"\u003e上传本地文件到Github库\u003c/h2\u003e\n\u003ch3 id=\"1-在github上创建仓库远程仓库\"\u003e1. 在GitHub上创建仓库（远程仓库）\u003c/h3\u003e\n\u003cp\u003e在 GitHub 上创建一个新的代码仓库\u003c/p\u003e\n\u003ch3 id=\"2-安装配置git\"\u003e2. 安装/配置Git\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e安装略，配置 Git：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e git config --global user.name \u0026quot;Your Name\u0026quot;\n git config --global user.email \u0026quot;your_email@example.com\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e这里的 \u0026ldquo;Your Name\u0026rdquo; 和 \u0026ldquo;\u003ca href=\"mailto:your_email@example.com\"\u003eyour_email@example.com\u003c/a\u003e\u0026rdquo; 分别为你的用户名和邮箱地址\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e\n\u003cp\u003e检查 Git 是否已经配置用户名和邮箱：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e git config --global user.name\n git config --global user.email\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"3-上传文件到github本地仓库远程仓库\"\u003e3. 上传文件到Github（本地仓库–\u0026gt;远程仓库）\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e初始化 Git 仓库, 执行以下命令：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e git init\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e将 csj_project 项目文件夹添加到本地仓库中，执行以下命令：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e git add csj_project/\n #或者输入\n git add .\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e将当前工作目录中的更改保存到本地代码仓库中，执行以下命令：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e git commit -m \u0026quot;Initial commit\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e在 GitHub 上创建一个新的远程仓库，获取复制该仓库的 SSH 或 HTTPS 链接\u003c/p\u003e","title":"如何上传本地文件到Github库"},{"content":"FLOPs 注意s小写，是floating point operations的缩写（这里的小s则表示复数），表示浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度\n","permalink":"http://localhost:1313/cspaulia-blog/posts/params/","summary":"\u003ch3 id=\"flops\"\u003eFLOPs\u003c/h3\u003e\n\u003cp\u003e注意s小写，是floating point operations的缩写（这里的小s则表示复数），表示浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度\u003c/p\u003e","title":"收集N个模型参数衡量指标"},{"content":"回复流程 整理罗列所有审稿人的意见，并进行分类\n约老师、约同门师兄姐妹开始讨论，给出回答\n整理所有需要补充的实验，估计大概要用到多少的算力资源\n写rebuttal文档\n论据准备\n理论论证：给出详细的推导过程，把过程给师兄、老师看 实验论证：多次检查实验结果 引用论证：明确给出论据的位置（把arXiv的链接贴出来） 表达要求\n用第二人称称呼审稿人（拉近距离） 态度诚恳、有理有据、逻辑清晰、有人情味的表达 撰写流程\n先写中文回答 DeepL翻译 再GPT润色 写完之后，给老师、师兄过目！！！！再统一进行回复。\n如果审稿人继续回复，则继续讨论：\n包括继续和师兄、老师进行一对一的交流 更新rebuttal文档 ","permalink":"http://localhost:1313/cspaulia-blog/posts/rebuttal/writing-tips-rebuttal/","summary":"\u003ch2 id=\"回复流程\"\u003e回复流程\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e整理罗列\u003c/strong\u003e所有审稿人的意见，并进行分类\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e约老师、约同门师兄姐妹开始\u003cstrong\u003e讨论\u003c/strong\u003e，给出回答\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e整理所有需要补充的实验\u003c/strong\u003e，估计大概要用到多少的\u003cstrong\u003e算力资源\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e写rebuttal文档\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e论据准备\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e理论论证：给出详细的推导过程，把过程给师兄、老师看\u003c/li\u003e\n\u003cli\u003e实验论证：多次检查实验结果\u003c/li\u003e\n\u003cli\u003e引用论证：明确给出论据的位置（把arXiv的链接贴出来）\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e表达要求\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e用第二人称称呼审稿人（拉近距离）\u003c/li\u003e\n\u003cli\u003e态度诚恳、有理有据、逻辑清晰、有人情味的表达\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e撰写流程\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e先写中文回答\u003c/li\u003e\n\u003cli\u003eDeepL翻译\u003c/li\u003e\n\u003cli\u003e再GPT润色\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e写完之后，\u003cstrong\u003e给老师、师兄过目\u003c/strong\u003e！！！！再统一进行回复。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e如果审稿人继续回复，则继续讨论：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e包括继续和师兄、老师进行一对一的交流\u003c/li\u003e\n\u003cli\u003e更新rebuttal文档\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e","title":"论文回复方法（Rebuttal Method）"},{"content":"分类任务损失函数 交叉熵（Cross Entropy） $$ \\text{H}_p(q) = \\sum_x q(x) \\log_2(\\frac{1}{p(x)}) = - \\sum_x q(x) \\log_2(p(x)) $$\n交叉熵为我们提供了一种表达两种概率分布的差异的方法。p和q的分布越不相同，p相对于q的交叉熵将越大于p的熵。\n在实际计算中，\n$$ \\text{L} = - \\sum_x q(y|x) \\log_2(p(y|x)) = - \\frac{1}{N} \\sum_x \\sum_c y_{xc} \\log_2(p(y_c|x)) $$\n其中，$N$表示样本数量，$x$表示样本，$c$表示类别，$y_{xc}$表示类别为$c$时$x$的预测标签，只有$c$与真实标签的类别$\\hat{c}$相同时，$y_{x\\hat{c}} = 1$，即$q(y_{\\hat{c}}|x)=1/N$，其余类别$y_{xc} = 0$，即$q(y_c|x)=0/N=0$。\n举个例子，\n预测（softmax归一化后） 真实值 [0.1, 0.2, 0.7] [0, 0, 1] [0.3, 0.4, 0.3] [0, 1, 0] [0.1, 0.2, 0.7] [1, 0, 0] 计算损失函数值：\n$$ \\text{sample 1 Loss} = - (0 \\times \\log{0.1} + 0 \\times \\log{0.2} + 1 \\times \\log{0.7}) = 0.36 $$\n$$ \\text{sample 2 Loss} = - (0 \\times \\log{0.3} + 1 \\times \\log{0.4} + 0 \\times \\log{0.3}) = 0.92 $$\n$$ \\text{sample 3 Loss} = - (1 \\times \\log{0.1} + 0 \\times \\log{0.2} + 0 \\times \\log{0.7}) = 2.30 $$\n$$ \\text{L} = \\frac{0.36+0.92+2.3}{3} = 1.19 $$\nKL 散度（KL Divergence） $$ \\text{D}_{\\text{KL}}(q | p) = - \\sum_i q(x) \\log_2(p(x)) + \\sum_x p(x) \\log_2(p(x)) = \\text{H}_p(q) - \\text{H}(p) $$\n在交叉熵的基础上减去了p的熵，衡量了两个分布之间的距离。\n在神经网络的训练当中，由于p往往是标签的分布，p的熵值是确定的。所以KL散度和交叉熵是等价的。但是由于交叉熵不惜要计算信息熵，计算更加简单，所以交叉熵使用更加广泛。\n二值交叉熵（Binary Cross Entropy） 模型预测结果：\n$$ P_\\theta(y=1)=\\theta ~~~~~~~ P_\\theta(y=0)=1 - \\theta $$\n合并上面两个式子，得到：\n$$ p_\\theta(y) = \\theta^y(1-\\theta)^{(1-y)} $$\n观测到这些数据点的对数似然为：\n$$ l(\\theta) = \\log \\prod^N_{i=1} p_\\theta(y_i) = \\log \\prod^N_{i=1}\\theta^y(1-\\theta)^{(1-y)} = \\sum_{i=1}^N [y_i\\log \\theta + (1-y_i)\\log(1-\\theta)] $$\n这个损失函数就是$y_i$与$\\theta$的交叉熵$H_y(\\theta)$。\n平衡交叉熵（Balenced Cross Entropy） 为了解决样本数量不平衡这个问题，我们可以选择给Cross Entropy添加权重。以二分类问题举例，Binary Cross Entropy已经介绍过Binary Cross Entropy：\n$$ \\text{L} = - \\sum_{i=1}^N [y_i\\log p + (1-y_i)\\log(1-p)] $$\n改写一下，\n$$ \\text{L} = \\begin{cases} -\\log(p) \u0026amp; \\text{if}~y=1 \\\\ -\\log(1-p) \u0026amp; \\text{otherwise} \\end{cases} $$\n再改写一下，\n$$ p_t= \\begin{cases} \u0026amp; p \u0026amp; \\text{if}~y=1 \\\\ \u0026amp; 1-p \u0026amp; \\text{otherwise} \\end{cases} $$\n$$ \\text{L} = -\\log(p_t) $$\n添加权重，\n$$ \\text{L} = -\\alpha_t\\log(p_t) $$\n其中$y=1$时$\\alpha_t=\\alpha$；$y=0$时$\\alpha_t=1-\\alpha$。$\\frac{\\alpha}{1-\\alpha}=\\frac{n}{m}$，$n$为$y=0$的样本（负样本）个数，$m$为$y=1$的样本（正样本）个数。\nBalenced Cross Entropy确实解决了样本不均衡问题，但并未解决样本难易问题。为解决这个问题，详见Focal Loss.\nFocal Loss $$ \\text{FL}(p_t) = (1-p_t)^\\gamma\\log(p_t) $$\n$p_t$是模型预测的结果的类别概率值。$−\\log(p_t)$和交叉熵损失函数一致，因此当前样本类别对应的那个$p_t$如果越小，说明预测越不准确，那么$(1-p_t)^{\\gamma}$这一项就会增大，这一项也作为困难样本的系数，预测越不准，Focal Loss越倾向于把这个样本当作困难样本，这个系数也就越大，目的是让困难样本对损失和梯度的贡献更大。\n前面的$\\alpha_t$是类别权重系数。如果你有一个类别不平衡的数据集，那么你肯定想对数量少的那一类在loss贡献上赋予一个高权重，这个$\\alpha_t$就起到这样的作用。因此，$\\alpha_t$应该是一个向量，向量的长度等于类别的个数，用于存放各个类别的权重。一般来说$\\alpha_t$中的值为每一个类别样本数量的倒数，相当于平衡样本的数量差距\nLovasz Loss Lovasz Loss的推导 IoU (intersection-over-union，也叫jaccard index)是自然图像分割比赛中常用的一个衡量分割效果的评价指标，所以一个自然的想法就是能否将IoU作为loss function来直接优化。交并比公式：\n$$ J_c(y^*, \\widetilde{y}) = \\frac{ | { y^* = c } \\cap { \\widetilde{y} = c } | }{ | { y^* = c } \\cup { \\widetilde{y} = c } | } $$\n其中$y^{*}$表示Ground Truth标签，$\\widetilde{y}$表示预测标签，$\\vert \\cdot \\vert$表示集合中的元素个数。可以看出上式的值是介于0到1之间的，由此可以设计出损失函数：\n$$ \\Delta_{J_c}(y^{*},\\widetilde{y})=1-J_c(y^{*},\\widetilde{y}) $$\n这个损失函数是离散的，无法直接求导，需要对其做光滑延拓。\n改写一下$\\Delta_{J_c}$,\n$$ \\Delta_{J_c} = 1-J_c(y^{*},\\widetilde{y}) = \\frac{\\vert M_c \\vert}{\\vert {y^{*}=c} \\cup M_c \\vert} \\tag{1} $$\n其中，$M_c(y^{*},\\widetilde{y}) = {y^{*}=c,\\widetilde{y}\\neq c} \\cup {y^{*} \\neq c,\\widetilde{y}=c}$，$M_c$是损失函数的自变量，它表达网络分割结果与Ground Truth标签不匹配的集合。$M_c$的定义域为${0,1}^p$，即$M_c \\in {0,1}^p$，$p$表示集合$M_c$中像素的个数。\n由于(1)是次模（submodular）函数，故可以对其做光滑延拓。\n定义1 若一个集合函数$\\Delta:{0,1}^p \\rightarrow \\mathbb{R}$对于所有的集合$A,B \\in {0,1}^p$满足\n$$ \\Delta(A) + \\Delta(B) \\geq \\Delta(A \\cup B) + \\Delta(A \\cap B) $$\n则我们称$\\Delta$是次模函数。\n定义2 Lovasz extension 现存在一集合函数$\\Delta:{0,1}^p \\rightarrow \\mathbb{R}$且$\\Delta(\\pmb{0})=0$，则其Lovasz extension为\n$$ \\overline{\\Delta} = \\sum_{i=1}^p m_i g_i(\\pmb{m}) \\tag{2} $$\n$$ g_i(m) = \\Delta({\\pi_1,\\cdots,\\pi_i}) - \\Delta({\\pi_1,\\cdots,\\pi_{i-1}}) $$\n$\\pi$是一个数组，根据元素$\\pmb{m}$降序排序。例如，$x_{\\pi_1} \\geq x_{\\pi_2} \\geq \\cdots \\geq x_{\\pi_p}$。\n此时$\\overline{\\Delta}$已经是一个连续、分段线性的函数了，可以直接对误差$m$求导，导数为$g(m)$。\nLovasz Loss在多类分割中的应用 假设$F_i(c)$表示的是模型最后输出的像素$i$预测为类别$c$的非归一化分数，则可以通过softmax函数将$F_i(c)$归一化得到像素$i$预测为类别$c$的概率：\n$$ f_i(c) = \\frac{e^{F_i(c)}}{\\sum_{c\u0026rsquo; \\in C} e^{F_i(c\u0026rsquo;)}} $$\n那么(2)中的$m_i(c)$可以定义为\n$$ m_i(c) = \\begin{cases} \u0026amp; 1-f_i(c) \u0026amp; \\text{if}~c=y_i^{*} \\\\ \u0026amp; f_i(c) \u0026amp; \\text{otherwise} \\end{cases} $$\n那么损失函数为\n$$ loss(\\pmb{f}(c)) = \\overline{\\Delta_{J_c}}(\\pmb{m}(c)) $$\n考虑到类别平均mIoU的计算方式，最终的损失函数为\n$$ loss(\\pmb{f}) = \\frac{1}{\\vert C \\vert} \\sum_{c \\in C} \\overline{\\Delta_{J_c}}(\\pmb{m}(c)) $$\nLovasz Loss在多类分割中的代码流程 步骤1 计算预测结果的误差\nsigns = 2. * predictions.float() - 1. errors = (1. - logits * Variable(signs)) errors_sorted, perm = torch.sort(errors, dim=0, descending=True) 这一步得到公式(2)中的$m_i$。\n步骤2 计算IoU得分\ngts = gt_sorted.sum() intersection = gts - gt_sorted.float().cumsum(0) union = gts + (1 - gt_sorted).float().cumsum(0) jaccard = 1. - intersection / union 这一步得到公式(1)的值，即IoU得分。\n步骤3 根据IoU得分计算Lovasz extension的梯度\njaccard[1:p] = jaccard[1:p] - jaccard[0:-1] 这一步得到公式(2)中的$g_i(\\pmb{m})$。\n步骤4 计算Loss\nloss = torch.dot(F.relu(errors_sorted), Variable(grad)) 这一步得到公式(2)的值。\n","permalink":"http://localhost:1313/cspaulia-blog/posts/loss/","summary":"\u003ch2 id=\"分类任务损失函数\"\u003e\u003cspan style=\"color: #CC7E40;\"\u003e分类任务损失函数\u003c/span\u003e\u003c/h2\u003e\n\u003ch3 id=\"交叉熵cross-entropy\"\u003e\u003cspan style=\"color: #DFC08A;\"\u003e交叉熵（Cross Entropy）\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e$$\n\\text{H}_p(q) = \\sum_x q(x) \\log_2(\\frac{1}{p(x)}) = - \\sum_x q(x) \\log_2(p(x))\n$$\u003c/p\u003e\n\u003cp\u003e交叉熵为我们提供了一种表达两种概率分布的差异的方法。p和q的分布越不相同，p相对于q的交叉熵将越大于p的熵。\u003c/p\u003e\n\u003cp\u003e在实际计算中，\u003c/p\u003e\n\u003cp\u003e$$\n\\text{L} = - \\sum_x q(y|x) \\log_2(p(y|x))\n= - \\frac{1}{N} \\sum_x \\sum_c y_{xc} \\log_2(p(y_c|x))\n$$\u003c/p\u003e\n\u003cp\u003e其中，$N$表示样本数量，$x$表示样本，$c$表示类别，$y_{xc}$表示类别为$c$时$x$的预测标签，只有$c$与真实标签的类别$\\hat{c}$相同时，$y_{x\\hat{c}} = 1$，即$q(y_{\\hat{c}}|x)=1/N$，其余类别$y_{xc} = 0$，即$q(y_c|x)=0/N=0$。\u003c/p\u003e\n\u003cp\u003e举个例子，\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: center\"\u003e预测（softmax归一化后）\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e真实值\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e[0.1, 0.2, 0.7]\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e[0, 0, 1]\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e[0.3, 0.4, 0.3]\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e[0, 1, 0]\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: center\"\u003e[0.1, 0.2, 0.7]\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e[1, 0, 0]\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e计算损失函数值：\u003c/p\u003e\n\u003cp\u003e$$\n\\text{sample 1 Loss} = - (0 \\times \\log{0.1} + 0 \\times \\log{0.2} + 1 \\times \\log{0.7}) = 0.36\n$$\u003c/p\u003e","title":"记录100种损失函数（Loss Function）"}]