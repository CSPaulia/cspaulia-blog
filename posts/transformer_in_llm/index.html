<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LM Architecture and Training | cspaulia-blog</title><meta name=keywords content="LLM,Training"><meta name=description content="关于语言模型架构和超参数的笔记"><meta name=author content="CSPaulia"><link rel=canonical href=https://cspaulia.github.io/cspaulia-blog/posts/transformer_in_llm/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/cspaulia-blog/assets/css/stylesheet.4a40da687e9ad320449d5d267445e114ee7b6620a3d534bde2b12baa408f07f5.css integrity="sha256-SkDaaH6a0yBEnV0mdEXhFO57ZiCj1TS94rErqkCPB/U=" rel="preload stylesheet" as=style><link rel=icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://cspaulia.github.io/cspaulia-blog/posts/transformer_in_llm/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1,trust:!0,strict:!1})})</script><meta property="og:url" content="https://cspaulia.github.io/cspaulia-blog/posts/transformer_in_llm/"><meta property="og:site_name" content="cspaulia-blog"><meta property="og:title" content="LM Architecture and Training"><meta property="og:description" content="关于语言模型架构和超参数的笔记"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-10T10:00:03+08:00"><meta property="article:modified_time" content="2025-11-04T17:58:25+08:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Training"><meta property="og:image" content="https://cspaulia.github.io/cspaulia-blog/posts/transformer_in_llm/LLMs.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cspaulia.github.io/cspaulia-blog/posts/transformer_in_llm/LLMs.jpg"><meta name=twitter:title content="LM Architecture and Training"><meta name=twitter:description content="关于语言模型架构和超参数的笔记"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://cspaulia.github.io/cspaulia-blog/posts/"},{"@type":"ListItem","position":2,"name":"LM Architecture and Training","item":"https://cspaulia.github.io/cspaulia-blog/posts/transformer_in_llm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LM Architecture and Training","name":"LM Architecture and Training","description":"关于语言模型架构和超参数的笔记","keywords":["LLM","Training"],"articleBody":"一篇非常棒的博客，它总结了几种流行的大模型之间的架构差异\n1. 原始Transformer vs 现代变体 下表总结了原始Transformer（Vaswani et al., 2017）与现代大语言模型（LLM）中的主流Transformer变体在架构和训练细节上的主要区别：\n方面 原始Transformer (2017) 现代LLM中的变体 归一化顺序 后归一化（Post-LN） 前归一化（Pre-LN） 激活函数 ReLU SwiGLU（GELU、SiLU、Swish等） Dropout 广泛使用 训练大模型时常常减小或去除 归一化类型 LayerNorm RMSNorm（LayerNorm、ScaleNorm等） 线性层 添加偏置 不添加偏置 注意力头 多头注意力（固定头数） GQA、MQA等 位置编码 绝对位置编码（sinusoidal） RoPE等 其它 - FlashAttention, MoE, 分层并行等 1.1. 前归一化 vs 后归一化 几乎所有的现代语言模型均采用前归一化（除了BERT），能使训练更加稳定\n左图为前归一化（pre-norm），右图为后归一化（post-norm）\n新！：左图为前归一化（pre-norm），右图为双归一化（‘double’ norm，使用者包括 Grok，Gemma 2）\n新！：Olom 2 仅使用非残差部分的后归一化\n1.2. LayerNorm vs RMSNorm 原始 Transformer：LayerNorm (GPT3/2/1，OPT，GPT-J，BLOOM)\n$$ y = \\frac{x - \\textbf{E}[x]}{\\sqrt{\\textbf{Var}[x] + \\epsilon}} * \\gamma + \\beta $$\n现代语言模型：RMSNorm（LLaMA-family，PaLM，T5）\n$$ y = \\frac{x}{\\sqrt{\\lVert x \\rVert^2_2 + \\epsilon}} * \\gamma $$\nRMSNorm的优势：运行速度更快，而并不影响精度\n更少的 Operations（无需计算均值） 更少的参数（没有偏置项需要存储） 1.3. FFN：有偏置 vs 无偏置 原始 Transformer：有偏置\n$$ \\textbf{FFN}(x) = \\max(0,xW_1+b_1)W_2+b_2 $$\n现代语言模型：无偏置\n$$ \\textbf{FFN}(x) = \\sigma(xW_1)W_2 $$\n无偏置的优势：更小的存储开销以及稳定的优化\n1.4. 激活函数 Activation Model ReLU Original transformer, T5, Gopher, Chinchilla, OPT GeLU GPT1/2/3, GPTJ, GPT-Neox, BLOOM GeGLU T5 v1.1, mT5, LaMDA, Phi3, Gemma 2, Gemma 3 SwiGLU LLaMa 1/2/3, PaLM, Mistral, OlMo, most models post 2023 激活函数的介绍详见 Post\n1.5. 位置编码 1.5.1. 余弦位置编码（Sinusoidal Positional Encoding） 主要思想：用不同频率的正弦、余弦波来编码不同维度的位置信息\n对于序列中第 $pos$ 个位置、向量维度 $i$，编码方式为:\n$$ \\begin{aligned} PE_{(pos,2i)} = sin(\\frac{pos}{10000^{2i/d_{model}}}) \\ PE_{(pos,2i+1)} = cos(\\frac{pos}{10000^{2i/d_{model}}}) \\end{aligned} $$\n其中：\n$pos$ 表示位置（从0开始） $i$ 表示维度索引 $d_{model}$ 表示模型的隐藏层维度 10000 是一个经验常数，控制频率范围 相对位置的捕捉 假设模型关注两个 token：位置 $pos_1$ 和 $pos_2$ ，那他们的编码分别是：\n$$ \\begin{aligned} E_1 = [sin(\\frac{pos_1}{10000^{0}}), cos(\\frac{pos_1}{10000^{0}}), sin(\\frac{pos_1}{10000^{2/d_{model}}}), cos(\\frac{pos_1}{10000^{2/d_{model}}}), \\dots ] \\\\ E_2 = [sin(\\frac{pos_2}{10000^{0}}), cos(\\frac{pos_2}{10000^{0}}), sin(\\frac{pos_2}{10000^{2/d_{model}}}), cos(\\frac{pos_2}{10000^{2/d_{model}}}), \\dots ] \\end{aligned} $$\n我们在模型内部做如下操作：\n$$ E_1 \\cdot E_2 = sin(\\frac{pos_1}{10000^{0}})sin(\\frac{pos_2}{10000^{0}}) + cos(\\frac{pos_1}{10000^{0}})cos(\\frac{pos_2}{10000^{0}}) + \\dots $$\n利用和差化积公式：\n$$ \\sin a \\sin b + \\cos a \\cos b = \\cos(a - b) $$\n可得到：\n$$ E_1 \\cdot E_2 = \\cos(\\frac{pos_1 - pos_2}{10000^{0}}) + \\cos(\\frac{pos_1 - pos_2}{10000^{2/d_{model}}}) + \\dots $$\n即可得到两个位置的相对距离\n1.5.2. 绝对位置编码（Absolute Positional Encoding）/ 可学习位置编码（Learnable Positional Encoding） 我个人认为绝对位置编码是一种概念，它表达将 token 的位置信息直接编码，而不是将 token 之间的相对位置进行编码\n可学习式位置嵌入学习一个嵌入矩阵：\n$$ P = [u_0, u_1, u_2, \\dots, u_{L-1}] \\in \\mathbb{R}^{L \\times d_{model}} $$\n其中：\n$L$ 是最大序列长度 每一行的 $u_i$ 是位置 $i$ 的可训练嵌入向量 输入到模型的最终向量：\n$$ Embed(x, i) = v_x + u_i $$\n其中 $v_x$ 是 token $x$ 的词嵌入向量\n1.5.3. 相对位置编码（Relative Positional Encoding） 绝对位置编码的缺点：\n泛化差：训练时的序列长度是固定的，比如 512；超出这个长度就无法使用 缺乏相对感知：模型知道第 5 个词、第 10 个词，但不知道它们“相隔 5 个位置” 然而自然语言的顺序关系往往是相对的：\n“the cat” 和 “the big cat” 的依赖关系中，“cat” 距离 “the” 只有几步之差\n因此，相对位置编码的目标是：让模型直接学习 “第 i 个 token 与第 j 个 token 的距离（i−j）” 对注意力的影响\n相对位置编码通过在注意力打分中显式地加入位置差信息:\n$$ e_{ij} = \\frac{(x_i W_Q)(x_j W_K + a^K_{ij})^T}{\\sqrt{d_k}} $$\n其中 $a_{ij}^K$ 是一个向量，表示 token i 和 token j 之间的相对位置信息\n1.5.4. RoPE（Rotary Position Embedding） 我们该如何让 添加位置编码后的嵌入向量 $x$ 和 $y$ 在完成点积后，只关注它们的相对位置呢？也就是要实现如下目标：\n$$ \\langle f(x, i), f(y, j) \\rangle = g(x, y, i-j) \\tag{1} $$ 其中 $\\langle \\cdot, \\cdot \\rangle$ 表示内积运算\n余弦位置编码：不满足（1）式: 在余弦位置编码中，token 的嵌入可以表征为 $Embed(x, i) = v_x + E_i$，其中 $v_x$ 是 token 的词嵌入向量，$E_i$ 是位置编码; $\\langle Embed(x, i), Embed(y, j) \\rangle = \\langle v_x + E_i, v_y + E_j \\rangle = \\langle v_x, v_y \\rangle + \\langle v_x, E_j \\rangle + \\langle E_i, v_y \\rangle + \\langle E_i, E_j \\rangle$，有许多内积项依赖于绝对位置 $i$ 和 $j$，而不仅仅是它们的差值 $i-j$ 绝对位置编码：不满足（1）式 相对位置编码：不满足（1）式中的内积形式： 几何解释消失：原来的点积可以看成两个向量夹角的余弦相似度（几何上可解释），加入 $a_{ij}$ 后，这个解释就失效 对称性破坏：$e_{ij}$ 与 $e_{ji}$ 不再一致，使模型捕捉方向信息 注意力的归一化解释变弱：softmax 之前的 logits 不再仅由向量相似度决定，额外偏置可能干扰注意力稳定性 RoPE 的核心思想是：通过复数旋转（或二维平面旋转）把位置嵌入到每个向量维度中\n$$ \\begin{aligned} x_p \u0026= [x_{p,0}, x_{p,1}, \\dots, x_{p,d-1}]\\\\ f_{{q,k}}(x_p,p) \u0026= \\mathbf{R}^d_{\\Theta,p},W_{{q,k}},x_p\\\\ \\mathbf{R}^d_{\\Theta,p} \u0026= \\begin{bmatrix} \\cos(p\\theta_0) \u0026 -\\sin(p\\theta_0) \u0026 \u0026 \u0026 \\\\ \\sin(p\\theta_0) \u0026 \\cos(p\\theta_0) \u0026 \u0026 \u0026 \\\\ \u0026 \u0026 \\cos(p\\theta_1) \u0026 -\\sin(p\\theta_1) \u0026 \\\\ \u0026 \u0026 \\sin(p\\theta_1) \u0026 \\cos(p\\theta_1) \u0026 \\\\ \u0026 \u0026 \u0026 \u0026 \\ddots \\\\ \u0026 \u0026 \u0026 \u0026 \u0026 \\cos(p\\theta_{d/2-1}) \u0026 -\\sin(p\\theta_{d/2-1}) \\\\ \u0026 \u0026 \u0026 \u0026 \u0026 \\sin(p\\theta_{d/2-1}) \u0026 \\cos(p\\theta_{d/2-1}) \\end{bmatrix} \\end{aligned} $$\n其中 $\\theta_k = 10000^{-2k/d}$\n接下来证明RoPE符合（1）式：\n对于 token $x$ 的两个相邻嵌入维度 $2i$ 和 $2i+1$，有：\n$$ \\tilde{x}_{p}^{(k)} = \\begin{bmatrix} cos(p\\theta_k) \u0026 -sin(p\\theta_k) \\\\ sin(p\\theta_k) \u0026 cos(p\\theta_k) \\end{bmatrix} \\begin{bmatrix} x_{p,2k} \\\\ x_{p,2k+1} \\end{bmatrix} =(x_{p,2k}+ix_{p,2k+1})e^{ip\\theta_k} $$\n因此有：\n$$ \\langle \\tilde{x}_{p}^{(k)}, \\tilde{y}_{q}^{(k)} \\rangle = \\tilde{x}_{p}^{(k)} \\cdot \\overline{\\tilde{y}}_{q}^{(k)} = (x_{p,2k}+ix_{p,2k+1})(y_{q,2k}-iy_{q,2k+1})e^{i(p-q)\\theta_k} $$\n符合（1）式中的要求\n2. 超参数 2.1. 前馈网络中的特征维度 假设 $d_{ff}$ 是前馈网络的隐藏层维度，$d_{model}$ 是模型的隐藏层维度\n$$ d_{ff} = 4d_{model} $$\n此时，标准 FFN 的参数量为：\n第一层：$d_{model} \\times d_{ff} = 4d_{model}^2$ 第二层：$d_{ff} \\times d_{model} = 4d_{model}^2$ 总计：$8d_{model}^2$ 对于包含GLU类激活函数的 FFN，参数量为：\nGLU中的content部分：$d_{model} \\times d’_{ff}$ GLU中的gate部分：$d_{model} \\times d’_{ff}$ 第二层：$d’_{ff} \\times d_{model}$ 总计：$3d_{model} \\times d’_{ff}$ 为了使包含GLU类激活函数的 FFN 与标准 FFN 的参数量相同，我们需要满足：\n$$ 3d_{model} \\times d’_{ff} = 8d_{model}^2 $$\n即\n$$ d’_{ff} = \\frac{8}{3}d_{model} $$\n下表总结了一些流行大模型中前馈网络隐藏层维度与模型隐藏层维度的比值：\nModel $( d_{ff} / d_{model} )$ PaLM 4.00 Mistral 7B 3.50 LLaMA-2 70B 3.50 LLaMA 70B 2.68 Qwen 14B 2.67 DeepSeek 67B 2.68 Yi 34B 2.85 T5 v1.1 2.50 2.2. 注意力头数与每头维度 我们尽量使得 $d_{head} \u003e d_{model} / num_{heads}$，很多模型选择令 $d_{head} = d_{model} / num_{heads}$\nModel Num heads Head dim Model dim Ratio GPT3 96 128 12288 1 T5 128 128 1024 16 T5 v1.1 64 64 4096 1 LaMDA 128 128 8192 2 PaLM 48 258 18432 1.48 LLaMA2 64 128 8192 1 2.3. 模型宽高比（aspect ratio） 这里的宽高比指的是：\n$$ d_{model} / num_{layers} $$\nModel ( d_{model} / n_{layer} ) BLOOM 205 T5 v1.1 171 PaLM (540B) 156 GPT3 / OPT / Mistral / Qwen 128 LLaMA / LLaMA2 / Chinchila 102 T5 (11B) 43 GPT2 33 太深的模型很难并行化，并且具有很高的延迟\n2.4. 字典大小（vocabulary size） 单语言：3-5万个 token 多语言：10-25万个 token 2.5 Dropout 和 权重衰减（weight decay） 老模型会更多的采用Dropout； 新模型会更多的采用权重衰减，其作用更多的在于与loss的互动（后期更快的loss下降），而非防止过拟合 Model Dropout* Weight decay Original transformer 0.1 0 GPT2 0.1 0.1 T5 0.1 0 GPT3 0.1 0.1 T5 v1.1 0 0 PaLM 0 (variable) OPT 0.1 0.1 LLaMA 0 0.1 Qwen 14B 0.1 0.1 3. 模型训练稳定性技巧 模型的训练，应当避免出现“尖刺”，如下图蓝色曲线所示：\nz-loss 观察出现在 LLM 的最后一层的softmax，softmax 的定义为：\n$$ P(y=i|x) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} = \\frac{e^{z_i}}{Z} $$\n因此在 Cross-Entropy Loss 中，我们有：\n$$ Loss_{CE} = -\\log P(y=i|x) = -\\log \\frac{e^{z_i}}{\\sum_j e^{z_j}} = -z_i + \\log Z $$\n当 $Z$ 为0时，会导致 $Loss_{CE}$ 过大，造成训练的不稳定\n因此我们想办法，令 $Z$ 趋近于 1，即 $\\log Z$ 趋近于 0，我们可以添加一个 z-loss 项：\n$$ Loss_{z} = ((\\log Z)^2 - 0)^2 = (\\log Z)^2 $$\n最终有：\n$$ Loss = Loss_{CE} + \\lambda Loss_{z} $$\n其中 $\\lambda$ 是一个很小的值，一般为 $1e-3$ 或 $1e-4$\n4. 模型结构优化 4.1. KV Cache 图片来源于网络\n常规的注意力计算：\n$$ Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V $$\n假设 $X \\in \\mathbb{R}^{b \\times T \\times D}$，$W_{\\{Q, K, V\\}} \\in \\mathbb{R}^{D \\times (hd)}$，其中 $T$ 是序列长度，$h$ 为注意力头数，$d$ 是每个注意力头的隐藏层维度，设 $D = hd$，则计算量为\n计算KQV：$3 \\times 2bTD^2 = 6bT(hd)^2$ 计算$Q \\times K$：$2bhT^2d$ 计算softmax：$n \\times bhT^2$（softmax包含 n 次计算操作） 计算$Output_{softmax} \\times V$：$2bhT^2d$ 计算输出线性层：$2bTD^2$ 总计算量 $\\approx 8bTD^2 + 4bhT^2d$（忽略softmax） 总存储开销为\n权重参数开销： $W_{\\{Q, K, V\\}}$ 存储开销：$3 \\times D(hd) = 3(hd)^2$ 输出线性层存储开销：$(hd)D = (hd)^2$ 中间激活开销： 输入存储：$bTD$ KQV存储：$3 \\times bhTd$ softmax后得到的注意力权重存储：$bhT^2$ 输出存储：$bTD$ （即下一层的输入，不计入本层开销） 总存储开销 $\\approx 4(hd)^2 + bTD + 3bhTd + bhT^2$ 使用 KV Cache 的注意力计算：\n训练时，使用 KV Cache 并不影响计算量，因此训练时往往不使用 KV Cache；\n但在推理时，假设输入序列长度为 $t$，则预测下一个 token 时，计算量为 $\\approx 8btD^2 + 4bht^2d$，若不使用 KV Cache，则预测下下个 token 的计算量为 $\\approx 8b(t+1)D^2 + 4bh(t+1)^2d$，这个计算量会随着序列长度的增加而显著增加；\n而使用 KV Cache 后，预测下一个 token 的计算量为：\nKQV计算：由于无需重新计算$Q_{1:(t-1)}, K_{1:(t-1)}, V_{1:(t-1)}$，只需要计算$Q_{t}, K_{t}, V_{t}$，因此计算量为$3 \\times 2bD^2$ 计算$Q_{t} \\times K_{1:t}$：$2bhtd$ 计算softmax：$n \\times bht$ 计算$Output_{softmax} \\times V_{1:t}$：$2bhtd$ 计算输出线性层：$2bD^2$ 总计算量 $\\approx 8bD^2 + 4bhtd$（忽略softmax） 计算量随序列长度的增加轻微增加。\n而使用 KV Cache 后，存储开销会稍稍增加。因为在推理阶段不使用 KV Cache，总存储开销仅为权重参数开销 $4(hd)^2$，而使用 KV Cache 后，总存储开销为权重参数开销 $4(hd)^2$ 加上 KV Cache 的存储开销 $2bhtd$，因此总存储开销 $\\approx 4(hd)^2 + 2bhtd$。\n4.2. MQA 和 GQA 为了减少 KV Cache 的存储开销，一个简单的思路就是让注意力头共享 K 和 V\n若 $h$ 个注意力头共享一个 K 和 V，则为 MQA（Multi-query Attention） 若将 $h$ 个注意力头划分为 $g$ 组，每组 $h/g$ 个头共享 K 和 V，则为 GQA（Grouped-query Attention） 如下图所示：\n模型 训练时结构 推理时结构 备注 GPT-3 / GPT-4 MHA MHA / GQA（部分优化版） GPT-4 reportedly uses GQA PaLM 2 GQA GQA 原生训练结构 Claude 3 GQA GQA Anthropic 公开结构说明 LLaMA 2 MHA GQA (converted) Meta 后期转换版 Mistral GQA GQA 端到端使用 GQA Falcon MQA MQA 优化长序列推理 Gemini 1.5 GQA GQA Google 用于多模态大模型 MQA 和 GQA 与 MHA 相比，计算复杂度不变，但存储开销减少，假设 $h$ 个注意力头共享 $k$ 个 K 和 V：\n权重参数开销： $W_{\\{Q\\}}$ 存储开销：$D(hd) = (hd)^2$ $W_{\\{K, V\\}}$ 存储开销：$2 \\times D(kd) = 2(hd)(kd)$ 输出线性层存储开销：$(hd)D = (hd)^2$ 中间激活开销： 输入存储：$bTD$ Q存储：$bhTd$ KV存储：$2bkdT$ softmax后得到的注意力权重存储：$bhT^2$ 输出存储：$bTD$ （即下一层的输入，不计入本层开销） 总存储开销 $\\approx 2(hd)^2 + 2hkd^2 + 2bTD + 2bkdT + bhT^2$ 4.3. 稀疏注意力（Sparse Attention） 稀疏注意力（Sparse Attention）：可以参考博客\n参考文献 stanford-cs336 lecture 3 ","wordCount":"1176","inLanguage":"en","image":"https://cspaulia.github.io/cspaulia-blog/posts/transformer_in_llm/LLMs.jpg","datePublished":"2025-10-10T10:00:03+08:00","dateModified":"2025-11-04T17:58:25+08:00","author":{"@type":"Person","name":"CSPaulia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://cspaulia.github.io/cspaulia-blog/posts/transformer_in_llm/"},"publisher":{"@type":"Organization","name":"cspaulia-blog","logo":{"@type":"ImageObject","url":"https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://cspaulia.github.io/cspaulia-blog/ accesskey=h title="Home (Alt + H)"><img src=https://cspaulia.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://cspaulia.github.io/cspaulia-blog/publications/ title=publications><span>publications</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/posts/ title=posts><span>posts</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/series/ title=series><span>series</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/categories/ title=categories><span>categories</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/tags/ title=tags><span>tags</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/archives/ title=archives><span>archives</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://cspaulia.github.io/cspaulia-blog/>Home</a>&nbsp;»&nbsp;<a href=https://cspaulia.github.io/cspaulia-blog/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">LM Architecture and Training</h1><div class=post-description>关于语言模型架构和超参数的笔记</div><div class=post-meta><span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg> October 10, 2025</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 19l7-7 3 3-7 7-3-3z"/><path d="M18 13l-1.5-7.5L2 2l3.5 14.5L13 18l5-5z"/><path d="M2 2l7.586 7.586"/><circle cx="11" cy="11" r="2"/></svg> November 4, 2025</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><path d="M14 2v6h6"/><line x1="8" y1="13" x2="16" y2="13"/><line x1="8" y1="17" x2="16" y2="17"/><line x1="8" y1="9" x2="12" y2="9"/></svg> 1176 words</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg> 6 min</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg> CSPaulia</span>
<span id=busuanzi_container_page_pv>｜ <span id=busuanzi_value_page_pv></span> views</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-%e5%8e%9f%e5%a7%8btransformer-vs-%e7%8e%b0%e4%bb%a3%e5%8f%98%e4%bd%93 aria-label="1. 原始Transformer vs 现代变体">1. 原始Transformer vs 现代变体</a><ul><li><a href=#11-%e5%89%8d%e5%bd%92%e4%b8%80%e5%8c%96-vs-%e5%90%8e%e5%bd%92%e4%b8%80%e5%8c%96 aria-label="1.1. 前归一化 vs 后归一化">1.1. 前归一化 vs 后归一化</a></li><li><a href=#12-layernorm-vs-rmsnorm aria-label="1.2. LayerNorm vs RMSNorm">1.2. LayerNorm vs RMSNorm</a></li><li><a href=#13-ffn%e6%9c%89%e5%81%8f%e7%bd%ae-vs-%e6%97%a0%e5%81%8f%e7%bd%ae aria-label="1.3. FFN：有偏置 vs 无偏置">1.3. FFN：有偏置 vs 无偏置</a></li><li><a href=#14-%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0 aria-label="1.4. 激活函数">1.4. 激活函数</a></li><li><a href=#15-%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81 aria-label="1.5. 位置编码">1.5. 位置编码</a></li></ul></li><li><a href=#2-%e8%b6%85%e5%8f%82%e6%95%b0 aria-label="2. 超参数">2. 超参数</a><ul><li><a href=#21-%e5%89%8d%e9%a6%88%e7%bd%91%e7%bb%9c%e4%b8%ad%e7%9a%84%e7%89%b9%e5%be%81%e7%bb%b4%e5%ba%a6 aria-label="2.1. 前馈网络中的特征维度">2.1. 前馈网络中的特征维度</a></li><li><a href=#22-%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%a4%b4%e6%95%b0%e4%b8%8e%e6%af%8f%e5%a4%b4%e7%bb%b4%e5%ba%a6 aria-label="2.2. 注意力头数与每头维度">2.2. 注意力头数与每头维度</a></li><li><a href=#23-%e6%a8%a1%e5%9e%8b%e5%ae%bd%e9%ab%98%e6%af%94aspect-ratio aria-label="2.3. 模型宽高比（aspect ratio）">2.3. 模型宽高比（aspect ratio）</a></li><li><a href=#24-%e5%ad%97%e5%85%b8%e5%a4%a7%e5%b0%8fvocabulary-size aria-label="2.4. 字典大小（vocabulary size）">2.4. 字典大小（vocabulary size）</a></li><li><a href=#25-dropout-%e5%92%8c-%e6%9d%83%e9%87%8d%e8%a1%b0%e5%87%8fweight-decay aria-label="2.5 Dropout 和 权重衰减（weight decay）">2.5 Dropout 和 权重衰减（weight decay）</a></li></ul></li><li><a href=#3-%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83%e7%a8%b3%e5%ae%9a%e6%80%a7%e6%8a%80%e5%b7%a7 aria-label="3. 模型训练稳定性技巧">3. 模型训练稳定性技巧</a><ul><li><a href=#z-loss aria-label=z-loss>z-loss</a></li></ul></li><li><a href=#4-%e6%a8%a1%e5%9e%8b%e7%bb%93%e6%9e%84%e4%bc%98%e5%8c%96 aria-label="4. 模型结构优化">4. 模型结构优化</a><ul><li><a href=#41-kv-cache aria-label="4.1. KV Cache">4.1. KV Cache</a></li><li><a href=#42-mqa-%e5%92%8c-gqa aria-label="4.2. MQA 和 GQA">4.2. MQA 和 GQA</a></li><li><a href=#43-%e7%a8%80%e7%96%8f%e6%b3%a8%e6%84%8f%e5%8a%9bsparse-attention aria-label="4.3. 稀疏注意力（Sparse Attention）">4.3. 稀疏注意力（Sparse Attention）</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h2[id],h3[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>一篇非常棒的<a href=https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison>博客</a>，它总结了几种流行的大模型之间的架构差异</p><h2 id=1-原始transformer-vs-现代变体>1. 原始Transformer vs 现代变体<a hidden class=anchor aria-hidden=true href=#1-原始transformer-vs-现代变体>#</a></h2><p>下表总结了原始Transformer（Vaswani et al., 2017）与现代大语言模型（LLM）中的主流Transformer变体在架构和训练细节上的主要区别：</p><table><thead><tr><th>方面</th><th>原始Transformer (2017)</th><th>现代LLM中的变体</th></tr></thead><tbody><tr><td>归一化顺序</td><td>后归一化（Post-LN）</td><td>前归一化（Pre-LN）</td></tr><tr><td>激活函数</td><td>ReLU</td><td>SwiGLU（GELU、SiLU、Swish等）</td></tr><tr><td>Dropout</td><td>广泛使用</td><td>训练大模型时常常减小或去除</td></tr><tr><td>归一化类型</td><td>LayerNorm</td><td>RMSNorm（LayerNorm、ScaleNorm等）</td></tr><tr><td>线性层</td><td>添加偏置</td><td>不添加偏置</td></tr><tr><td>注意力头</td><td>多头注意力（固定头数）</td><td>GQA、MQA等</td></tr><tr><td>位置编码</td><td>绝对位置编码（sinusoidal）</td><td>RoPE等</td></tr><tr><td>其它</td><td>-</td><td>FlashAttention, MoE, 分层并行等</td></tr></tbody></table><h3 id=11-前归一化-vs-后归一化>1.1. 前归一化 vs 后归一化<a hidden class=anchor aria-hidden=true href=#11-前归一化-vs-后归一化>#</a></h3><p>几乎所有的现代语言模型均采用前归一化（除了BERT），能使训练更加稳定</p><p>左图为前归一化（pre-norm），右图为后归一化（post-norm）</p><img src=pre-post-norm.png alt=pre-vs-post width=300><p><strong>新！</strong>：左图为前归一化（pre-norm），右图为双归一化（&lsquo;double&rsquo; norm，使用者包括 Grok，Gemma 2）</p><img src=pre-post-norm.png alt=pre-vs-post width=300><p><strong>新！</strong>：Olom 2 仅使用非残差部分的后归一化</p><hr><h3 id=12-layernorm-vs-rmsnorm>1.2. LayerNorm vs RMSNorm<a hidden class=anchor aria-hidden=true href=#12-layernorm-vs-rmsnorm>#</a></h3><p>原始 Transformer：<strong>LayerNorm</strong> (GPT3/2/1，OPT，GPT-J，BLOOM)</p><p>$$
y = \frac{x - \textbf{E}[x]}{\sqrt{\textbf{Var}[x] + \epsilon}} * \gamma + \beta
$$</p><p>现代语言模型：<strong>RMSNorm</strong>（LLaMA-family，PaLM，T5）</p><p>$$
y = \frac{x}{\sqrt{\lVert x \rVert^2_2 + \epsilon}} * \gamma
$$</p><p>RMSNorm的优势：运行速度更快，而并不影响精度</p><ul><li>更少的 Operations（无需计算均值）</li><li>更少的参数（没有偏置项需要存储）</li></ul><hr><h3 id=13-ffn有偏置-vs-无偏置>1.3. FFN：有偏置 vs 无偏置<a hidden class=anchor aria-hidden=true href=#13-ffn有偏置-vs-无偏置>#</a></h3><p>原始 Transformer：有偏置</p><p>$$
\textbf{FFN}(x) = \max(0,xW_1+b_1)W_2+b_2
$$</p><p>现代语言模型：无偏置</p><p>$$
\textbf{FFN}(x) = \sigma(xW_1)W_2
$$</p><p>无偏置的优势：更小的存储开销以及稳定的优化</p><hr><h3 id=14-激活函数>1.4. 激活函数<a hidden class=anchor aria-hidden=true href=#14-激活函数>#</a></h3><table><thead><tr><th style=text-align:center>Activation</th><th style=text-align:center>Model</th></tr></thead><tbody><tr><td style=text-align:center>ReLU</td><td style=text-align:center>Original transformer, T5, Gopher, Chinchilla, OPT</td></tr><tr><td style=text-align:center>GeLU</td><td style=text-align:center>GPT1/2/3, GPTJ, GPT-Neox, BLOOM</td></tr><tr><td style=text-align:center>GeGLU</td><td style=text-align:center>T5 v1.1, mT5, LaMDA, Phi3, Gemma 2, Gemma 3</td></tr><tr><td style=text-align:center>SwiGLU</td><td style=text-align:center>LLaMa 1/2/3, PaLM, Mistral, OlMo, most models post 2023</td></tr></tbody></table><p>激活函数的介绍详见 <a href=../activation/>Post</a></p><hr><h3 id=15-位置编码>1.5. 位置编码<a hidden class=anchor aria-hidden=true href=#15-位置编码>#</a></h3><h4 id=151-余弦位置编码sinusoidal-positional-encoding>1.5.1. 余弦位置编码（Sinusoidal Positional Encoding）<a hidden class=anchor aria-hidden=true href=#151-余弦位置编码sinusoidal-positional-encoding>#</a></h4><p><strong>主要思想</strong>：用不同频率的正弦、余弦波来编码不同维度的位置信息</p><p>对于序列中第 $pos$ 个位置、向量维度 $i$，编码方式为:</p><p>$$
\begin{aligned}
PE_{(pos,2i)} = sin(\frac{pos}{10000^{2i/d_{model}}}) \
PE_{(pos,2i+1)} = cos(\frac{pos}{10000^{2i/d_{model}}})
\end{aligned}
$$</p><p>其中：</p><ul><li>$pos$ 表示位置（从0开始）</li><li>$i$ 表示维度索引</li><li>$d_{model}$ 表示模型的隐藏层维度</li><li>10000 是一个经验常数，控制频率范围</li></ul><h5 id=相对位置的捕捉>相对位置的捕捉<a hidden class=anchor aria-hidden=true href=#相对位置的捕捉>#</a></h5><p>假设模型关注两个 token：位置 $pos_1$ 和 $pos_2$ ，那他们的编码分别是：</p><p>$$
\begin{aligned}
E_1 = [sin(\frac{pos_1}{10000^{0}}), cos(\frac{pos_1}{10000^{0}}), sin(\frac{pos_1}{10000^{2/d_{model}}}), cos(\frac{pos_1}{10000^{2/d_{model}}}), \dots ] \\
E_2 = [sin(\frac{pos_2}{10000^{0}}), cos(\frac{pos_2}{10000^{0}}), sin(\frac{pos_2}{10000^{2/d_{model}}}), cos(\frac{pos_2}{10000^{2/d_{model}}}), \dots ]
\end{aligned}
$$</p><p>我们在模型内部做如下操作：</p><p>$$
E_1 \cdot E_2 = sin(\frac{pos_1}{10000^{0}})sin(\frac{pos_2}{10000^{0}}) + cos(\frac{pos_1}{10000^{0}})cos(\frac{pos_2}{10000^{0}}) + \dots
$$</p><p>利用和差化积公式：</p><p>$$
\sin a \sin b + \cos a \cos b = \cos(a - b)
$$</p><p>可得到：</p><p>$$
E_1 \cdot E_2 = \cos(\frac{pos_1 - pos_2}{10000^{0}}) + \cos(\frac{pos_1 - pos_2}{10000^{2/d_{model}}}) + \dots
$$</p><p>即可得到两个位置的相对距离</p><hr><h4 id=152-绝对位置编码absolute-positional-encoding-可学习位置编码learnable-positional-encoding>1.5.2. 绝对位置编码（Absolute Positional Encoding）/ 可学习位置编码（Learnable Positional Encoding）<a hidden class=anchor aria-hidden=true href=#152-绝对位置编码absolute-positional-encoding-可学习位置编码learnable-positional-encoding>#</a></h4><blockquote><p>我个人认为绝对位置编码是一种概念，它表达将 token 的位置信息直接编码，而不是将 token 之间的相对位置进行编码</p></blockquote><p>可学习式位置嵌入<strong>学习一个嵌入矩阵</strong>：</p><p>$$
P = [u_0, u_1, u_2, \dots, u_{L-1}] \in \mathbb{R}^{L \times d_{model}}
$$</p><p>其中：</p><ul><li>$L$ 是最大序列长度</li><li>每一行的 $u_i$ 是位置 $i$ 的可训练嵌入向量</li></ul><p>输入到模型的最终向量：</p><p>$$
Embed(x, i) = v_x + u_i
$$</p><p>其中 $v_x$ 是 token $x$ 的词嵌入向量</p><hr><h4 id=153-相对位置编码relative-positional-encoding>1.5.3. 相对位置编码（Relative Positional Encoding）<a hidden class=anchor aria-hidden=true href=#153-相对位置编码relative-positional-encoding>#</a></h4><p>绝对位置编码的缺点：</p><ul><li>泛化差：训练时的序列长度是固定的，比如 512；超出这个长度就无法使用</li><li>缺乏相对感知：模型知道第 5 个词、第 10 个词，但不知道它们“相隔 5 个位置”</li></ul><p>然而自然语言的顺序关系往往是相对的：</p><blockquote><p>“the cat” 和 “the big cat” 的依赖关系中，“cat” 距离 “the” 只有几步之差</p></blockquote><p>因此，相对位置编码的目标是：让模型直接学习 “第 i 个 token 与第 j 个 token 的距离（i−j）” 对注意力的影响</p><p>相对位置编码通过在注意力打分中显式地加入位置差信息:</p><p>$$
e_{ij} = \frac{(x_i W_Q)(x_j W_K + a^K_{ij})^T}{\sqrt{d_k}}
$$</p><p>其中 $a_{ij}^K$ 是一个向量，表示 token i 和 token j 之间的相对位置信息</p><hr><h4 id=154-roperotary-position-embedding>1.5.4. RoPE（Rotary Position Embedding）<a hidden class=anchor aria-hidden=true href=#154-roperotary-position-embedding>#</a></h4><p>我们该如何让 <strong>添加位置编码后的嵌入向量</strong> $x$ 和 $y$ 在完成点积后，只关注它们的相对位置呢？也就是要实现如下目标：</p><div id=eq:goal>$$
\langle f(x, i), f(y, j) \rangle = g(x, y, i-j) \tag{1}
$$</div><p>其中 $\langle \cdot, \cdot \rangle$ 表示内积运算</p><ul><li>余弦位置编码：不满足<a href=#eq:goal>（1）</a>式:<ul><li>在余弦位置编码中，token 的嵌入可以表征为 $Embed(x, i) = v_x + E_i$，其中 $v_x$ 是 token 的词嵌入向量，$E_i$ 是位置编码;</li><li>$\langle Embed(x, i), Embed(y, j) \rangle = \langle v_x + E_i, v_y + E_j \rangle = \langle v_x, v_y \rangle + \langle v_x, E_j \rangle + \langle E_i, v_y \rangle + \langle E_i, E_j \rangle$，有许多内积项依赖于绝对位置 $i$ 和 $j$，而不仅仅是它们的差值 $i-j$</li></ul></li><li>绝对位置编码：不满足<a href=#eq:goal>（1）</a>式</li><li>相对位置编码：不满足<a href=#eq:goal>（1）</a>式中的内积形式：<ul><li>几何解释消失：原来的点积可以看成两个向量夹角的余弦相似度（几何上可解释），加入 $a_{ij}$ 后，这个解释就失效</li><li>对称性破坏：$e_{ij}$ 与 $e_{ji}$ 不再一致，使模型捕捉方向信息</li><li>注意力的归一化解释变弱：softmax 之前的 logits 不再仅由向量相似度决定，额外偏置可能干扰注意力稳定性</li></ul></li></ul><p>RoPE 的核心思想是：通过复数旋转（或二维平面旋转）把位置嵌入到每个向量维度中</p><img src=rope_example.png alt=rope-example width=400><p>$$
\begin{aligned}
x_p &= [x_{p,0}, x_{p,1}, \dots, x_{p,d-1}]\\
f_{{q,k}}(x_p,p) &= \mathbf{R}^d_{\Theta,p},W_{{q,k}},x_p\\
\mathbf{R}^d_{\Theta,p}
&=
\begin{bmatrix}
\cos(p\theta_0) & -\sin(p\theta_0) & & & \\
\sin(p\theta_0) & \cos(p\theta_0) & & & \\
& & \cos(p\theta_1) & -\sin(p\theta_1) & \\
& & \sin(p\theta_1) & \cos(p\theta_1) & \\
& & & & \ddots \\
& & & & & \cos(p\theta_{d/2-1}) & -\sin(p\theta_{d/2-1}) \\
& & & & & \sin(p\theta_{d/2-1}) & \cos(p\theta_{d/2-1})
\end{bmatrix}
\end{aligned}
$$</p><p>其中 $\theta_k = 10000^{-2k/d}$</p><p>接下来证明RoPE符合<a href=#eq:goal>（1）</a>式：</p><p>对于 token $x$ 的两个相邻嵌入维度 $2i$ 和 $2i+1$，有：</p><p>$$
\tilde{x}_{p}^{(k)} = \begin{bmatrix}
cos(p\theta_k) & -sin(p\theta_k) \\
sin(p\theta_k) & cos(p\theta_k)
\end{bmatrix}
\begin{bmatrix}
x_{p,2k} \\
x_{p,2k+1}
\end{bmatrix}
=(x_{p,2k}+ix_{p,2k+1})e^{ip\theta_k}
$$</p><p>因此有：</p><p>$$
\langle \tilde{x}_{p}^{(k)}, \tilde{y}_{q}^{(k)} \rangle = \tilde{x}_{p}^{(k)} \cdot \overline{\tilde{y}}_{q}^{(k)} = (x_{p,2k}+ix_{p,2k+1})(y_{q,2k}-iy_{q,2k+1})e^{i(p-q)\theta_k}
$$</p><p>符合<a href=#eq:goal>（1）</a>式中的要求</p><hr><h2 id=2-超参数>2. 超参数<a hidden class=anchor aria-hidden=true href=#2-超参数>#</a></h2><h3 id=21-前馈网络中的特征维度>2.1. 前馈网络中的特征维度<a hidden class=anchor aria-hidden=true href=#21-前馈网络中的特征维度>#</a></h3><p>假设 $d_{ff}$ 是前馈网络的隐藏层维度，$d_{model}$ 是模型的隐藏层维度</p><p>$$
d_{ff} = 4d_{model}
$$</p><p>此时，标准 FFN 的参数量为：</p><ul><li>第一层：$d_{model} \times d_{ff} = 4d_{model}^2$</li><li>第二层：$d_{ff} \times d_{model} = 4d_{model}^2$</li><li>总计：$8d_{model}^2$</li></ul><p>对于<strong>包含GLU类激活函数</strong>的 FFN，参数量为：</p><ul><li>GLU中的content部分：$d_{model} \times d&rsquo;_{ff}$</li><li>GLU中的gate部分：$d_{model} \times d&rsquo;_{ff}$</li><li>第二层：$d&rsquo;_{ff} \times d_{model}$</li><li>总计：$3d_{model} \times d&rsquo;_{ff}$</li></ul><p>为了使包含GLU类激活函数的 FFN 与标准 FFN 的参数量相同，我们需要满足：</p><p>$$
3d_{model} \times d&rsquo;_{ff} = 8d_{model}^2
$$</p><p>即</p><p>$$
d&rsquo;_{ff} = \frac{8}{3}d_{model}
$$</p><p>下表总结了一些流行大模型中前馈网络隐藏层维度与模型隐藏层维度的比值：</p><table><thead><tr><th>Model</th><th>$( d_{ff} / d_{model} )$</th></tr></thead><tbody><tr><td>PaLM</td><td>4.00</td></tr><tr><td>Mistral 7B</td><td>3.50</td></tr><tr><td>LLaMA-2 70B</td><td>3.50</td></tr><tr><td>LLaMA 70B</td><td>2.68</td></tr><tr><td>Qwen 14B</td><td>2.67</td></tr><tr><td>DeepSeek 67B</td><td>2.68</td></tr><tr><td>Yi 34B</td><td>2.85</td></tr><tr><td>T5 v1.1</td><td>2.50</td></tr></tbody></table><hr><h3 id=22-注意力头数与每头维度>2.2. 注意力头数与每头维度<a hidden class=anchor aria-hidden=true href=#22-注意力头数与每头维度>#</a></h3><p>我们尽量使得 $d_{head} > d_{model} / num_{heads}$，很多模型选择令 $d_{head} = d_{model} / num_{heads}$</p><table><thead><tr><th>Model</th><th>Num heads</th><th>Head dim</th><th>Model dim</th><th>Ratio</th></tr></thead><tbody><tr><td>GPT3</td><td>96</td><td>128</td><td>12288</td><td>1</td></tr><tr><td>T5</td><td>128</td><td>128</td><td>1024</td><td>16</td></tr><tr><td>T5 v1.1</td><td>64</td><td>64</td><td>4096</td><td>1</td></tr><tr><td>LaMDA</td><td>128</td><td>128</td><td>8192</td><td>2</td></tr><tr><td>PaLM</td><td>48</td><td>258</td><td>18432</td><td>1.48</td></tr><tr><td>LLaMA2</td><td>64</td><td>128</td><td>8192</td><td>1</td></tr></tbody></table><hr><h3 id=23-模型宽高比aspect-ratio>2.3. 模型宽高比（aspect ratio）<a hidden class=anchor aria-hidden=true href=#23-模型宽高比aspect-ratio>#</a></h3><p>这里的宽高比指的是：</p><p>$$
d_{model} / num_{layers}
$$</p><table><thead><tr><th>Model</th><th>( d_{model} / n_{layer} )</th></tr></thead><tbody><tr><td>BLOOM</td><td>205</td></tr><tr><td>T5 v1.1</td><td>171</td></tr><tr><td>PaLM (540B)</td><td>156</td></tr><tr><td>GPT3 / OPT / Mistral / Qwen</td><td>128</td></tr><tr><td>LLaMA / LLaMA2 / Chinchila</td><td>102</td></tr><tr><td>T5 (11B)</td><td>43</td></tr><tr><td>GPT2</td><td>33</td></tr></tbody></table><p>太深的模型很难并行化，并且具有很高的延迟</p><img src=parallel.png alt=model-parallelism width=400><hr><h3 id=24-字典大小vocabulary-size>2.4. 字典大小（vocabulary size）<a hidden class=anchor aria-hidden=true href=#24-字典大小vocabulary-size>#</a></h3><ul><li>单语言：3-5万个 token</li><li>多语言：10-25万个 token</li></ul><hr><h3 id=25-dropout-和-权重衰减weight-decay>2.5 Dropout 和 权重衰减（weight decay）<a hidden class=anchor aria-hidden=true href=#25-dropout-和-权重衰减weight-decay>#</a></h3><ul><li>老模型会更多的采用Dropout；</li><li>新模型会更多的采用权重衰减，其作用更多的在于与loss的互动（后期更快的loss下降），而非防止过拟合</li></ul><table><thead><tr><th>Model</th><th>Dropout*</th><th>Weight decay</th></tr></thead><tbody><tr><td>Original transformer</td><td>0.1</td><td>0</td></tr><tr><td>GPT2</td><td>0.1</td><td>0.1</td></tr><tr><td>T5</td><td>0.1</td><td>0</td></tr><tr><td>GPT3</td><td>0.1</td><td>0.1</td></tr><tr><td>T5 v1.1</td><td>0</td><td>0</td></tr><tr><td>PaLM</td><td>0</td><td>(variable)</td></tr><tr><td>OPT</td><td>0.1</td><td>0.1</td></tr><tr><td>LLaMA</td><td>0</td><td>0.1</td></tr><tr><td>Qwen 14B</td><td>0.1</td><td>0.1</td></tr></tbody></table><img src=weight_decay_effect.png alt=weight-decay-effect width=400><hr><h2 id=3-模型训练稳定性技巧>3. 模型训练稳定性技巧<a hidden class=anchor aria-hidden=true href=#3-模型训练稳定性技巧>#</a></h2><p>模型的训练，应当避免出现“尖刺”，如下图蓝色曲线所示：</p><img src=stability.png alt=training-stability-techniques width=600><h3 id=z-loss>z-loss<a hidden class=anchor aria-hidden=true href=#z-loss>#</a></h3><img src=softmax_in_llm.png alt=softmax_in_llm width=200><p>观察出现在 LLM 的最后一层的softmax，softmax 的定义为：</p><p>$$
P(y=i|x) = \frac{e^{z_i}}{\sum_j e^{z_j}} = \frac{e^{z_i}}{Z}
$$</p><p>因此在 Cross-Entropy Loss 中，我们有：</p><p>$$
Loss_{CE} = -\log P(y=i|x) = -\log \frac{e^{z_i}}{\sum_j e^{z_j}} = -z_i + \log Z
$$</p><p>当 $Z$ 为0时，会导致 $Loss_{CE}$ 过大，造成训练的不稳定</p><p>因此我们想办法，令 $Z$ 趋近于 1，即 $\log Z$ 趋近于 0，我们可以添加一个 z-loss 项：</p><p>$$
Loss_{z} = ((\log Z)^2 - 0)^2 = (\log Z)^2
$$</p><p>最终有：</p><p>$$
Loss = Loss_{CE} + \lambda Loss_{z}
$$</p><p>其中 $\lambda$ 是一个很小的值，一般为 $1e-3$ 或 $1e-4$</p><hr><h2 id=4-模型结构优化>4. 模型结构优化<a hidden class=anchor aria-hidden=true href=#4-模型结构优化>#</a></h2><h3 id=41-kv-cache>4.1. KV Cache<a hidden class=anchor aria-hidden=true href=#41-kv-cache>#</a></h3><img src=kv_cache.gif alt=kv-cache width=600><p>图片来源于<a href=https://medium.com/@joaolages/kv-caching-explained-276520203249>网络</a></p><p><strong>常规的注意力计算</strong>：</p><p>$$
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
$$</p><p>假设 $X \in \mathbb{R}^{b \times T \times D}$，$W_{\{Q, K, V\}} \in \mathbb{R}^{D \times (hd)}$，其中 $T$ 是序列长度，$h$ 为注意力头数，$d$ 是每个注意力头的隐藏层维度，设 $D = hd$，则计算量为</p><ul><li>计算KQV：$3 \times 2bTD^2 = 6bT(hd)^2$</li><li>计算$Q \times K$：$2bhT^2d$</li><li>计算softmax：$n \times bhT^2$（softmax包含 n 次计算操作）</li><li>计算$Output_{softmax} \times V$：$2bhT^2d$</li><li>计算输出线性层：$2bTD^2$</li><li>总计算量 $\approx 8bTD^2 + 4bhT^2d$（忽略softmax）</li></ul><p>总存储开销为</p><ul><li>权重参数开销：<ul><li>$W_{\{Q, K, V\}}$ 存储开销：$3 \times D(hd) = 3(hd)^2$</li><li>输出线性层存储开销：$(hd)D = (hd)^2$</li></ul></li><li>中间激活开销：<ul><li>输入存储：$bTD$</li><li>KQV存储：$3 \times bhTd$</li><li>softmax后得到的注意力权重存储：$bhT^2$</li><li>输出存储：$bTD$ （即下一层的输入，不计入本层开销）</li></ul></li><li>总存储开销 $\approx 4(hd)^2 + bTD + 3bhTd + bhT^2$</li></ul><p><strong>使用 KV Cache 的注意力计算</strong>：</p><p>训练时，使用 KV Cache 并不影响计算量，因此训练时往往不使用 KV Cache；</p><p>但在推理时，假设输入序列长度为 $t$，则预测下一个 token 时，计算量为 $\approx 8btD^2 + 4bht^2d$，若不使用 KV Cache，则预测下下个 token 的计算量为 $\approx 8b(t+1)D^2 + 4bh(t+1)^2d$，这个计算量会随着序列长度的增加而显著增加；</p><p>而使用 KV Cache 后，预测下一个 token 的计算量为：</p><ul><li>KQV计算：由于无需重新计算$Q_{1:(t-1)}, K_{1:(t-1)}, V_{1:(t-1)}$，只需要计算$Q_{t}, K_{t}, V_{t}$，因此计算量为$3 \times 2bD^2$</li><li>计算$Q_{t} \times K_{1:t}$：$2bhtd$</li><li>计算softmax：$n \times bht$</li><li>计算$Output_{softmax} \times V_{1:t}$：$2bhtd$</li><li>计算输出线性层：$2bD^2$</li><li>总计算量 $\approx 8bD^2 + 4bhtd$（忽略softmax）</li></ul><p>计算量随序列长度的增加轻微增加。</p><p>而使用 KV Cache 后，存储开销会稍稍增加。因为在推理阶段不使用 KV Cache，总存储开销仅为权重参数开销 $4(hd)^2$，而使用 KV Cache 后，总存储开销为权重参数开销 $4(hd)^2$ 加上 KV Cache 的存储开销 $2bhtd$，因此总存储开销 $\approx 4(hd)^2 + 2bhtd$。</p><hr><h3 id=42-mqa-和-gqa>4.2. MQA 和 GQA<a hidden class=anchor aria-hidden=true href=#42-mqa-和-gqa>#</a></h3><p>为了减少 KV Cache 的存储开销，一个简单的思路就是让<strong>注意力头</strong>共享 K 和 V</p><ul><li>若 $h$ 个注意力头共享一个 K 和 V，则为 MQA（Multi-query Attention）</li><li>若将 $h$ 个注意力头划分为 $g$ 组，每组 $h/g$ 个头共享 K 和 V，则为 GQA（Grouped-query Attention）</li></ul><p>如下图所示：</p><img src=attention_variant.png alt=attention-variants width=600><table><thead><tr><th>模型</th><th>训练时结构</th><th>推理时结构</th><th>备注</th></tr></thead><tbody><tr><td>GPT-3 / GPT-4</td><td>MHA</td><td>MHA / GQA（部分优化版）</td><td>GPT-4 reportedly uses GQA</td></tr><tr><td>PaLM 2</td><td>GQA</td><td>GQA</td><td>原生训练结构</td></tr><tr><td>Claude 3</td><td>GQA</td><td>GQA</td><td>Anthropic 公开结构说明</td></tr><tr><td>LLaMA 2</td><td>MHA</td><td>GQA (converted)</td><td>Meta 后期转换版</td></tr><tr><td>Mistral</td><td>GQA</td><td>GQA</td><td>端到端使用 GQA</td></tr><tr><td>Falcon</td><td>MQA</td><td>MQA</td><td>优化长序列推理</td></tr><tr><td>Gemini 1.5</td><td>GQA</td><td>GQA</td><td>Google 用于多模态大模型</td></tr></tbody></table><p>MQA 和 GQA 与 MHA 相比，计算复杂度不变，但存储开销减少，假设 $h$ 个注意力头共享 $k$ 个 K 和 V：</p><ul><li>权重参数开销：<ul><li>$W_{\{Q\}}$ 存储开销：$D(hd) = (hd)^2$</li><li>$W_{\{K, V\}}$ 存储开销：$2 \times D(kd) = 2(hd)(kd)$</li><li>输出线性层存储开销：$(hd)D = (hd)^2$</li></ul></li><li>中间激活开销：<ul><li>输入存储：$bTD$</li><li>Q存储：$bhTd$</li><li>KV存储：$2bkdT$</li><li>softmax后得到的注意力权重存储：$bhT^2$</li><li>输出存储：$bTD$ （即下一层的输入，不计入本层开销）</li></ul></li><li>总存储开销 $\approx 2(hd)^2 + 2hkd^2 + 2bTD + 2bkdT + bhT^2$</li></ul><hr><h3 id=43-稀疏注意力sparse-attention>4.3. 稀疏注意力（Sparse Attention）<a hidden class=anchor aria-hidden=true href=#43-稀疏注意力sparse-attention>#</a></h3><p>稀疏注意力（Sparse Attention）：可以参考<a href=https://newsletter.theaiedge.io/p/understanding-the-sparse-transformers>博客</a></p><img src=sparse_attention.png alt=sparse-attention><hr><div class=zhihu-ref><div class=zhihu-ref-title>参考文献</div><ol><li><a href=https://github.com/stanford-cs336/spring2025-lectures/blob/e9cb2488fdb53ea37f0e38924ec3a1701925cef3/nonexecutable/2025%20Lecture%203%20-%20architecture.pdf target=_blank>stanford-cs336 lecture 3</a></li></ol></div></div><br><div><div class=copyright-card><div class=copyright-card-main><div class=copyright-info><h3 class=copyright-title>LM Architecture and Training</h3><p class=copyright-link><a href=https://cspaulia.github.io/cspaulia-blog/posts/transformer_in_llm/>https://cspaulia.github.io/cspaulia-blog/posts/transformer_in_llm/</a></p><div class=copyright-meta><span><strong>Author</strong><br>CSPaulia</span>
<span><strong>Published at</strong><br>October 10, 2025</span>
<span><strong>Copyright</strong><br><a rel=license href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank>CC BY-NC-SA 4.0</a></span></div></div><div class=copyright-icon>©</div></div></div></div><footer class=post-footer><p style=font-size:medium;margin-bottom:5px;font-weight:700>categories</p><ul class=post-tags><li><a href=https://cspaulia.github.io/cspaulia-blog/categories/large-language-model/>Large Language Model</a></li></ul><p style=font-size:medium;margin-bottom:5px;font-weight:700>tags</p><ul class=post-tags><li><a href=https://cspaulia.github.io/cspaulia-blog/tags/llm/>LLM</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/tags/training/>Training</a></li></ul><nav class=paginav><a class=next href=https://cspaulia.github.io/cspaulia-blog/posts/flow/><span class=title>Next »</span><br><span>流匹配与扩散模型</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share LM Architecture and Training on x" href="https://x.com/intent/tweet/?text=LM%20Architecture%20and%20Training&amp;url=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fposts%2ftransformer_in_llm%2f&amp;hashtags=LLM%2cTraining"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LM Architecture and Training on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fposts%2ftransformer_in_llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LM Architecture and Training on telegram" href="https://telegram.me/share/url?text=LM%20Architecture%20and%20Training&amp;url=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fposts%2ftransformer_in_llm%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://cspaulia.github.io/cspaulia-blog/>cspaulia-blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>