<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>SFT and RLHF | cspaulia-blog</title><meta name=keywords content="SFT,Reinforcement Learning,RLHF"><meta name=description content="Introduction of SFT and RLHF."><meta name=author content="CSPaulia"><link rel=canonical href=https://cspaulia.github.io/cspaulia-blog/posts/sft_rlhf/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><link crossorigin=anonymous href=/cspaulia-blog/assets/css/stylesheet.fde8f6c2e9953ef5c9e36120a9b34726a3fd3201d621f8855b216db33fc435f5.css integrity="sha256-/ej2wumVPvXJ42EgqbNHJqP9MgHWIfiFWyFtsz/ENfU=" rel="preload stylesheet" as=style><link rel=icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://cspaulia.github.io/cspaulia-blog/posts/sft_rlhf/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1,trust:!0,strict:!1})})</script><meta property="og:url" content="https://cspaulia.github.io/cspaulia-blog/posts/sft_rlhf/"><meta property="og:site_name" content="cspaulia-blog"><meta property="og:title" content="SFT and RLHF"><meta property="og:description" content="Introduction of SFT and RLHF."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-29T11:30:03+08:00"><meta property="article:modified_time" content="2025-11-04T17:58:25+08:00"><meta property="article:tag" content="SFT"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="RLHF"><meta property="og:image" content="https://cspaulia.github.io/cspaulia-blog/posts/sft_rlhf/cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cspaulia.github.io/cspaulia-blog/posts/sft_rlhf/cover.jpg"><meta name=twitter:title content="SFT and RLHF"><meta name=twitter:description content="Introduction of SFT and RLHF."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://cspaulia.github.io/cspaulia-blog/posts/"},{"@type":"ListItem","position":2,"name":"SFT and RLHF","item":"https://cspaulia.github.io/cspaulia-blog/posts/sft_rlhf/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"SFT and RLHF","name":"SFT and RLHF","description":"Introduction of SFT and RLHF.","keywords":["SFT","Reinforcement Learning","RLHF"],"articleBody":"1. Post Training 三阶段 来源于 InstructGPT[1]\n收集数据并训练监督策略 从提示词数据集中采样一个提示词 标注者对数据进行标注（该标注即为期望输出） 该标注数据用于对 LLM 进行监督训练 收集对比数据并训练奖励模型 采样一个提示词及多个模型的输出 标注者从“最好”到“最差”对这些输出进行排序 该标注数据用于对奖励模型进行训练 在训练好的奖励模型的加持下利用强化学习优化策略 从数据集中采用一个新的提示词 利用策略生成一个输出 奖励模型计算输出的奖励分数（Reward） 根据奖励分数利用 PPO 等强化学习方法对策略进行更新 2. SFT 数据集的构建 2.1. 数据集中的问题 来自 FLAN 数据集的一个例子:\nWhat is this text about? OPTIONS: - World - Sport - Business - Science/Tech\n自然的对话一般不包含这样的选项 来自 OpenAssistant 的例子:\nQuestion: Can you write a short introduction about the relevance of the term\"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\nAnswer: “Monopsony” refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. […]. Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue. References: Bivens, J., \u0026 Mishel, L.(2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.\n人类标注者很难撰写如此长而详细的答案 输出中的符号（甚至风格）如何抉择 是否输出参考文献 如何输出准确的参考文献：我们往往希望模型能够输出更加严谨、学术的输出，因此我们会在数据集的回答中加入参考文献，但是模型可能只学到了“输出内容要加上参考文献”，这会导致模型产生幻觉（Hallucination） 是否输出复杂的知识 数据集的规模大小 数据集的安全问题 如何决定模型输出的长短：通过下图你可以发现，不同指令数据的输入和输出长度区别是很大的 2.2. SFT 数据集构建经验 SFT 在模型已经具备某些能力的前提下，通过数据来“抽取”这些能力的表现效果最好；但如果试图用 SFT 去“添加”模型原本不具备的新行为，往往效果不佳 并不是所有事实正确的数据都会提升模型表现，有时即使是高质量的事实数据，也可能干扰模型已有的分布或对齐，反而让性能下降 某些类型的数据（例如安全性、遵循指令、风格等）哪怕只有少量，也能对模型带来巨大提升，不过，模型的长尾行为（覆盖面广、稀疏分布的场景）则更依赖于大量数据来改善 2.3. 在预训练中使用指令微调（Instruction Tuning） 在网页或预训练数据集上进行预训练 将指令微调数据混入预训练中 额外进行一个简短的指令微调 2.4. ‘Midtraining’ / ‘Two-phase Training’ 这种方案[2]好似已被大部分 LLM 公司采纳（但没有详细的文档）：\n在 Stable 阶段采用纯预训练数据集训练（如上图左侧所示）； 在 Decay 阶段采用预训练+指令微调混合数据集进行训练（如上图右侧所示） 3. RLHF（Reinforcement Learning with Human Feedback） 3.1. 从模仿（Imitation）到优化（optimization） 模仿（Imitation，即SFT）：根据一些参考分布 $p^*(y|x)$ 调整模型的输出分布，使得输出分布 $\\hat{p}(y|x) \\approx p^*(y|x)$\n从纯生成建模的角度看，SFT 就是让生成模型学会模仿参考分布 训练需要来自参考策略的数据（比如人工标注的数据集），否则无法“模仿” 优化（Optimization，即RLHF）：不断调整输出分布$\\hat{p}(y|x)$，使得 $\\max\\limits_{p} E_p[R(y,x)]$，其中 $R(y,x)$ 为奖励（Reward）\n优化的不是抽象的“真实分布”，而是一个我们能定义并测量的奖励函数（在 RLHF 里，这个奖励函数来自人类反馈，比如人类排序、偏好比较，或者训练出的 reward model） 在这个阶段，我们不再把语言模型看作“近似真实分布$p^*(y|x)$的模型”（像 SFT 那样），而是把它当作一个 策略 policy，用来最大化奖励信号 3.2. 需要RLHF的原因 成本：SFT 的成本非常高昂，尤其是标注成本 G-V Gap (Generation-Value Gap)：人们写的内容（生成分布 G）和 人们实际偏好的内容（价值模型 V 的视角）并不总是一致 在一项过去的实验[3]中发现，一些标注者在对比自己写的摘要和模型写的摘要时，有时会更喜欢模型写的摘要，也就是说人类标注并不是最优的\n3.3. 如何获得RLHF的数据 方案一：（在网页上）让模型输出 N 个结果，让标注者（用户）对结果进行排序\n存在的问题： 标注结果可能是低质量的、不正确的，甚至是用大预言模型生成的，这取决于标注者本身 RLHF 标注者的分布会极大的影响模型的行为 如果标注者中亚洲人多，则模型会倾向于输出亚洲风格的结果\n不同的标注者关注点不同 有些标注者更关注格式，有些标注者更关注内容\n方案二：使用大语言模型（如GPT-4）对模型（可以是多个不同的模型）输出的 N 个结果进行排序，我们常常称之为 AI Feedback\n3.4. 实现RLHF的方法 3.4.1. 人类反馈下的PPO（PPO with Human Feedback） 原始的PPO算法 = Policy Gradient + Off Policy，其优化目标为：\n$$ \\max L^{CLIP}(\\theta) - \\beta KL[\\pi_\\theta||\\pi_{old}] $$\n其中$L^{CLIP}(\\theta) = \\mathbb{E}_t \\Big[ \\min \\big( r_t(\\theta) \\hat{A}_t,; \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t \\big) \\Big]$，\n其中$r_t(\\theta) = \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)}$，\n人类反馈下的PPO的优化目标为：\n$$ L^{RLHF}(\\theta) = \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) ] - \\beta D_{KL} \\left[ \\pi(y|x) || \\pi_{\\text{ref}}(y|x) \\right] $$\n其中，\n$r(x,y)$：是奖励模型对输出$y$在输入$x$下的奖励评分 $\\pi(y∣x)$：是当前训练的策略（即RLHF中训练的模型），表示在输入$x$下生成$y$的概率分布 $\\pi_{\\text{ref}}(y∣x)$：是参考策略，通常是一个冻结的 SFT 模型，用于对比当前模型的生成结果，保证不会过度偏离基准模型 $D_{KL} \\left[ \\pi(y|x) || \\pi_{\\text{ref}}(y|x) \\right]$：是当前策略与参考策略之间的 KL 散度，用于约束新策略不能偏离参考策略太远 问题一：为什么传统PPO中优化$\\mathbb{E}_t [r_t(\\theta) \\hat{A}_t]$，而RLHF优化$\\mathbb{E}_{x, y} [ r(x,y) ]$？\n在传统的强化学习任务中，我们往往会从离线策略中获得优势函数$\\hat{A}_t$，因此优化目标为$\\max \\mathbb{E}_t [r_t(\\theta) \\hat{A}_t]$ 在LLM基于人类反馈的强化学习任务，我们选择利用奖励模型（Reward model）直接对在线的LLM的输出进行打分，因此无需使用$r_t(\\theta)$进行重要性采样修正 问题二：传统PPO中的$\\pi_{old}$和RLHF PPO中的$\\pi_{\\text{ref}}$有什么区别？\n$\\pi_{old}$：上一次迭代的策略，用于收集数据和做重要性采样，会不断更新，通常就是“前一版策略” $\\pi_{\\text{ref}}$：一个固定的参考策略，一般是 SFT（监督微调模型），也就是在 RLHF 训练前冻结的模型，不会更新 3.4.2. DPO DPO训练目标与PPO一致：\n$$ \\max L^{RLHF}(\\theta) = \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) ] - \\beta D_{KL} \\left[ \\pi(y|x) || \\pi_{\\text{ref}}(y|x) \\right] $$\n由该优化目标可得最优Policy为：\n$$ \\pi(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y)) $$\n其中，$Z(x) = \\sum\\limits_y \\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))$\n点击展开公式推导 $$ \\begin{align} \u0026\\max\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) ] - \\beta D_{KL} \\left[ \\pi(y|x) || \\pi_{\\text{ref}}(y|x) \\right] \\\\ = \u0026\\max\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) ] - \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\beta \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} ] \\\\ = \u0026\\max\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ r(x,y) - \\beta \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} ] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\beta \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - r(x,y) ] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)} - \\log \\exp(\\frac{1}{\\beta}r(x,y)) ] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))}] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))\\frac{1}{Z(x)} Z(x)}] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))} - \\log Z(x)] \\\\ \\end{align} $$\n其中$Z(x) = \\sum\\limits_y \\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))$，\n令$\\pi^\\star(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y)) = \\frac{\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))}{\\sum\\limits_y \\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))}$，则有\n$$ \\begin{align} \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))} - \\log Z(x)] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi^\\star(y|x)} - \\log Z(x)] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [ \\log \\frac{\\pi(y|x)}{\\pi^\\star(y|x)}] \\\\ = \u0026\\min\\limits_\\pi \\mathbb{E}_{x \\sim D, y \\sim \\pi} [D_{KL} (\\pi(y|x) || \\pi^\\star(y|x))] \\\\ \\end{align} $$\n最优解为$\\pi(y|x) = \\pi^\\star(y|x) = \\frac{1}{Z(x)}\\pi_{\\text{ref}}(y|x)\\exp(\\frac{1}{\\beta}r(x,y))$\n将最优policy带入Bradley-Terry模型建模的最大似然估计中，即可得到DPO损失函数为：\n$$ Loss_{DPO} = - \\ln \\sigma \\left( \\beta \\ln \\frac{\\pi(y^+ \\mid x)}{\\pi_{ref}(y^+ \\mid x)} - \\beta \\ln \\frac{\\pi(y^- \\mid x)}{\\pi_{ref}(y^- \\mid x)} \\right) $$\n其中$y^+$：是 人类偏好（或高质量的生成样本），是被标注为“更优”的样本，$y^-$：是 负偏好（或低质量的生成样本），是被标注为“较差”的样本\n点击展开Bradley-Terry模型简介 Bradley-Terry模型：\n$$ P(i\u003ej) = \\frac{\\alpha_i}{\\alpha_i + \\alpha_j} $$\n$P(i\u003ej)$表示第$i$个元素战胜第$j$个元素的概率，一般的loss函数为\n$$ Loss = -\\mathbb{E}_{(\\alpha_x, \\alpha_y) \\sim D} [ \\ln \\frac{\\alpha_i}{\\alpha_i + \\alpha_j}] $$\n大模型中的Bradley-Terry模型：\n$$ P(y_1\u003ey_2) = \\frac{r(x,y_1)}{r(x,y_1) + r(x,y_2)} $$\n其中$x$为输入的prompt，$y$为输出，$r(x,y)$为奖励得分，为防止$r(x,y)$为负数，加入指数函数：\n$$ P(y_1\u003ey_2) = \\frac{\\exp(r(x,y_1))}{\\exp(r(x,y_1)) + \\exp(r(x,y_2))} $$\n其Loss为\n$$ \\begin{align} \\text{Loss} \u0026= - \\mathbb{E}_{(x, y^+, y^-) \\sim D} [ \\ln ( \\frac{\\exp(r(x, y^+))}{\\exp(r(x, y^+)) + \\exp(r(x, y^-))} ) ] \\\\ \u0026= - \\mathbb{E}_{(x, y^+, y^-) \\sim D} [ \\ln ( \\frac{1}{1 + \\exp(r(x, y^-)) - r(x, y^+)} ) ] \\\\ \u0026= - \\mathbb{E}_{(x, y^+, y^-) \\sim D} [ \\ln \\sigma ( r(x, y^+) - r(x, y^-) ) ] \\end{align} $$\n其中$\\sigma (x) = \\frac{1}{1+\\exp(-x)}$为sigmoid函数\n点击展开公式推导 $$ \\begin{align} \u0026\\pi(y \\mid x) = \\frac{1}{Z(x)} \\pi_{ref}(y \\mid x) \\exp( \\frac{1}{\\beta} r(x, y)) \\\\ \\Rightarrow \u0026\\exp( \\frac{1}{\\beta} r(x, y) ) = \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} Z(x) \\\\ \\Rightarrow \u0026r(x, y) = \\beta \\ln ( \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} Z(x) ) \\\\ \\Rightarrow \u0026r(x, y) = \\beta \\ln ( \\frac{\\pi(y \\mid x)}{\\pi_{ref}(y \\mid x)} ) + \\beta \\ln Z(x) \\\\ \\end{align} $$\n将奖励评分送入Bradley-Terry模型得到：\n$$ \\begin{align} Loss \u0026 = - \\ln \\sigma ( r(x, y^+) - r(x, y^-) ) \\\\ \u0026 = - \\ln \\sigma (\\beta \\ln ( \\frac{\\pi(y^+ \\mid x)}{\\pi_{ref}(y^+ \\mid x)} ) + \\beta \\ln Z(x) - \\beta \\ln ( \\frac{\\pi(y^- \\mid x)}{\\pi_{ref}(y^- \\mid x)} ) - \\beta \\ln Z(x)) \\\\ \u0026 = - \\ln \\sigma (\\beta \\ln ( \\frac{\\pi(y^+ \\mid x)}{\\pi_{ref}(y^+ \\mid x)} ) - \\beta \\ln ( \\frac{\\pi(y^- \\mid x)}{\\pi_{ref}(y^- \\mid x)} )) \\end{align} $$\n参考文献 Ouyang L, Wu J, Jiang X, et al. Training language models to follow instructions with human feedback[J]. Advances in neural information processing systems, 2022, 35: 27730-27744. Hu S, Tu Y, Han X, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies[J]. arXiv preprint arXiv:2404.06395, 2024. Zhang T, Ladhak F, Durmus E, et al. Benchmarking large language models for news summarization[J]. Transactions of the Association for Computational Linguistics, 2024, 12: 39-57. stanford-cs336 lecture 15 ","wordCount":"994","inLanguage":"en","image":"https://cspaulia.github.io/cspaulia-blog/posts/sft_rlhf/cover.jpg","datePublished":"2025-09-29T11:30:03+08:00","dateModified":"2025-11-04T17:58:25+08:00","author":{"@type":"Person","name":"CSPaulia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://cspaulia.github.io/cspaulia-blog/posts/sft_rlhf/"},"publisher":{"@type":"Organization","name":"cspaulia-blog","logo":{"@type":"ImageObject","url":"https://cspaulia.github.io/cspaulia-blog/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://cspaulia.github.io/cspaulia-blog/ accesskey=h title="Home (Alt + H)"><img src=https://cspaulia.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://cspaulia.github.io/cspaulia-blog/publications/ title=publications><span>publications</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/posts/ title=posts><span>posts</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/series/ title=series><span>series</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/categories/ title=categories><span>categories</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/tags/ title=tags><span>tags</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/archives/ title=archives><span>archives</span></a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://cspaulia.github.io/cspaulia-blog/>Home</a>&nbsp;»&nbsp;<a href=https://cspaulia.github.io/cspaulia-blog/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">SFT and RLHF</h1><div class=post-description>Introduction of SFT and RLHF.</div><div class=post-meta><span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg> September 29, 2025</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 19l7-7 3 3-7 7-3-3z"/><path d="M18 13l-1.5-7.5L2 2l3.5 14.5L13 18l5-5z"/><path d="M2 2l7.586 7.586"/><circle cx="11" cy="11" r="2"/></svg> November 4, 2025</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><path d="M14 2v6h6"/><line x1="8" y1="13" x2="16" y2="13"/><line x1="8" y1="17" x2="16" y2="17"/><line x1="8" y1="9" x2="12" y2="9"/></svg> 994 words</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg> 5 min</span> ｜ <span class=post-meta-item><svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20 21v-2a4 4 0 00-4-4H8a4 4 0 00-4 4v2"/><circle cx="12" cy="7" r="4"/></svg> CSPaulia</span>
<span id=busuanzi_container_page_pv>｜ <span id=busuanzi_value_page_pv></span> views</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-post-training-%e4%b8%89%e9%98%b6%e6%ae%b5 aria-label="1. Post Training 三阶段">1. Post Training 三阶段</a></li><li><a href=#2-sft-%e6%95%b0%e6%8d%ae%e9%9b%86%e7%9a%84%e6%9e%84%e5%bb%ba aria-label="2. SFT 数据集的构建">2. SFT 数据集的构建</a><ul><li><a href=#21-%e6%95%b0%e6%8d%ae%e9%9b%86%e4%b8%ad%e7%9a%84%e9%97%ae%e9%a2%98 aria-label="2.1. 数据集中的问题">2.1. 数据集中的问题</a></li><li><a href=#22-sft-%e6%95%b0%e6%8d%ae%e9%9b%86%e6%9e%84%e5%bb%ba%e7%bb%8f%e9%aa%8c aria-label="2.2. SFT 数据集构建经验">2.2. SFT 数据集构建经验</a></li><li><a href=#23-%e5%9c%a8%e9%a2%84%e8%ae%ad%e7%bb%83%e4%b8%ad%e4%bd%bf%e7%94%a8%e6%8c%87%e4%bb%a4%e5%be%ae%e8%b0%83instruction-tuning aria-label="2.3. 在预训练中使用指令微调（Instruction Tuning）">2.3. 在预训练中使用指令微调（Instruction Tuning）</a></li><li><a href=#24-midtraining--two-phase-training aria-label="2.4. &lsquo;Midtraining&rsquo; / &lsquo;Two-phase Training&rsquo;">2.4. &lsquo;Midtraining&rsquo; / &lsquo;Two-phase Training&rsquo;</a></li></ul></li><li><a href=#3-rlhfreinforcement-learning-with-human-feedback aria-label="3. RLHF（Reinforcement Learning with Human Feedback）">3. RLHF（Reinforcement Learning with Human Feedback）</a><ul><li><a href=#31-%e4%bb%8e%e6%a8%a1%e4%bb%bfimitation%e5%88%b0%e4%bc%98%e5%8c%96optimization aria-label="3.1. 从模仿（Imitation）到优化（optimization）">3.1. 从模仿（Imitation）到优化（optimization）</a></li><li><a href=#32-%e9%9c%80%e8%a6%81rlhf%e7%9a%84%e5%8e%9f%e5%9b%a0 aria-label="3.2. 需要RLHF的原因">3.2. 需要RLHF的原因</a></li><li><a href=#33-%e5%a6%82%e4%bd%95%e8%8e%b7%e5%be%97rlhf%e7%9a%84%e6%95%b0%e6%8d%ae aria-label="3.3. 如何获得RLHF的数据">3.3. 如何获得RLHF的数据</a></li><li><a href=#34-%e5%ae%9e%e7%8e%b0rlhf%e7%9a%84%e6%96%b9%e6%b3%95 aria-label="3.4. 实现RLHF的方法">3.4. 实现RLHF的方法</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h2[id],h3[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=1-post-training-三阶段>1. Post Training 三阶段<a hidden class=anchor aria-hidden=true href=#1-post-training-三阶段>#</a></h2><blockquote><p>来源于 InstructGPT<a href=#ref1>[1]</a></p></blockquote><p><img alt=stages loading=lazy src=/cspaulia-blog/posts/sft_rlhf/stage.png></p><ol><li>收集数据并训练<strong>监督</strong>策略<ul><li>从提示词数据集中采样一个提示词</li><li>标注者对数据进行标注（该标注即为期望输出）</li><li>该标注数据用于对 LLM 进行监督训练</li></ul></li><li>收集<strong>对比数据</strong>并训练<strong>奖励模型</strong><ul><li>采样一个提示词及多个模型的输出</li><li>标注者从“最好”到“最差”对这些输出进行排序</li><li>该标注数据用于对奖励模型进行训练</li></ul></li><li>在<strong>训练好的奖励模型</strong>的加持下利用<strong>强化学习</strong>优化策略<ul><li>从数据集中采用一个新的提示词</li><li>利用策略生成一个输出</li><li>奖励模型计算输出的奖励分数（Reward）</li><li>根据奖励分数利用 PPO 等强化学习方法对策略进行更新</li></ul></li></ol><hr><h2 id=2-sft-数据集的构建>2. SFT 数据集的构建<a hidden class=anchor aria-hidden=true href=#2-sft-数据集的构建>#</a></h2><h3 id=21-数据集中的问题>2.1. 数据集中的问题<a hidden class=anchor aria-hidden=true href=#21-数据集中的问题>#</a></h3><blockquote><p>来自 FLAN 数据集的一个例子:</p><p>What is this text about? OPTIONS: - World - Sport - Business - Science/Tech</p></blockquote><ul><li>自然的对话一般不包含这样的选项</li></ul><blockquote><p>来自 OpenAssistant 的例子:</p><p>Question: Can you write a short introduction about the relevance of the term"monopsony" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.</p><p>Answer: &ldquo;Monopsony&rdquo; refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. [&mldr;]. Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue. References: Bivens, J., & Mishel, L.(2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.</p></blockquote><ul><li>人类标注者很难撰写如此<strong>长而详细</strong>的答案</li><li>输出中的符号（甚至风格）如何抉择</li><li>是否输出<strong>参考文献</strong><ul><li>如何输出准确的参考文献：我们往往希望模型能够输出更加严谨、学术的输出，因此我们会在数据集的回答中加入参考文献，但是模型可能只学到了“输出内容要加上参考文献”，这会导致模型产生幻觉（Hallucination）</li></ul></li><li>是否输出<strong>复杂的知识</strong></li><li>数据集的规模大小</li><li>数据集的安全问题</li><li>如何决定模型输出的长短：通过下图你可以发现，不同指令数据的输入和输出长度区别是很大的</li></ul><p><img alt=instruction_dataset loading=lazy src=/cspaulia-blog/posts/sft_rlhf/instrction_dataset.png></p><hr><h3 id=22-sft-数据集构建经验>2.2. SFT 数据集构建经验<a hidden class=anchor aria-hidden=true href=#22-sft-数据集构建经验>#</a></h3><ol><li>SFT 在模型已经具备某些能力的前提下，通过数据来“抽取”这些能力的表现效果最好；但如果试图用 SFT 去“添加”模型原本不具备的新行为，往往效果不佳</li><li>并不是所有事实正确的数据都会提升模型表现，有时即使是高质量的事实数据，也可能干扰模型已有的分布或对齐，反而让性能下降</li><li>某些类型的数据（例如安全性、遵循指令、风格等）哪怕只有少量，也能对模型带来巨大提升，不过，模型的长尾行为（覆盖面广、稀疏分布的场景）则更依赖于大量数据来改善</li></ol><hr><h3 id=23-在预训练中使用指令微调instruction-tuning>2.3. 在预训练中使用指令微调（Instruction Tuning）<a hidden class=anchor aria-hidden=true href=#23-在预训练中使用指令微调instruction-tuning>#</a></h3><ol><li>在网页或预训练数据集上进行预训练</li><li>将指令微调数据混入预训练中</li><li>额外进行一个简短的指令微调</li></ol><hr><h3 id=24-midtraining--two-phase-training>2.4. &lsquo;Midtraining&rsquo; / &lsquo;Two-phase Training&rsquo;<a hidden class=anchor aria-hidden=true href=#24-midtraining--two-phase-training>#</a></h3><p><img alt=minicpm loading=lazy src=/cspaulia-blog/posts/sft_rlhf/minicpm.png></p><p>这种方案<a href=#ref2>[2]</a>好似已被大部分 LLM 公司采纳（但没有详细的文档）：</p><ul><li>在 Stable 阶段采用纯预训练数据集训练（如上图左侧所示）；</li><li>在 Decay 阶段采用预训练+指令微调混合数据集进行训练（如上图右侧所示）</li></ul><hr><h2 id=3-rlhfreinforcement-learning-with-human-feedback>3. RLHF（Reinforcement Learning with Human Feedback）<a hidden class=anchor aria-hidden=true href=#3-rlhfreinforcement-learning-with-human-feedback>#</a></h2><h3 id=31-从模仿imitation到优化optimization>3.1. 从模仿（Imitation）到优化（optimization）<a hidden class=anchor aria-hidden=true href=#31-从模仿imitation到优化optimization>#</a></h3><p><strong>模仿（Imitation，即SFT）</strong>：根据一些参考分布 $p^*(y|x)$ 调整模型的输出分布，使得输出分布 $\hat{p}(y|x) \approx p^*(y|x)$</p><ul><li>从纯生成建模的角度看，SFT 就是让生成模型学会模仿参考分布</li><li>训练需要来自参考策略的数据（比如人工标注的数据集），否则无法“模仿”</li></ul><p><strong>优化（Optimization，即RLHF）</strong>：不断调整输出分布$\hat{p}(y|x)$，使得 $\max\limits_{p} E_p[R(y,x)]$，其中 $R(y,x)$ 为奖励（Reward）</p><ul><li>优化的不是抽象的“真实分布”，而是一个我们能定义并测量的奖励函数（在 RLHF 里，这个奖励函数来自人类反馈，比如人类排序、偏好比较，或者训练出的 reward model）</li><li>在这个阶段，我们不再把语言模型看作“近似真实分布$p^*(y|x)$的模型”（像 SFT 那样），而是把它当作一个 <strong>策略 policy</strong>，用来最大化奖励信号</li></ul><hr><h3 id=32-需要rlhf的原因>3.2. 需要RLHF的原因<a hidden class=anchor aria-hidden=true href=#32-需要rlhf的原因>#</a></h3><ol><li><strong>成本</strong>：SFT 的成本非常高昂，尤其是标注成本</li><li><strong>G-V Gap (Generation-Value Gap)</strong>：人们写的内容（生成分布 G）和 人们实际偏好的内容（价值模型 V 的视角）并不总是一致</li></ol><blockquote><p>在一项过去的实验<a href=#ref3>[3]</a>中发现，一些标注者在对比自己写的摘要和模型写的摘要时，有时会更喜欢模型写的摘要，也就是说人类标注并不是最优的</p></blockquote><hr><h3 id=33-如何获得rlhf的数据>3.3. 如何获得RLHF的数据<a hidden class=anchor aria-hidden=true href=#33-如何获得rlhf的数据>#</a></h3><p><strong>方案一</strong>：（在网页上）让模型输出 N 个结果，让标注者（用户）对结果进行排序</p><ul><li>存在的问题：<ul><li>标注结果可能是<strong>低质量的</strong>、<strong>不正确的</strong>，甚至是用大预言模型生成的，这取决于标注者本身</li><li>RLHF 标注者的分布会极大的影响模型的行为</li></ul><blockquote><p>如果标注者中亚洲人多，则模型会倾向于输出亚洲风格的结果</p></blockquote><ul><li>不同的标注者关注点不同</li></ul><blockquote><p>有些标注者更关注格式，有些标注者更关注内容</p></blockquote></li></ul><p><strong>方案二</strong>：使用大语言模型（如GPT-4）对模型（可以是多个不同的模型）输出的 N 个结果进行排序，我们常常称之为 AI Feedback</p><hr><h3 id=34-实现rlhf的方法>3.4. 实现RLHF的方法<a hidden class=anchor aria-hidden=true href=#34-实现rlhf的方法>#</a></h3><h4 id=341-人类反馈下的ppoppo-with-human-feedback>3.4.1. 人类反馈下的PPO（PPO with Human Feedback）<a hidden class=anchor aria-hidden=true href=#341-人类反馈下的ppoppo-with-human-feedback>#</a></h4><p>原始的PPO算法 = Policy Gradient + Off Policy，其优化目标为：</p><p>$$
\max L^{CLIP}(\theta) - \beta KL[\pi_\theta||\pi_{old}]
$$</p><p>其中$L^{CLIP}(\theta) = \mathbb{E}_t \Big[ \min \big( r_t(\theta) \hat{A}_t,; \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t \big) \Big]$，</p><p>其中$r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}$，</p><p><img alt=rlhf loading=lazy src=/cspaulia-blog/posts/sft_rlhf/rlhf.png></p><p>人类反馈下的PPO的优化目标为：</p><p>$$
L^{RLHF}(\theta) = \mathbb{E}_{x \sim D, y \sim \pi} [ r(x,y) ] - \beta D_{KL} \left[ \pi(y|x) || \pi_{\text{ref}}(y|x) \right]
$$</p><p>其中，</p><ul><li>$r(x,y)$：是奖励模型对输出$y$在输入$x$下的奖励评分</li><li>$\pi(y∣x)$：是当前训练的策略（即RLHF中训练的模型），表示在输入$x$下生成$y$的概率分布</li><li>$\pi_{\text{ref}}(y∣x)$：是参考策略，通常是一个冻结的 SFT 模型，用于对比当前模型的生成结果，保证不会过度偏离基准模型</li><li>$D_{KL} \left[ \pi(y|x) || \pi_{\text{ref}}(y|x) \right]$：是当前策略与参考策略之间的 KL 散度，用于约束新策略不能偏离参考策略太远</li></ul><p><strong>问题一</strong>：为什么传统PPO中优化$\mathbb{E}_t [r_t(\theta) \hat{A}_t]$，而RLHF优化$\mathbb{E}_{x, y} [ r(x,y) ]$？</p><ul><li>在传统的强化学习任务中，我们往往会从<strong>离线</strong>策略中获得优势函数$\hat{A}_t$，因此优化目标为$\max \mathbb{E}_t [r_t(\theta) \hat{A}_t]$</li><li>在LLM基于人类反馈的强化学习任务，我们选择利用奖励模型（Reward model）直接对<strong>在线</strong>的LLM的输出进行打分，因此无需使用$r_t(\theta)$进行重要性采样修正</li></ul><p><strong>问题二</strong>：传统PPO中的$\pi_{old}$和RLHF PPO中的$\pi_{\text{ref}}$有什么区别？</p><ul><li>$\pi_{old}$：上一次迭代的策略，用于收集数据和做重要性采样，会不断更新，通常就是“前一版策略”</li><li>$\pi_{\text{ref}}$：一个固定的参考策略，一般是 SFT（监督微调模型），也就是在 RLHF 训练前冻结的模型，不会更新</li></ul><hr><h4 id=342-dpo>3.4.2. DPO<a hidden class=anchor aria-hidden=true href=#342-dpo>#</a></h4><p><img alt=dpo loading=lazy src=/cspaulia-blog/posts/sft_rlhf/dpo.png></p><p>DPO训练目标与PPO一致：</p><p>$$
\max L^{RLHF}(\theta) = \mathbb{E}_{x \sim D, y \sim \pi} [ r(x,y) ] - \beta D_{KL} \left[ \pi(y|x) || \pi_{\text{ref}}(y|x) \right]
$$</p><p>由该优化目标可得最优Policy为：</p><p>$$
\pi(y|x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))
$$</p><p>其中，$Z(x) = \sum\limits_y \pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))$</p><details><summary>点击展开公式推导</summary><p>$$
\begin{align}
&\max\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ r(x,y) ] - \beta D_{KL} \left[ \pi(y|x) || \pi_{\text{ref}}(y|x) \right] \\
= &\max\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ r(x,y) ] - \mathbb{E}_{x \sim D, y \sim \pi} [ \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} ] \\
= &\max\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ r(x,y) - \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} ] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - r(x,y) ] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \log \exp(\frac{1}{\beta}r(x,y)) ] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))}] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))\frac{1}{Z(x)} Z(x)}] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \log \frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))} - \log Z(x)] \\
\end{align}
$$</p><p>其中$Z(x) = \sum\limits_y \pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))$，</p><p>令$\pi^\star(y|x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y)) = \frac{\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))}{\sum\limits_y \pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))}$，则有</p><p>$$
\begin{align}
&\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \log \frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))} - \log Z(x)] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \log \frac{\pi(y|x)}{\pi^\star(y|x)} - \log Z(x)] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [ \log \frac{\pi(y|x)}{\pi^\star(y|x)}] \\
= &\min\limits_\pi \mathbb{E}_{x \sim D, y \sim \pi} [D_{KL} (\pi(y|x) || \pi^\star(y|x))] \\
\end{align}
$$</p><p>最优解为$\pi(y|x) = \pi^\star(y|x) = \frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp(\frac{1}{\beta}r(x,y))$</p></details><p>将最优policy带入Bradley-Terry模型建模的最大似然估计中，即可得到DPO损失函数为：</p><p>$$
Loss_{DPO} = - \ln \sigma \left( \beta \ln \frac{\pi(y^+ \mid x)}{\pi_{ref}(y^+ \mid x)} - \beta \ln \frac{\pi(y^- \mid x)}{\pi_{ref}(y^- \mid x)} \right)
$$</p><p>其中$y^+$：是 人类偏好（或高质量的生成样本），是被标注为“更优”的样本，$y^-$：是 负偏好（或低质量的生成样本），是被标注为“较差”的样本</p><details><summary>点击展开Bradley-Terry模型简介</summary><p><strong>Bradley-Terry模型</strong>：</p><p>$$
P(i>j) = \frac{\alpha_i}{\alpha_i + \alpha_j}
$$</p><p>$P(i>j)$表示第$i$个元素战胜第$j$个元素的概率，一般的loss函数为</p><p>$$
Loss = -\mathbb{E}_{(\alpha_x, \alpha_y) \sim D} [ \ln \frac{\alpha_i}{\alpha_i + \alpha_j}]
$$</p><p><strong>大模型中的Bradley-Terry模型</strong>：</p><p>$$
P(y_1>y_2) = \frac{r(x,y_1)}{r(x,y_1) + r(x,y_2)}
$$</p><p>其中$x$为输入的prompt，$y$为输出，$r(x,y)$为奖励得分，为防止$r(x,y)$为负数，加入指数函数：</p><p>$$
P(y_1>y_2) = \frac{\exp(r(x,y_1))}{\exp(r(x,y_1)) + \exp(r(x,y_2))}
$$</p><p>其Loss为</p><p>$$
\begin{align}
\text{Loss} &= - \mathbb{E}_{(x, y^+, y^-) \sim D} [ \ln ( \frac{\exp(r(x, y^+))}{\exp(r(x, y^+)) + \exp(r(x, y^-))} ) ] \\
&= - \mathbb{E}_{(x, y^+, y^-) \sim D} [ \ln ( \frac{1}{1 + \exp(r(x, y^-)) - r(x, y^+)} ) ] \\
&= - \mathbb{E}_{(x, y^+, y^-) \sim D} [ \ln \sigma ( r(x, y^+) - r(x, y^-) ) ]
\end{align}
$$</p><p>其中$\sigma (x) = \frac{1}{1+\exp(-x)}$为sigmoid函数</p></details><details><summary>点击展开公式推导</summary><p>$$
\begin{align}
&\pi(y \mid x) = \frac{1}{Z(x)} \pi_{ref}(y \mid x) \exp( \frac{1}{\beta} r(x, y)) \\
\Rightarrow &\exp( \frac{1}{\beta} r(x, y) ) = \frac{\pi(y \mid x)}{\pi_{ref}(y \mid x)} Z(x) \\
\Rightarrow &amp;r(x, y) = \beta \ln ( \frac{\pi(y \mid x)}{\pi_{ref}(y \mid x)} Z(x) ) \\
\Rightarrow &amp;r(x, y) = \beta \ln ( \frac{\pi(y \mid x)}{\pi_{ref}(y \mid x)} ) + \beta \ln Z(x) \\
\end{align}
$$</p><p>将奖励评分送入Bradley-Terry模型得到：</p><p>$$
\begin{align}
Loss & = - \ln \sigma ( r(x, y^+) - r(x, y^-) ) \\
& = - \ln \sigma (\beta \ln ( \frac{\pi(y^+ \mid x)}{\pi_{ref}(y^+ \mid x)} ) + \beta \ln Z(x) - \beta \ln ( \frac{\pi(y^- \mid x)}{\pi_{ref}(y^- \mid x)} ) - \beta \ln Z(x)) \\
& = - \ln \sigma (\beta \ln ( \frac{\pi(y^+ \mid x)}{\pi_{ref}(y^+ \mid x)} ) - \beta \ln ( \frac{\pi(y^- \mid x)}{\pi_{ref}(y^- \mid x)} ))
\end{align}
$$</p></details><hr><div class=zhihu-ref><div class=zhihu-ref-title>参考文献</div><ol><li id=ref1><a href=https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf target=_blank>Ouyang L, Wu J, Jiang X, et al. Training language models to follow instructions with human feedback[J]. Advances in neural information processing systems, 2022, 35: 27730-27744.</a></li><li id=ref2><a href=https://arxiv.org/pdf/2404.06395 target=_blank>Hu S, Tu Y, Han X, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies[J]. arXiv preprint arXiv:2404.06395, 2024.</a></li><li id=ref3><a href="https://watermark02.silverchair.com/tacl_a_00632.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA0kwggNFBgkqhkiG9w0BBwagggM2MIIDMgIBADCCAysGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM56gOdOIymzmdZk7SAgEQgIIC_MoTKqHYldlrb721Q5h2KffVo_kOA5sAdAk6Iko4Wu_cYFNYwzooRlN4Vmg64QkEP1rbgfQi1FVMWnNzCPbYCyhxpMlFcYX6bAPaLSTZiI3kUweSL5mPGPsGGycoEX1MAF5i4yKnT4pM7UKL5izeIaflqaWa-Rzo0cqhI66vUbfbp5WQILP2RQOA5qNoLUFbXtLx0TUgxuo-HaFfh3L3IEi1f3Loyj2-UJw4V7Dr7DYCf39XXfICpALlzjoTlJ_3s5YqMcbKn-9my4-DVh4EjU4QenszgsYc2GuHAXE23nUeFGfX93vMXESnP8mwRkVuKuaGodxiW-SZnO4jEQfAv8qH692lrJmkq9iT_WIvHE__hJQtIVr6cZgY9fXGrBzGmK6Hkz79PyMaAK5VglXB-dqB9JrQJQzXdjoIV4qmCKzVGOwYtT5qOOtoNPzHOTaBseERNSMMJw4Jq8t8S4-8GbFfmUcBdQrgs2HM5uYn2igRj_F-xMyRVe5y-jl8s_0Q8dFfliXCESTTI8p0NIqqz0uCY7iB64TemqFVEtR8M1FNbbJ82bhBAKg1zoJtG8hC3pIiQiXawsZzd9oFbx0GFwDdbvbYETEp5R3sJQ8wT1Ra2WhCPicvfSWgdC2EZRCWM4NvUZhsnpBfvZ4DlaE6xWma0u1x_rzrkfabjJcPnbC05EaF9dfx5pJ-X5uPnb1bDyIZQ5zyoueKtaPZzQVtP0mSH7qozQLRLR6UsdHH1RexlpnBOUyYtQtU50Xs0rySHvuWDYaeMICgOQwYGl8heOYlIUzexCgVvyuJz3c2HgwbUxQI3eAH05D74ApGpnaqKy1QNqQJKKXxIZ8QmpQYC4vnKBes20kXjILZYtSzqB6QZOHtfQNee_NTb3E3Q8cJANsOVBZeiIub1HggCbXtfvFmu1R9sTZDQCOrT_U-ABpM1ANqxk32Ty81QZjlj0A8vKh_wFqmoCff8IWI2kb0srnEyK0wt7gzw5fX1cbFfM7d53Y5T9eg_8t857NX" target=_blank>Zhang T, Ladhak F, Durmus E, et al. Benchmarking large language models for news summarization[J]. Transactions of the Association for Computational Linguistics, 2024, 12: 39-57.</a></li><li id=ref4><a href=https://github.com/stanford-cs336/spring2025-lectures/blob/61eddac004df975466cff0329b615f2d24230069/nonexecutable/2025%20Lecture%2015%20-%20RLHF%20Alignment.pdf target=_blank>stanford-cs336 lecture 15</a></li></ol></div></div><br><div><div class=copyright-card><div class=copyright-card-main><div class=copyright-info><h3 class=copyright-title>SFT and RLHF</h3><p class=copyright-link><a href=https://cspaulia.github.io/cspaulia-blog/posts/sft_rlhf/>https://cspaulia.github.io/cspaulia-blog/posts/sft_rlhf/</a></p><div class=copyright-meta><span><strong>Author</strong><br>CSPaulia</span>
<span><strong>Published at</strong><br>September 29, 2025</span>
<span><strong>Copyright</strong><br><a rel=license href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank>CC BY-NC-SA 4.0</a></span></div></div><div class=copyright-icon>©</div></div></div></div><footer class=post-footer><p style=font-size:medium;margin-bottom:5px;font-weight:700>categories</p><ul class=post-tags><li><a href=https://cspaulia.github.io/cspaulia-blog/categories/deep-learning-skills/>Deep Learning Skills</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/categories/large-language-model/>Large Language Model</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/categories/reinforce-learning/>Reinforce Learning</a></li></ul><p style=font-size:medium;margin-bottom:5px;font-weight:700>tags</p><ul class=post-tags><li><a href=https://cspaulia.github.io/cspaulia-blog/tags/sft/>SFT</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/tags/reinforcement-learning/>Reinforcement Learning</a></li><li><a href=https://cspaulia.github.io/cspaulia-blog/tags/rlhf/>RLHF</a></li></ul><nav class=paginav><a class=prev href=https://cspaulia.github.io/cspaulia-blog/posts/flow/><span class=title>« Prev</span><br><span>流匹配与扩散模型</span>
</a><a class=next href=https://cspaulia.github.io/cspaulia-blog/posts/primitives/><span class=title>Next »</span><br><span>基本原语（primitives）与资源计算</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share SFT and RLHF on x" href="https://x.com/intent/tweet/?text=SFT%20and%20RLHF&amp;url=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fposts%2fsft_rlhf%2f&amp;hashtags=SFT%2cReinforcementLearning%2cRLHF"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SFT and RLHF on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fposts%2fsft_rlhf%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share SFT and RLHF on telegram" href="https://telegram.me/share/url?text=SFT%20and%20RLHF&amp;url=https%3a%2f%2fcspaulia.github.io%2fcspaulia-blog%2fposts%2fsft_rlhf%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://cspaulia.github.io/cspaulia-blog/>cspaulia-blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>